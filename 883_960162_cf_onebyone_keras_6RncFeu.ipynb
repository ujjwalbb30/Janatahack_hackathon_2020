{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>We present novel understandings of the Gamma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
       "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>Milky Way open clusters are very diverse in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>Proving that a cryptographic protocol is cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              TITLE  \\\n",
       "0  20973  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  20974  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  20975         Case For Static AMSDU Aggregation in WLANs   \n",
       "3  20976  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  20977  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                                            ABSTRACT  \n",
       "0    We present novel understandings of the Gamma...  \n",
       "1    Meteorites contain minerals from Solar Syste...  \n",
       "2    Frame aggregation is a mechanism by which mu...  \n",
       "3    Milky Way open clusters are very diverse in ...  \n",
       "4    Proving that a cryptographic protocol is cor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import pandas and numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#read the train csv file and explore\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test_2 = pd.read_csv('test.csv')\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shape of the dataframe is:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20972, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'column names are as follows:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'TITLE', 'ABSTRACT', 'Computer Science', 'Physics', 'Mathematics',\n",
       "       'Statistics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#perform EDA on the dataframe\n",
    "display(\"shape of the dataframe is:\",df.shape)\n",
    "display(\"column names are as follows:\",df.columns)\n",
    "index_final = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20972 entries, 0 to 20971\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   ID                    20972 non-null  int64 \n",
      " 1   TITLE                 20972 non-null  object\n",
      " 2   ABSTRACT              20972 non-null  object\n",
      " 3   Computer Science      20972 non-null  int64 \n",
      " 4   Physics               20972 non-null  int64 \n",
      " 5   Mathematics           20972 non-null  int64 \n",
      " 6   Statistics            20972 non-null  int64 \n",
      " 7   Quantitative Biology  20972 non-null  int64 \n",
      " 8   Quantitative Finance  20972 non-null  int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10486.500000</td>\n",
       "      <td>0.409784</td>\n",
       "      <td>0.286716</td>\n",
       "      <td>0.267881</td>\n",
       "      <td>0.248236</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>0.011873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6054.239259</td>\n",
       "      <td>0.491806</td>\n",
       "      <td>0.452238</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.164947</td>\n",
       "      <td>0.108317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5243.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10486.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15729.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20972.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  Computer Science       Physics   Mathematics  \\\n",
       "count  20972.000000      20972.000000  20972.000000  20972.000000   \n",
       "mean   10486.500000          0.409784      0.286716      0.267881   \n",
       "std     6054.239259          0.491806      0.452238      0.442866   \n",
       "min        1.000000          0.000000      0.000000      0.000000   \n",
       "25%     5243.750000          0.000000      0.000000      0.000000   \n",
       "50%    10486.500000          0.000000      0.000000      0.000000   \n",
       "75%    15729.250000          1.000000      1.000000      1.000000   \n",
       "max    20972.000000          1.000000      1.000000      1.000000   \n",
       "\n",
       "         Statistics  Quantitative Biology  Quantitative Finance  \n",
       "count  20972.000000          20972.000000          20972.000000  \n",
       "mean       0.248236              0.027990              0.011873  \n",
       "std        0.432000              0.164947              0.108317  \n",
       "min        0.000000              0.000000              0.000000  \n",
       "25%        0.000000              0.000000              0.000000  \n",
       "50%        0.000000              0.000000              0.000000  \n",
       "75%        0.000000              0.000000              0.000000  \n",
       "max        1.000000              1.000000              1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer Science',\n",
       " 'Physics',\n",
       " 'Mathematics',\n",
       " 'Statistics',\n",
       " 'Quantitative Biology',\n",
       " 'Quantitative Finance']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the number of research paper under each tag\n",
    "#tag names\n",
    "tagnames = df.drop(['ID','TITLE','ABSTRACT'],axis=1).columns.tolist()\n",
    "display(tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Computer Science': 8594,\n",
       " 'Physics': 6013,\n",
       " 'Mathematics': 5618,\n",
       " 'Statistics': 5206,\n",
       " 'Quantitative Biology': 587,\n",
       " 'Quantitative Finance': 249}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagno = {}\n",
    "for name in tagnames:\n",
    "    tagno.update({name:df[name].sum()})\n",
    "display(tagno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib and plot number of research paper under each tag\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(tagno)),list(tagno.values()),align ='center')\n",
    "plt.xticks(range(len(tagno)),list(tagno.keys()),rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum tags that belong to a research paper are:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24acf46ea30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the maximum number of tags present for a research paper\n",
    "df['combinations'] = df['Computer Science'] + df['Physics'] + df['Mathematics'] + df['Statistics'] + df['Quantitative Biology'] + df['Quantitative Finance']\n",
    "display(\"maximum tags that belong to a research paper are:\", df['combinations'].max())\n",
    "df['combinations'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Computer Science', 'Physics'],\n",
       " ['Computer Science', 'Mathematics'],\n",
       " ['Computer Science', 'Statistics'],\n",
       " ['Computer Science', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Quantitative Finance'],\n",
       " ['Physics', 'Mathematics'],\n",
       " ['Physics', 'Statistics'],\n",
       " ['Physics', 'Quantitative Biology'],\n",
       " ['Physics', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Statistics'],\n",
       " ['Mathematics', 'Quantitative Biology'],\n",
       " ['Mathematics', 'Quantitative Finance'],\n",
       " ['Statistics', 'Quantitative Biology'],\n",
       " ['Statistics', 'Quantitative Finance'],\n",
       " ['Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Physics', 'Mathematics'],\n",
       " ['Computer Science', 'Physics', 'Statistics'],\n",
       " ['Computer Science', 'Physics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Physics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Mathematics', 'Statistics'],\n",
       " ['Computer Science', 'Mathematics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Mathematics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Statistics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Statistics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Physics', 'Mathematics', 'Statistics'],\n",
       " ['Physics', 'Mathematics', 'Quantitative Biology'],\n",
       " ['Physics', 'Mathematics', 'Quantitative Finance'],\n",
       " ['Physics', 'Statistics', 'Quantitative Biology'],\n",
       " ['Physics', 'Statistics', 'Quantitative Finance'],\n",
       " ['Physics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Statistics', 'Quantitative Biology'],\n",
       " ['Mathematics', 'Statistics', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Statistics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Computer Science'],\n",
       " ['Physics'],\n",
       " ['Mathematics'],\n",
       " ['Statistics'],\n",
       " ['Quantitative Biology'],\n",
       " ['Quantitative Finance']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'total tag combinations that can be possibly present:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find all unique combinations of 3 tags\n",
    "from itertools import combinations\n",
    "comb2,comb3 = list(combinations(tagnames,2)),list(combinations(tagnames,3))\n",
    "totcomb = [list(ele) for ele in comb2+comb3]\n",
    "totcomb = totcomb + [[el] for el in tagnames]\n",
    "display(totcomb)\n",
    "display(\"total tag combinations that can be possibly present:\", len(totcomb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Computer Science',): 4910,\n",
       "         ('Mathematics',): 3610,\n",
       "         ('Computer Science', 'Statistics'): 2285,\n",
       "         ('Physics',): 5120,\n",
       "         ('Quantitative Biology',): 443,\n",
       "         ('Statistics',): 1636,\n",
       "         ('Physics', 'Mathematics'): 293,\n",
       "         ('Mathematics', 'Statistics'): 825,\n",
       "         ('Computer Science', 'Mathematics'): 682,\n",
       "         ('Quantitative Finance',): 209,\n",
       "         ('Computer Science', 'Physics'): 437,\n",
       "         ('Computer Science', 'Mathematics', 'Statistics'): 179,\n",
       "         ('Physics', 'Statistics'): 99,\n",
       "         ('Computer Science', 'Physics', 'Statistics'): 36,\n",
       "         ('Computer Science', 'Quantitative Biology'): 30,\n",
       "         ('Statistics', 'Quantitative Biology'): 105,\n",
       "         ('Statistics', 'Quantitative Finance'): 24,\n",
       "         ('Physics', 'Mathematics', 'Statistics'): 9,\n",
       "         ('Computer Science', 'Quantitative Finance'): 9,\n",
       "         ('Quantitative Biology', 'Quantitative Finance'): 4,\n",
       "         ('Computer Science', 'Statistics', 'Quantitative Biology'): 5,\n",
       "         ('Computer Science', 'Physics', 'Mathematics'): 19,\n",
       "         ('Computer Science', 'Statistics', 'Quantitative Finance'): 2,\n",
       "         ('Mathematics', 'Statistics', 'Quantitative Finance'): 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'number of unique combinations'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the counts of all unique combinations of tags\n",
    "from collections import Counter\n",
    "co = []\n",
    "for index, row in df.iterrows():\n",
    "    l = []\n",
    "    for name in tagnames:\n",
    "        if row[name] == 1:\n",
    "            l.append(name)\n",
    "    co.append(l)\n",
    "Ot = Counter([tuple(i) for i in co])\n",
    "display(Ot,\"number of unique combinations\",len(Ot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reconstructing',\n",
       " 'Subject-Specific',\n",
       " 'Effect',\n",
       " 'Maps',\n",
       " 'Rotation',\n",
       " 'Invariance',\n",
       " 'Neural',\n",
       " 'Network',\n",
       " 'Spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'A',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'Maxwell--Landau--Lifshitz--Gilbert',\n",
       " 'system',\n",
       " 'Comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'Discrete',\n",
       " 'Wavelet',\n",
       " 'Transforms',\n",
       " 'and',\n",
       " 'Wavelet',\n",
       " 'Tensor',\n",
       " 'Train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'FTIR',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'On',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'On',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " '1I/`Oumuamua',\n",
       " '(2017)',\n",
       " 'U1',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'Adverse',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'polymer',\n",
       " 'coating',\n",
       " 'on',\n",
       " 'heat',\n",
       " 'transport',\n",
       " 'at',\n",
       " 'solid-liquid',\n",
       " 'interface',\n",
       " 'SPH',\n",
       " 'calculations',\n",
       " 'of',\n",
       " 'Mars-scale',\n",
       " 'collisions:',\n",
       " 'the',\n",
       " 'role',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Equation',\n",
       " 'of',\n",
       " 'State,',\n",
       " 'material',\n",
       " 'rheologies,',\n",
       " 'and',\n",
       " 'numerical',\n",
       " 'effects',\n",
       " '$\\\\mathcal{R}_{0}$',\n",
       " 'fails',\n",
       " 'to',\n",
       " 'predict',\n",
       " 'the',\n",
       " 'outbreak',\n",
       " 'potential',\n",
       " 'in',\n",
       " 'the',\n",
       " 'presence',\n",
       " 'of',\n",
       " 'natural-boosting',\n",
       " 'immunity',\n",
       " 'A',\n",
       " 'global',\n",
       " 'sensitivity',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'reduced',\n",
       " 'order',\n",
       " 'models',\n",
       " 'for',\n",
       " 'hydraulically-fractured',\n",
       " 'horizontal',\n",
       " 'wells',\n",
       " 'Role-separating',\n",
       " 'ordering',\n",
       " 'in',\n",
       " 'social',\n",
       " 'dilemmas',\n",
       " 'controlled',\n",
       " 'by',\n",
       " 'topological',\n",
       " 'frustration',\n",
       " 'Dynamics',\n",
       " 'of',\n",
       " 'exciton',\n",
       " 'magnetic',\n",
       " 'polarons',\n",
       " 'in',\n",
       " 'CdMnSe/CdMgSe',\n",
       " 'quantum',\n",
       " 'wells:',\n",
       " 'the',\n",
       " 'effect',\n",
       " 'of',\n",
       " 'self-localization',\n",
       " 'On',\n",
       " 'Varieties',\n",
       " 'of',\n",
       " 'Ordered',\n",
       " 'Automata',\n",
       " 'Direct',\n",
       " 'Evidence',\n",
       " 'of',\n",
       " 'Spontaneous',\n",
       " 'Abrikosov',\n",
       " 'Vortex',\n",
       " 'State',\n",
       " 'in',\n",
       " 'Ferromagnetic',\n",
       " 'Superconductor',\n",
       " 'EuFe$_2$(As$_{1-x}$P$_x$)$_2$',\n",
       " 'with',\n",
       " '$x=0.21$',\n",
       " 'A',\n",
       " 'rank',\n",
       " '18',\n",
       " 'Waring',\n",
       " 'decomposition',\n",
       " 'of',\n",
       " '$sM_{\\\\langle',\n",
       " '3\\\\rangle}$',\n",
       " 'with',\n",
       " '432',\n",
       " 'symmetries',\n",
       " 'The',\n",
       " 'PdBI',\n",
       " 'Arcsecond',\n",
       " 'Whirlpool',\n",
       " 'Survey',\n",
       " '(PAWS).',\n",
       " 'The',\n",
       " 'Role',\n",
       " 'of',\n",
       " 'Spiral',\n",
       " 'Arms',\n",
       " 'in',\n",
       " 'Cloud',\n",
       " 'and',\n",
       " 'Star',\n",
       " 'Formation',\n",
       " 'Higher',\n",
       " 'structure',\n",
       " 'in',\n",
       " 'the',\n",
       " 'unstable',\n",
       " 'Adams',\n",
       " 'spectral',\n",
       " 'sequence',\n",
       " 'Comparing',\n",
       " 'Covariate',\n",
       " 'Prioritization',\n",
       " 'via',\n",
       " 'Matching',\n",
       " 'to',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Methods',\n",
       " 'for',\n",
       " 'Causal',\n",
       " 'Inference',\n",
       " 'using',\n",
       " 'Five',\n",
       " 'Empirical',\n",
       " 'Applications',\n",
       " 'Acoustic',\n",
       " 'Impedance',\n",
       " 'Calculation',\n",
       " 'via',\n",
       " 'Numerical',\n",
       " 'Solution',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Inverse',\n",
       " 'Helmholtz',\n",
       " 'Problem',\n",
       " 'Deciphering',\n",
       " 'noise',\n",
       " 'amplification',\n",
       " 'and',\n",
       " 'reduction',\n",
       " 'in',\n",
       " 'open',\n",
       " 'chemical',\n",
       " 'reaction',\n",
       " 'networks',\n",
       " 'Many-Body',\n",
       " 'Localization:',\n",
       " 'Stability',\n",
       " 'and',\n",
       " 'Instability',\n",
       " 'Fault',\n",
       " 'Detection',\n",
       " 'and',\n",
       " 'Isolation',\n",
       " 'Tools',\n",
       " '(FDITOOLS)',\n",
       " \"User's\",\n",
       " 'Guide',\n",
       " 'Complexity',\n",
       " 'of',\n",
       " 'Deciding',\n",
       " 'Detectability',\n",
       " 'in',\n",
       " 'Discrete',\n",
       " 'Event',\n",
       " 'Systems',\n",
       " 'The',\n",
       " 'Knaster-Tarski',\n",
       " 'theorem',\n",
       " 'versus',\n",
       " 'monotone',\n",
       " 'nonexpansive',\n",
       " 'mappings',\n",
       " 'Efficient',\n",
       " 'methods',\n",
       " 'for',\n",
       " 'computing',\n",
       " 'integrals',\n",
       " 'in',\n",
       " 'electronic',\n",
       " 'structure',\n",
       " 'calculations',\n",
       " 'Diffraction-Aware',\n",
       " 'Sound',\n",
       " 'Localization',\n",
       " 'for',\n",
       " 'a',\n",
       " 'Non-Line-of-Sight',\n",
       " 'Source',\n",
       " \"Jacob's\",\n",
       " 'ladders,',\n",
       " 'crossbreeding',\n",
       " 'in',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " '$ζ$-factorization',\n",
       " 'formulas',\n",
       " 'and',\n",
       " 'selection',\n",
       " 'of',\n",
       " 'families',\n",
       " 'of',\n",
       " '$ζ$-kindred',\n",
       " 'real',\n",
       " 'continuous',\n",
       " 'functions',\n",
       " 'Minimax',\n",
       " 'Estimation',\n",
       " 'of',\n",
       " 'the',\n",
       " '$L_1$',\n",
       " 'Distance',\n",
       " 'Density',\n",
       " 'large',\n",
       " 'deviations',\n",
       " 'for',\n",
       " 'multidimensional',\n",
       " 'stochastic',\n",
       " 'hyperbolic',\n",
       " 'conservation',\n",
       " 'laws',\n",
       " 'mixup:',\n",
       " 'Beyond',\n",
       " 'Empirical',\n",
       " 'Risk',\n",
       " 'Minimization',\n",
       " 'Equality',\n",
       " 'of',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'definitions',\n",
       " 'of',\n",
       " 'Brakke',\n",
       " 'flow',\n",
       " 'Dynamic',\n",
       " 'Base',\n",
       " 'Station',\n",
       " 'Repositioning',\n",
       " 'to',\n",
       " 'Improve',\n",
       " 'Spectral',\n",
       " 'Efficiency',\n",
       " 'of',\n",
       " 'Drone',\n",
       " 'Small',\n",
       " 'Cells',\n",
       " 'An',\n",
       " 'Unsupervised',\n",
       " 'Homogenization',\n",
       " 'Pipeline',\n",
       " 'for',\n",
       " 'Clustering',\n",
       " 'Similar',\n",
       " 'Patients',\n",
       " 'using',\n",
       " 'Electronic',\n",
       " 'Health',\n",
       " 'Record',\n",
       " 'Data',\n",
       " 'Deep',\n",
       " 'Neural',\n",
       " 'Network',\n",
       " 'Optimized',\n",
       " 'to',\n",
       " 'Resistive',\n",
       " 'Memory',\n",
       " 'with',\n",
       " 'Nonlinear',\n",
       " 'Current-Voltage',\n",
       " 'Characteristics',\n",
       " 'Rate-Distortion',\n",
       " 'Region',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Gray-Wyner',\n",
       " 'Model',\n",
       " 'with',\n",
       " 'Side',\n",
       " 'Information',\n",
       " 'Fourier-based',\n",
       " 'numerical',\n",
       " 'approximation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Weertman',\n",
       " 'equation',\n",
       " 'for',\n",
       " 'moving',\n",
       " 'dislocations',\n",
       " 'Design',\n",
       " 'Decisions',\n",
       " 'for',\n",
       " 'Weave:',\n",
       " 'A',\n",
       " 'Real-Time',\n",
       " 'Web-based',\n",
       " 'Collaborative',\n",
       " 'Visualization',\n",
       " 'Framework',\n",
       " 'Suzaku',\n",
       " 'Analysis',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Supernova',\n",
       " 'Remnant',\n",
       " 'G306.3-0.9',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Gamma-ray',\n",
       " 'View',\n",
       " 'of',\n",
       " 'Its',\n",
       " 'Neighborhood',\n",
       " 'Japanese',\n",
       " 'Sentiment',\n",
       " 'Classification',\n",
       " 'using',\n",
       " 'a',\n",
       " 'Tree-Structured',\n",
       " 'Long',\n",
       " 'Short-Term',\n",
       " 'Memory',\n",
       " 'with',\n",
       " 'Attention',\n",
       " 'Covariances,',\n",
       " 'Robustness,',\n",
       " 'and',\n",
       " 'Variational',\n",
       " 'Bayes',\n",
       " 'Are',\n",
       " 'multi-factor',\n",
       " 'Gaussian',\n",
       " 'term',\n",
       " 'structure',\n",
       " 'models',\n",
       " 'still',\n",
       " 'useful?',\n",
       " 'An',\n",
       " 'empirical',\n",
       " 'analysis',\n",
       " 'on',\n",
       " 'Italian',\n",
       " 'BTPs',\n",
       " 'Probing',\n",
       " 'valley',\n",
       " 'filtering',\n",
       " 'effect',\n",
       " 'by',\n",
       " 'Andreev',\n",
       " 'reflection',\n",
       " 'in',\n",
       " 'zigzag',\n",
       " 'graphene',\n",
       " 'nanoribbon',\n",
       " 'Generalized',\n",
       " 'Approximate',\n",
       " 'Message-Passing',\n",
       " 'Decoder',\n",
       " 'for',\n",
       " 'Universal',\n",
       " 'Sparse',\n",
       " 'Superposition',\n",
       " 'Codes',\n",
       " 'LAAIR:',\n",
       " 'A',\n",
       " 'Layered',\n",
       " 'Architecture',\n",
       " 'for',\n",
       " 'Autonomous',\n",
       " 'Interactive',\n",
       " 'Robots',\n",
       " '3D',\n",
       " 'Human',\n",
       " 'Pose',\n",
       " 'Estimation',\n",
       " 'in',\n",
       " 'RGBD',\n",
       " 'Images',\n",
       " 'for',\n",
       " 'Robotic',\n",
       " 'Task',\n",
       " 'Learning',\n",
       " 'Simultaneous',\n",
       " 'non-vanishing',\n",
       " 'for',\n",
       " 'Dirichlet',\n",
       " 'L-functions',\n",
       " 'Wehrl',\n",
       " 'Entropy',\n",
       " 'Based',\n",
       " 'Quantification',\n",
       " 'of',\n",
       " 'Nonclassicality',\n",
       " 'for',\n",
       " 'Single',\n",
       " 'Mode',\n",
       " 'Quantum',\n",
       " 'Optical',\n",
       " 'States',\n",
       " 'Attention-based',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Person',\n",
       " 'Retrieval',\n",
       " 'Large',\n",
       " 'Scale',\n",
       " 'Automated',\n",
       " 'Forecasting',\n",
       " 'for',\n",
       " 'Monitoring',\n",
       " 'Network',\n",
       " 'Safety',\n",
       " 'and',\n",
       " 'Security',\n",
       " 'Contextual',\n",
       " 'Regression:',\n",
       " 'An',\n",
       " 'Accurate',\n",
       " 'and',\n",
       " 'Conveniently',\n",
       " 'Interpretable',\n",
       " 'Nonlinear',\n",
       " 'Model',\n",
       " 'for',\n",
       " 'Mining',\n",
       " 'Discovery',\n",
       " 'from',\n",
       " 'Scientific',\n",
       " 'Data',\n",
       " 'Multi-time',\n",
       " 'correlators',\n",
       " 'in',\n",
       " 'continuous',\n",
       " 'measurement',\n",
       " 'of',\n",
       " 'qubit',\n",
       " 'observables',\n",
       " 'Parallelism,',\n",
       " 'Concurrency',\n",
       " 'and',\n",
       " 'Distribution',\n",
       " 'in',\n",
       " 'Constraint',\n",
       " 'Handling',\n",
       " 'Rules:',\n",
       " 'A',\n",
       " 'Survey',\n",
       " 'Robustness',\n",
       " 'against',\n",
       " 'the',\n",
       " 'channel',\n",
       " 'effect',\n",
       " 'in',\n",
       " 'pathological',\n",
       " 'voice',\n",
       " 'detection',\n",
       " 'An',\n",
       " 'Effective',\n",
       " 'Framework',\n",
       " 'for',\n",
       " 'Constructing',\n",
       " 'Exponent',\n",
       " 'Lattice',\n",
       " 'Basis',\n",
       " 'of',\n",
       " 'Nonzero',\n",
       " 'Algebraic',\n",
       " 'Numbers',\n",
       " 'Competing',\n",
       " 'evolutionary',\n",
       " 'paths',\n",
       " 'in',\n",
       " 'growing',\n",
       " 'populations',\n",
       " 'with',\n",
       " 'applications',\n",
       " 'to',\n",
       " 'multidrug',\n",
       " 'resistance',\n",
       " 'Transient',\n",
       " 'flows',\n",
       " 'in',\n",
       " 'active',\n",
       " 'porous',\n",
       " 'media',\n",
       " 'An',\n",
       " 'information',\n",
       " 'model',\n",
       " 'for',\n",
       " 'modular',\n",
       " 'robots:',\n",
       " 'the',\n",
       " 'Hardware',\n",
       " 'Robot',\n",
       " 'Information',\n",
       " 'Model',\n",
       " '(HRIM)',\n",
       " 'Detecting',\n",
       " 'Adversarial',\n",
       " 'Samples',\n",
       " 'Using',\n",
       " 'Density',\n",
       " 'Ratio',\n",
       " 'Estimates',\n",
       " 'The',\n",
       " 'Query',\n",
       " 'Complexity',\n",
       " 'of',\n",
       " 'Cake',\n",
       " 'Cutting',\n",
       " 'Stacked',\n",
       " 'Convolutional',\n",
       " 'and',\n",
       " 'Recurrent',\n",
       " 'Neural',\n",
       " 'Networks',\n",
       " 'for',\n",
       " 'Music',\n",
       " 'Emotion',\n",
       " 'Recognition',\n",
       " 'Timed',\n",
       " 'Automata',\n",
       " 'with',\n",
       " 'Polynomial',\n",
       " 'Delay',\n",
       " 'and',\n",
       " 'their',\n",
       " 'Expressiveness',\n",
       " 'Superconducting',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'Cu',\n",
       " 'intercalated',\n",
       " 'Bi$_2$Se$_3$',\n",
       " 'studied',\n",
       " 'by',\n",
       " 'Muon',\n",
       " 'Spin',\n",
       " 'Spectroscopy',\n",
       " 'Time-domain',\n",
       " 'THz',\n",
       " 'spectroscopy',\n",
       " 'reveals',\n",
       " 'coupled',\n",
       " 'protein-hydration',\n",
       " 'dielectric',\n",
       " 'response',\n",
       " 'in',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'native',\n",
       " 'and',\n",
       " 'fibrils',\n",
       " 'of',\n",
       " 'human',\n",
       " 'lyso-zyme',\n",
       " 'Inversion',\n",
       " 'of',\n",
       " 'Qubit',\n",
       " 'Energy',\n",
       " 'Levels',\n",
       " 'in',\n",
       " 'Qubit-Oscillator',\n",
       " 'Circuits',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Deep-Strong-Coupling',\n",
       " 'Regime',\n",
       " 'Deep',\n",
       " 'Multiple',\n",
       " 'Instance',\n",
       " 'Feature',\n",
       " 'Learning',\n",
       " 'via',\n",
       " 'Variational',\n",
       " 'Autoencoder',\n",
       " 'Regularity',\n",
       " 'of',\n",
       " 'envelopes',\n",
       " 'in',\n",
       " 'Kähler',\n",
       " 'classes',\n",
       " '$S^1$-equivariant',\n",
       " 'Index',\n",
       " 'theorems',\n",
       " 'and',\n",
       " 'Morse',\n",
       " 'inequalities',\n",
       " 'on',\n",
       " 'complex',\n",
       " 'manifolds',\n",
       " 'with',\n",
       " 'boundary',\n",
       " 'Internal',\n",
       " 'Model',\n",
       " 'from',\n",
       " 'Observations',\n",
       " 'for',\n",
       " 'Reward',\n",
       " 'Shaping',\n",
       " 'Characterizations',\n",
       " 'of',\n",
       " 'quasitrivial',\n",
       " 'symmetric',\n",
       " 'nondecreasing',\n",
       " 'associative',\n",
       " 'operations',\n",
       " 'Multivariate',\n",
       " 'Dependency',\n",
       " 'Measure',\n",
       " 'based',\n",
       " 'on',\n",
       " 'Copula',\n",
       " 'and',\n",
       " 'Gaussian',\n",
       " 'Kernel',\n",
       " 'The',\n",
       " 'nature',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tensor',\n",
       " 'order',\n",
       " 'in',\n",
       " 'Cd2Re2O7',\n",
       " 'Efficient',\n",
       " 'and',\n",
       " 'consistent',\n",
       " 'inference',\n",
       " 'of',\n",
       " 'ancestral',\n",
       " 'sequences',\n",
       " 'in',\n",
       " 'an',\n",
       " 'evolutionary',\n",
       " 'model',\n",
       " 'with',\n",
       " 'insertions',\n",
       " 'and',\n",
       " 'deletions',\n",
       " 'under',\n",
       " 'dense',\n",
       " 'taxon',\n",
       " 'sampling',\n",
       " 'Flow',\n",
       " 'Characteristics',\n",
       " 'and',\n",
       " 'Cores',\n",
       " 'of',\n",
       " 'Complex',\n",
       " 'Network',\n",
       " 'and',\n",
       " 'Multiplex',\n",
       " 'Type',\n",
       " 'Systems',\n",
       " 'Pattern-forming',\n",
       " 'fronts',\n",
       " 'in',\n",
       " 'a',\n",
       " 'Swift-Hohenberg',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'directional',\n",
       " 'quenching',\n",
       " '-',\n",
       " 'parallel',\n",
       " 'and',\n",
       " 'oblique',\n",
       " 'stripes',\n",
       " 'Generalized',\n",
       " 'Minimum',\n",
       " 'Distance',\n",
       " 'Estimators',\n",
       " 'in',\n",
       " 'Linear',\n",
       " 'Regression',\n",
       " 'with',\n",
       " 'Dependent',\n",
       " 'Errors',\n",
       " 'Live',\n",
       " 'Service',\n",
       " 'Migration',\n",
       " 'in',\n",
       " 'Mobile',\n",
       " 'Edge',\n",
       " 'Clouds',\n",
       " 'Induced',\n",
       " 'density',\n",
       " 'correlations',\n",
       " 'in',\n",
       " 'a',\n",
       " 'sonic',\n",
       " 'black',\n",
       " 'hole',\n",
       " 'condensate',\n",
       " 'Genus',\n",
       " 'growth',\n",
       " 'in',\n",
       " '$\\\\mathbb{Z}_p$-towers',\n",
       " 'of',\n",
       " 'function',\n",
       " 'fields',\n",
       " 'Topological',\n",
       " 'Phases',\n",
       " 'emerging',\n",
       " 'from',\n",
       " 'Spin-Orbital',\n",
       " 'Physics',\n",
       " 'Accurate',\n",
       " 'and',\n",
       " 'Diverse',\n",
       " 'Sampling',\n",
       " 'of',\n",
       " 'Sequences',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " '\"Best',\n",
       " 'of',\n",
       " 'Many\"',\n",
       " 'Sample',\n",
       " 'Objective',\n",
       " 'Exploring',\n",
       " 'RNN-Transducer',\n",
       " 'for',\n",
       " 'Chinese',\n",
       " 'Speech',\n",
       " 'Recognition',\n",
       " 'A',\n",
       " 'Debt-Aware',\n",
       " 'Learning',\n",
       " 'Approach',\n",
       " 'for',\n",
       " 'Resource',\n",
       " 'Adaptations',\n",
       " 'in',\n",
       " 'Cloud',\n",
       " 'Elasticity',\n",
       " 'Management',\n",
       " 'Semi-simplicial',\n",
       " 'spaces',\n",
       " 'Constraints,',\n",
       " 'Lazy',\n",
       " 'Constraints,',\n",
       " 'or',\n",
       " 'Propagators',\n",
       " 'in',\n",
       " 'ASP',\n",
       " 'Solving:',\n",
       " 'An',\n",
       " 'Empirical',\n",
       " 'Analysis',\n",
       " 'A',\n",
       " 'Unified',\n",
       " 'Approach',\n",
       " 'to',\n",
       " 'Nonlinear',\n",
       " 'Transformation',\n",
       " 'Materials',\n",
       " 'Stationary',\n",
       " 'crack',\n",
       " 'propagation',\n",
       " 'in',\n",
       " 'a',\n",
       " 'two-dimensional',\n",
       " 'visco-elastic',\n",
       " 'network',\n",
       " 'model',\n",
       " 'A',\n",
       " 'note',\n",
       " 'on',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'group',\n",
       " 'of',\n",
       " 'Kodaira',\n",
       " 'fibrations',\n",
       " 'Photo-Chemically',\n",
       " 'Directed',\n",
       " 'Self-Assembly',\n",
       " 'of',\n",
       " 'Carbon',\n",
       " 'Nanotubes',\n",
       " 'on',\n",
       " 'Surfaces',\n",
       " 'Split-and-augmented',\n",
       " 'Gibbs',\n",
       " 'sampler',\n",
       " '-',\n",
       " 'Application',\n",
       " 'to',\n",
       " 'large-scale',\n",
       " 'inference',\n",
       " 'problems',\n",
       " 'Does',\n",
       " 'a',\n",
       " 'generalized',\n",
       " 'Chaplygin',\n",
       " 'gas',\n",
       " 'correctly',\n",
       " 'describe',\n",
       " 'the',\n",
       " 'cosmological',\n",
       " 'dark',\n",
       " 'sector?',\n",
       " 'The',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'subdiffusion',\n",
       " 'on',\n",
       " 'the',\n",
       " 'NTA',\n",
       " 'size',\n",
       " 'measurements',\n",
       " 'of',\n",
       " 'extracellular',\n",
       " 'vesicles',\n",
       " 'in',\n",
       " 'biological',\n",
       " 'samples',\n",
       " 'Empirical',\n",
       " 'regression',\n",
       " 'quantile',\n",
       " 'process',\n",
       " 'with',\n",
       " 'possible',\n",
       " 'application',\n",
       " 'to',\n",
       " 'risk',\n",
       " 'analysis',\n",
       " 'Primordial',\n",
       " 'perturbations',\n",
       " 'from',\n",
       " 'inflation',\n",
       " 'with',\n",
       " 'a',\n",
       " 'hyperbolic',\n",
       " 'field-space',\n",
       " 'Role',\n",
       " 'of',\n",
       " 'Vanadyl',\n",
       " 'Oxygen',\n",
       " 'in',\n",
       " 'Understanding',\n",
       " 'Metallic',\n",
       " 'Behavior',\n",
       " 'of',\n",
       " 'V2O5(001)',\n",
       " 'Nanorods',\n",
       " 'Graph',\n",
       " 'Convolution:',\n",
       " 'A',\n",
       " 'High-Order',\n",
       " 'and',\n",
       " 'Adaptive',\n",
       " 'Approach',\n",
       " 'Learning',\n",
       " 'Sparse',\n",
       " 'Representations',\n",
       " 'in',\n",
       " 'Reinforcement',\n",
       " 'Learning',\n",
       " 'with',\n",
       " 'Sparse',\n",
       " 'Coding',\n",
       " 'Almost',\n",
       " 'euclidean',\n",
       " 'Isoperimetric',\n",
       " 'Inequalities',\n",
       " 'in',\n",
       " 'spaces',\n",
       " 'satisfying',\n",
       " 'local',\n",
       " 'Ricci',\n",
       " 'curvature',\n",
       " 'lower',\n",
       " 'bounds',\n",
       " 'Exponential',\n",
       " 'Sums',\n",
       " 'and',\n",
       " 'Riesz',\n",
       " 'energies',\n",
       " 'One',\n",
       " 'dimensionalization',\n",
       " 'in',\n",
       " 'the',\n",
       " 'spin-1',\n",
       " 'Heisenberg',\n",
       " 'model',\n",
       " 'on',\n",
       " 'the',\n",
       " 'anisotropic',\n",
       " 'triangular',\n",
       " 'lattice',\n",
       " 'Memory',\n",
       " 'Aware',\n",
       " 'Synapses:',\n",
       " 'Learning',\n",
       " 'what',\n",
       " '(not)',\n",
       " 'to',\n",
       " 'forget',\n",
       " 'Uniform',\n",
       " 'Spectral',\n",
       " 'Convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Stochastic',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject-specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data.',\n",
       " 'Given',\n",
       " 'a',\n",
       " \"subject's\",\n",
       " 'data,',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels:',\n",
       " 'global,',\n",
       " 'i.e.',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject,',\n",
       " 'and',\n",
       " 'local,',\n",
       " 'i.e.',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " \"subject's\",\n",
       " 'data.',\n",
       " 'While',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used,',\n",
       " 'local',\n",
       " 'inference,',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject-specific',\n",
       " 'effect',\n",
       " 'maps,',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'article,',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method,',\n",
       " 'named',\n",
       " 'RSM,',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject-specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular,',\n",
       " 'binary',\n",
       " 'classifiers.',\n",
       " 'RSM',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers.',\n",
       " 'The',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper-type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner,',\n",
       " 'i.e.',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence.',\n",
       " 'Reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'Maximum-A-Posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier-specific',\n",
       " 'fashion.',\n",
       " 'Experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " \"Alzheimer's\",\n",
       " 'Disease',\n",
       " 'Neuroimaging',\n",
       " 'Initiative',\n",
       " '(ADNI)',\n",
       " 'database.',\n",
       " 'Results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'RSM',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging.',\n",
       " 'Analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ADNI',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'RSM',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject-specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non-imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " \"Alzheimer's\",\n",
       " 'Disease',\n",
       " '(AD),',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Mini',\n",
       " 'Mental',\n",
       " 'State',\n",
       " 'Examination',\n",
       " 'Score',\n",
       " 'and',\n",
       " 'Cerebrospinal',\n",
       " 'Fluid',\n",
       " 'amyloid-$\\\\beta$',\n",
       " 'levels.',\n",
       " 'Further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'ADNI',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'RSM',\n",
       " 'is',\n",
       " 'used.',\n",
       " 'Rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'paper,',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " '(CNN)',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " '2-D',\n",
       " 'symbol',\n",
       " 'recognition.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " '2-D',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non-overlap',\n",
       " 'target.',\n",
       " 'Last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least,',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one-shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance.',\n",
       " 'We',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics,',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics.',\n",
       " 'In',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics,',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us,',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics,',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls.',\n",
       " 'We',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Gegenbauer',\n",
       " 'polynomials.',\n",
       " 'We',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'Poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball,',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls,',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Cauchy-Hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Lie',\n",
       " 'ball.',\n",
       " 'The',\n",
       " 'stochastic',\n",
       " 'Landau--Lifshitz--Gilbert',\n",
       " '(LLG)',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Maxwell',\n",
       " 'equations',\n",
       " '(the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'MLLG',\n",
       " 'system)',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " '(fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories).',\n",
       " 'We',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'LLG',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time-differentiable',\n",
       " 'solutions.',\n",
       " 'We',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " '$\\\\theta$-linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'consequence,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions,',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " '(depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " '$\\\\theta$).',\n",
       " 'Hence,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'MLLG',\n",
       " 'system.',\n",
       " 'Numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method.',\n",
       " 'Fourier-transform',\n",
       " 'infra-red',\n",
       " '(FTIR)',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " '7',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms.',\n",
       " 'Wavelet',\n",
       " 'Tensor',\n",
       " 'Train',\n",
       " '(WTT)',\n",
       " 'and',\n",
       " 'Discrete',\n",
       " 'Wavelet',\n",
       " 'Transforms',\n",
       " '(DWT)',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'FTIR',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants.',\n",
       " 'Various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks.',\n",
       " 'Best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'WTT',\n",
       " 'and',\n",
       " 'DWT',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar,',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra.',\n",
       " 'Unlike',\n",
       " 'DWT,',\n",
       " 'WTT',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " '(rank),',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications.',\n",
       " 'Let',\n",
       " '$\\\\Omega',\n",
       " '\\\\subset',\n",
       " '\\\\mathbb{R}^n$',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'Hayman-type',\n",
       " 'asymmetry',\n",
       " 'condition,',\n",
       " 'and',\n",
       " 'let',\n",
       " '$',\n",
       " 'D',\n",
       " '$',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " '\"obstacle\".',\n",
       " 'We',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'Dirichlet',\n",
       " 'eigenvalue',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '$.',\n",
       " 'First,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '$',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'Dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1}',\n",
       " '>',\n",
       " '0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\Omega',\n",
       " '$.',\n",
       " 'In',\n",
       " 'short,',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\mu_\\\\Omega',\n",
       " ':=',\n",
       " '\\\\max_{x}\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '\\\\end{equation}',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega)',\n",
       " '$,',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1}',\n",
       " '$.',\n",
       " 'Second,',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1(\\\\Omega)}',\n",
       " '$',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " '$',\n",
       " '\\\\Omega',\n",
       " '$.',\n",
       " 'Finally,',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " '$',\n",
       " 'D',\n",
       " '$',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega)',\n",
       " '$,',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1(\\\\Omega)}',\n",
       " '$.',\n",
       " 'We',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " '1I/`Oumuamua',\n",
       " '(2017',\n",
       " 'U1)',\n",
       " 'on',\n",
       " '2017',\n",
       " 'October',\n",
       " '30',\n",
       " 'with',\n",
       " 'Lowell',\n",
       " \"Observatory's\",\n",
       " '4.3-m',\n",
       " 'Discovery',\n",
       " 'Channel',\n",
       " 'Telescope.',\n",
       " 'From',\n",
       " 'these',\n",
       " 'observations,',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak-to-trough',\n",
       " 'amplitude',\n",
       " 'of',\n",
       " 'at',\n",
       " 'least',\n",
       " '1.2',\n",
       " 'mag.',\n",
       " 'This',\n",
       " 'lightcurve',\n",
       " 'segment',\n",
       " 'rules',\n",
       " 'out',\n",
       " 'rotation',\n",
       " 'periods',\n",
       " 'less',\n",
       " 'than',\n",
       " '3',\n",
       " 'hr',\n",
       " 'and',\n",
       " 'suggests',\n",
       " 'that',\n",
       " 'the',\n",
       " 'period',\n",
       " 'is',\n",
       " 'at',\n",
       " 'least',\n",
       " '5',\n",
       " 'hr.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'assumption',\n",
       " 'that',\n",
       " 'the',\n",
       " 'variability',\n",
       " 'is',\n",
       " 'due',\n",
       " 'to',\n",
       " 'a',\n",
       " 'changing',\n",
       " 'cross',\n",
       " 'section,',\n",
       " 'the',\n",
       " 'axial',\n",
       " 'ratio',\n",
       " 'is',\n",
       " 'at',\n",
       " 'least',\n",
       " '3:1.',\n",
       " 'We',\n",
       " 'saw',\n",
       " 'no',\n",
       " 'evidence',\n",
       " 'for',\n",
       " 'a',\n",
       " 'coma',\n",
       " 'or',\n",
       " 'tail',\n",
       " 'in',\n",
       " 'either',\n",
       " 'individual',\n",
       " 'images',\n",
       " 'or',\n",
       " 'in',\n",
       " 'a',\n",
       " 'stacked',\n",
       " 'image',\n",
       " 'having',\n",
       " 'an',\n",
       " 'equivalent',\n",
       " 'exposure',\n",
       " 'time',\n",
       " 'of',\n",
       " '9000',\n",
       " 's.',\n",
       " 'The',\n",
       " 'ability',\n",
       " 'of',\n",
       " 'metallic',\n",
       " 'nanoparticles',\n",
       " 'to',\n",
       " 'supply',\n",
       " 'heat',\n",
       " 'to',\n",
       " 'a',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analysing the words in titles and abstracts\n",
    "titles = []\n",
    "abstracts = []\n",
    "for index,row in df.iterrows():\n",
    "    l = row['TITLE'].split()\n",
    "    m = row['ABSTRACT'].split()\n",
    "    for el in l:\n",
    "        titles.append(el)\n",
    "    for ele in m:\n",
    "        abstracts.append(ele)\n",
    "display(titles)\n",
    "display(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>combinations</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance  combinations  \\\n",
       "0                     0             1   \n",
       "1                     0             1   \n",
       "2                     0             1   \n",
       "3                     0             1   \n",
       "4                     0             2   \n",
       "\n",
       "                             title_abstract_combined  \n",
       "0  Reconstructing Subject-Specific Effect Maps   ...  \n",
       "1  Rotation Invariance Neural Network   Rotation ...  \n",
       "2  Spherical polyharmonics and Poisson kernels fo...  \n",
       "3  A finite element approximation for the stochas...  \n",
       "4  Comparative study of Discrete Wavelet Transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>We present novel understandings of the Gamma...</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
       "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>Milky Way open clusters are very diverse in ...</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>Proving that a cryptographic protocol is cor...</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              TITLE  \\\n",
       "0  20973  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  20974  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  20975         Case For Static AMSDU Aggregation in WLANs   \n",
       "3  20976  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  20977  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0    We present novel understandings of the Gamma...   \n",
       "1    Meteorites contain minerals from Solar Syste...   \n",
       "2    Frame aggregation is a mechanism by which mu...   \n",
       "3    Milky Way open clusters are very diverse in ...   \n",
       "4    Proving that a cryptographic protocol is cor...   \n",
       "\n",
       "                             title_abstract_combined  \n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...  \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...  \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...  \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...  \n",
       "4  Witness-Functions versus Interpretation-Functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine title and abstract\n",
    "df['title_abstract_combined'] = df['TITLE'] + ' ' + df['ABSTRACT']\n",
    "df_test_2['title_abstract_combined'] = df_test_2['TITLE'] + ' ' + df_test_2['ABSTRACT']\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...  \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...  \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...  \n",
       "3                     0  A finite element approximation for the stochas...  \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove ID, TITLE, ABSTRACT, combinations, title_processed, and abstract_processed columns\n",
    "df = df.drop(labels = ['ID','TITLE','ABSTRACT','combinations'], axis = 1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...\n",
       "1  Laboratory mid-IR spectra of equilibrated and ...\n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...\n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...\n",
       "4  Witness-Functions versus Interpretation-Functi..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test_2 = df_test_2.drop(labels = ['ID','TITLE','ABSTRACT'], axis = 1)\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstructing subject specific effect maps pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotation invariance neural network rotation in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spherical polyharmonics and poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>a finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>comparative study of discrete wavelet transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstructing subject specific effect maps pr...  \n",
       "1  rotation invariance neural network rotation in...  \n",
       "2  spherical polyharmonics and poisson kernels fo...  \n",
       "3  a finite element approximation for the stochas...  \n",
       "4  comparative study of discrete wavelet transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>closed form marginal likelihood in gamma poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratory mid ir spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case for static amsdu aggregation in wlans fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>the gaia eso survey the inner disk intermediat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>witness functions versus interpretation functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  closed form marginal likelihood in gamma poiss...  \n",
       "1  laboratory mid ir spectra of equilibrated and ...  \n",
       "2  case for static amsdu aggregation in wlans fra...  \n",
       "3  the gaia eso survey the inner disk intermediat...  \n",
       "4  witness functions versus interpretation functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the regular expressions library\n",
    "import re\n",
    "\n",
    "#remove punctuation from title and abstract\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined'].map(lambda x: re.sub('[,\\.!?0-9]', ' ',x))\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined'].map(lambda x: re.sub('[,\\.!?0-9]', ' ',x))\n",
    "\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined_processed'].map(lambda x: re.sub('[\\W_]+', ' ',x))\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined_processed'].map(lambda x: re.sub('[\\W_]+', ' ',x))\n",
    "\n",
    "#convert the title and abstract to lowercase\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined_processed'].map(lambda x: x.lower())\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined_processed'].map(lambda x: x.lower())\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reconstructing',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data',\n",
       " 'given',\n",
       " 'a',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels',\n",
       " 'global',\n",
       " 'i',\n",
       " 'e',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'and',\n",
       " 'local',\n",
       " 'i',\n",
       " 'e',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'while',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'local',\n",
       " 'inference',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands',\n",
       " 'in',\n",
       " 'this',\n",
       " 'article',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method',\n",
       " 'named',\n",
       " 'rsm',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'rsm',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper',\n",
       " 'type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner',\n",
       " 'i',\n",
       " 'e',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'maximum',\n",
       " 'a',\n",
       " 'posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier',\n",
       " 'specific',\n",
       " 'fashion',\n",
       " 'experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'neuroimaging',\n",
       " 'initiative',\n",
       " 'adni',\n",
       " 'database',\n",
       " 'results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'rsm',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging',\n",
       " 'analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'rsm',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non',\n",
       " 'imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'ad',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mini',\n",
       " 'mental',\n",
       " 'state',\n",
       " 'examination',\n",
       " 'score',\n",
       " 'and',\n",
       " 'cerebrospinal',\n",
       " 'fluid',\n",
       " 'amyloid',\n",
       " 'beta',\n",
       " 'levels',\n",
       " 'further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'rsm',\n",
       " 'is',\n",
       " 'used',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'cnn',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'recognition',\n",
       " 'we',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non',\n",
       " 'overlap',\n",
       " 'target',\n",
       " 'last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'we',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'we',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gegenbauer',\n",
       " 'polynomials',\n",
       " 'we',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cauchy',\n",
       " 'hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lie',\n",
       " 'ball',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'maxwell',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'system',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'maxwell',\n",
       " 'equations',\n",
       " 'the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " 'fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories',\n",
       " 'we',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time',\n",
       " 'differentiable',\n",
       " 'solutions',\n",
       " 'we',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " 'theta',\n",
       " 'linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system',\n",
       " 'as',\n",
       " 'a',\n",
       " 'consequence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'theta',\n",
       " 'hence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method',\n",
       " 'comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'and',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'fourier',\n",
       " 'transform',\n",
       " 'infra',\n",
       " 'red',\n",
       " 'ftir',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'dwt',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks',\n",
       " 'best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'dwt',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra',\n",
       " 'unlike',\n",
       " 'dwt',\n",
       " 'wtt',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " 'rank',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications',\n",
       " 'on',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'let',\n",
       " 'omega',\n",
       " 'subset',\n",
       " 'mathbb',\n",
       " 'r',\n",
       " 'n',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'hayman',\n",
       " 'type',\n",
       " 'asymmetry',\n",
       " 'condition',\n",
       " 'and',\n",
       " 'let',\n",
       " 'd',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'obstacle',\n",
       " 'we',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'eigenvalue',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'first',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " 'x',\n",
       " 'd',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'of',\n",
       " 'omega',\n",
       " 'in',\n",
       " 'short',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " 'begin',\n",
       " 'equation',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'max',\n",
       " 'x',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'end',\n",
       " 'equation',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'second',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " 'omega',\n",
       " 'finally',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " 'd',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'we',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'on',\n",
       " 'october',\n",
       " 'with',\n",
       " 'lowell',\n",
       " 'observatory',\n",
       " 's',\n",
       " 'm',\n",
       " 'discovery',\n",
       " 'channel',\n",
       " 'telescope',\n",
       " 'from',\n",
       " 'these',\n",
       " 'observations',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['reconstructing',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data',\n",
       " 'given',\n",
       " 'a',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels',\n",
       " 'global',\n",
       " 'i',\n",
       " 'e',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'and',\n",
       " 'local',\n",
       " 'i',\n",
       " 'e',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'while',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'local',\n",
       " 'inference',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands',\n",
       " 'in',\n",
       " 'this',\n",
       " 'article',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method',\n",
       " 'named',\n",
       " 'rsm',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'rsm',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper',\n",
       " 'type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner',\n",
       " 'i',\n",
       " 'e',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'maximum',\n",
       " 'a',\n",
       " 'posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier',\n",
       " 'specific',\n",
       " 'fashion',\n",
       " 'experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'neuroimaging',\n",
       " 'initiative',\n",
       " 'adni',\n",
       " 'database',\n",
       " 'results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'rsm',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging',\n",
       " 'analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'rsm',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non',\n",
       " 'imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'ad',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mini',\n",
       " 'mental',\n",
       " 'state',\n",
       " 'examination',\n",
       " 'score',\n",
       " 'and',\n",
       " 'cerebrospinal',\n",
       " 'fluid',\n",
       " 'amyloid',\n",
       " 'beta',\n",
       " 'levels',\n",
       " 'further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'rsm',\n",
       " 'is',\n",
       " 'used',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'cnn',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'recognition',\n",
       " 'we',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non',\n",
       " 'overlap',\n",
       " 'target',\n",
       " 'last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'we',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'we',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gegenbauer',\n",
       " 'polynomials',\n",
       " 'we',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cauchy',\n",
       " 'hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lie',\n",
       " 'ball',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'maxwell',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'system',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'maxwell',\n",
       " 'equations',\n",
       " 'the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " 'fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories',\n",
       " 'we',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time',\n",
       " 'differentiable',\n",
       " 'solutions',\n",
       " 'we',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " 'theta',\n",
       " 'linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system',\n",
       " 'as',\n",
       " 'a',\n",
       " 'consequence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'theta',\n",
       " 'hence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method',\n",
       " 'comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'and',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'fourier',\n",
       " 'transform',\n",
       " 'infra',\n",
       " 'red',\n",
       " 'ftir',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'dwt',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks',\n",
       " 'best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'dwt',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra',\n",
       " 'unlike',\n",
       " 'dwt',\n",
       " 'wtt',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " 'rank',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications',\n",
       " 'on',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'let',\n",
       " 'omega',\n",
       " 'subset',\n",
       " 'mathbb',\n",
       " 'r',\n",
       " 'n',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'hayman',\n",
       " 'type',\n",
       " 'asymmetry',\n",
       " 'condition',\n",
       " 'and',\n",
       " 'let',\n",
       " 'd',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'obstacle',\n",
       " 'we',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'eigenvalue',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'first',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " 'x',\n",
       " 'd',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'of',\n",
       " 'omega',\n",
       " 'in',\n",
       " 'short',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " 'begin',\n",
       " 'equation',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'max',\n",
       " 'x',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'end',\n",
       " 'equation',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'second',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " 'omega',\n",
       " 'finally',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " 'd',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'we',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'on',\n",
       " 'october',\n",
       " 'with',\n",
       " 'lowell',\n",
       " 'observatory',\n",
       " 's',\n",
       " 'm',\n",
       " 'discovery',\n",
       " 'channel',\n",
       " 'telescope',\n",
       " 'from',\n",
       " 'these',\n",
       " 'observations',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analysing the words in titles and abstracts\n",
    "titles_pr = []\n",
    "abstracts_pr = []\n",
    "for index,row in df.iterrows():\n",
    "    l = row['title_abstract_combined_processed'].split()\n",
    "    m = row['title_abstract_combined_processed'].split()\n",
    "    for el in l:\n",
    "        titles_pr.append(el)\n",
    "    for ele in m:\n",
    "        abstracts_pr.append(ele)\n",
    "display(titles_pr)\n",
    "display(abstracts_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstructing subject specific effect maps pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotation invariance neural network rotation in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spherical polyharmonics poisson kernels polyha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>finite element approximation stochastic maxwel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>comparative study discrete wavelet transforms ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstructing subject specific effect maps pr...  \n",
       "1  rotation invariance neural network rotation in...  \n",
       "2  spherical polyharmonics poisson kernels polyha...  \n",
       "3  finite element approximation stochastic maxwel...  \n",
       "4  comparative study discrete wavelet transforms ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>closed form marginal likelihood gamma poisson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratory mid ir spectra equilibrated igneous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case static amsdu aggregation wlans frame aggr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>gaia eso survey inner disk intermediate age op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>witness functions versus interpretation functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  closed form marginal likelihood gamma poisson ...  \n",
       "1  laboratory mid ir spectra equilibrated igneous...  \n",
       "2  case static amsdu aggregation wlans frame aggr...  \n",
       "3  gaia eso survey inner disk intermediate age op...  \n",
       "4  witness functions versus interpretation functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#remove stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\"\",sentence)\n",
    "df['title_abstract_combined_processed']=df['title_abstract_combined_processed'].apply(removeStopWords)\n",
    "df_test_2['title_abstract_combined_processed']=df_test_2['title_abstract_combined_processed'].apply(removeStopWords)\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstruct subject specif effect map predict ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotat invari neural network rotat invari trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spheric polyharmon poisson kernel polyharmon f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>finit element approxim stochast maxwel landau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>compar studi discret wavelet transform wavelet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstruct subject specif effect map predict ...  \n",
       "1  rotat invari neural network rotat invari trans...  \n",
       "2  spheric polyharmon poisson kernel polyharmon f...  \n",
       "3  finit element approxim stochast maxwel landau ...  \n",
       "4  compar studi discret wavelet transform wavelet...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>close form margin likelihood gamma poisson mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratori mid ir spectra equilibr igneous met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case static amsdu aggreg wlan frame aggreg mec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>gaia eso survey inner disk intermedi age open ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>wit function versus interpret function secreci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  close form margin likelihood gamma poisson mat...  \n",
       "1  laboratori mid ir spectra equilibr igneous met...  \n",
       "2  case static amsdu aggreg wlan frame aggreg mec...  \n",
       "3  gaia eso survey inner disk intermedi age open ...  \n",
       "4  wit function versus interpret function secreci...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#do stemming on title and abstract\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "df['title_abstract_combined_processed']=df['title_abstract_combined_processed'].apply(stemming)\n",
    "df_test_2['title_abstract_combined_processed']=df_test_2['title_abstract_combined_processed'].apply(stemming)\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import logging\n",
    "logging.basicConfig(format = \"%(levelname)s - %(asctime)s: %(message)s\", datefmt = '%H:%M:%S', level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:29:58: collecting all words and their counts\n",
      "INFO - 11:29:58: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 11:30:03: PROGRESS: at sentence #10000, processed 1020749 words and 570778 word types\n",
      "INFO - 11:30:05: PROGRESS: at sentence #20000, processed 2035896 words and 962748 word types\n",
      "INFO - 11:30:05: collected 996639 word types from a corpus of 2134263 words (unigram + bigrams) and 20972 sentences\n",
      "INFO - 11:30:05: using 996639 counts as vocab in Phrases<0 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 11:30:05: collecting all words and their counts\n",
      "INFO - 11:30:05: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 11:30:07: collected 523371 word types from a corpus of 915660 words (unigram + bigrams) and 8989 sentences\n",
      "INFO - 11:30:07: using 523371 counts as vocab in Phrases<0 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [row.split() for row in df['title_abstract_combined_processed']]\n",
    "sent_test_2 = [row.split() for row in df_test_2['title_abstract_combined_processed']]\n",
    "phrases = Phrases(sent, min_count = 50, progress_per = 10000)\n",
    "phrases_test_2 = Phrases(sent_test_2, min_count = 50, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:30:07: source_vocab length 996639\n",
      "INFO - 11:30:19: Phraser built with 480 phrasegrams\n",
      "INFO - 11:30:19: source_vocab length 523371\n",
      "INFO - 11:30:26: Phraser built with 166 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "bigram_test_2 = Phraser(phrases_test_2)\n",
    "sentences_test_2 = bigram[sent_test_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36007"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'use',\n",
       " 'method',\n",
       " 'data',\n",
       " 'result',\n",
       " 'system',\n",
       " 'base',\n",
       " 'show',\n",
       " 'network',\n",
       " 'problem']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23180"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences_test_2:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'use',\n",
       " 'method',\n",
       " 'data',\n",
       " 'system',\n",
       " 'result',\n",
       " 'base',\n",
       " 'network',\n",
       " 'show',\n",
       " 'algorithm']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "display(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count = 5, window = 2, size = 500, sample = 6e-5, alpha = 0.05, min_alpha = 0.00001, negative = 5, workers = cores-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:30:36: collecting all words and their counts\n",
      "INFO - 11:30:36: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 11:30:39: PROGRESS: at sentence #10000, processed 983758 words, keeping 24853 word types\n",
      "INFO - 11:30:42: PROGRESS: at sentence #20000, processed 1962056 words, keeping 35161 word types\n",
      "INFO - 11:30:42: collected 36007 word types from a corpus of 2056851 raw words and 20972 sentences\n",
      "INFO - 11:30:42: Loading a fresh vocabulary\n",
      "INFO - 11:30:42: effective_min_count=5 retains 13262 unique words (36% of original 36007, drops 22745)\n",
      "INFO - 11:30:42: effective_min_count=5 leaves 2016739 word corpus (98% of original 2056851, drops 40112)\n",
      "INFO - 11:30:42: deleting the raw counts dictionary of 36007 items\n",
      "INFO - 11:30:42: sample=6e-05 downsamples 1245 most-common words\n",
      "INFO - 11:30:42: downsampling leaves estimated 1063097 word corpus (52.7% of prior 2016739)\n",
      "INFO - 11:30:42: estimated required memory for 13262 words and 500 dimensions: 59679000 bytes\n",
      "INFO - 11:30:42: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.15 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per = 10000)\n",
    "print('Time to build vocab: {} mins'.format(round((time()-t)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:30:45: training model with 4 workers on 13262 vocabulary and 500 features, using sg=0 hs=0 sample=6e-05 negative=5 window=2\n",
      "INFO - 11:30:46: EPOCH 1 - PROGRESS: at 14.51% examples, 149684 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:47: EPOCH 1 - PROGRESS: at 29.95% examples, 155158 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 11:30:48: EPOCH 1 - PROGRESS: at 43.89% examples, 151292 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:49: EPOCH 1 - PROGRESS: at 59.20% examples, 153468 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:50: EPOCH 1 - PROGRESS: at 74.27% examples, 154178 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:30:51: EPOCH 1 - PROGRESS: at 87.36% examples, 149929 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:52: EPOCH 1 - PROGRESS: at 98.07% examples, 144652 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:30:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:30:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:30:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:30:52: EPOCH - 1 : training on 2056851 raw words (1061907 effective words) took 7.3s, 144909 effective words/s\n",
      "INFO - 11:30:53: EPOCH 2 - PROGRESS: at 13.53% examples, 143686 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:54: EPOCH 2 - PROGRESS: at 25.62% examples, 134854 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:55: EPOCH 2 - PROGRESS: at 39.11% examples, 136428 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:56: EPOCH 2 - PROGRESS: at 54.39% examples, 142327 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:58: EPOCH 2 - PROGRESS: at 69.44% examples, 144252 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:30:59: EPOCH 2 - PROGRESS: at 81.97% examples, 141757 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:00: EPOCH 2 - PROGRESS: at 93.28% examples, 137792 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:00: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:00: EPOCH - 2 : training on 2056851 raw words (1062141 effective words) took 7.7s, 137174 effective words/s\n",
      "INFO - 11:31:01: EPOCH 3 - PROGRESS: at 11.63% examples, 117142 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:02: EPOCH 3 - PROGRESS: at 23.20% examples, 119047 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:03: EPOCH 3 - PROGRESS: at 35.72% examples, 123362 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:04: EPOCH 3 - PROGRESS: at 47.21% examples, 123215 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:05: EPOCH 3 - PROGRESS: at 59.68% examples, 125065 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:06: EPOCH 3 - PROGRESS: at 74.74% examples, 130253 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:07: EPOCH 3 - PROGRESS: at 91.81% examples, 136341 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:08: EPOCH - 3 : training on 2056851 raw words (1062426 effective words) took 7.6s, 139071 effective words/s\n",
      "INFO - 11:31:09: EPOCH 4 - PROGRESS: at 15.48% examples, 159008 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:10: EPOCH 4 - PROGRESS: at 28.46% examples, 146598 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:11: EPOCH 4 - PROGRESS: at 41.99% examples, 144357 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:12: EPOCH 4 - PROGRESS: at 53.87% examples, 139917 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:13: EPOCH 4 - PROGRESS: at 66.96% examples, 138977 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:14: EPOCH 4 - PROGRESS: at 83.40% examples, 144144 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:15: EPOCH 4 - PROGRESS: at 95.66% examples, 141855 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:15: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:15: EPOCH - 4 : training on 2056851 raw words (1062838 effective words) took 7.5s, 140967 effective words/s\n",
      "INFO - 11:31:16: EPOCH 5 - PROGRESS: at 13.09% examples, 131374 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:17: EPOCH 5 - PROGRESS: at 26.08% examples, 134169 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 11:31:18: EPOCH 5 - PROGRESS: at 38.17% examples, 131199 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:19: EPOCH 5 - PROGRESS: at 53.41% examples, 138499 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:21: EPOCH 5 - PROGRESS: at 69.44% examples, 144307 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:31:22: EPOCH 5 - PROGRESS: at 85.84% examples, 148955 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:22: EPOCH - 5 : training on 2056851 raw words (1063671 effective words) took 7.0s, 152306 effective words/s\n",
      "INFO - 11:31:23: EPOCH 6 - PROGRESS: at 15.48% examples, 161717 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:24: EPOCH 6 - PROGRESS: at 31.89% examples, 165861 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:25: EPOCH 6 - PROGRESS: at 48.17% examples, 166656 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:26: EPOCH 6 - PROGRESS: at 63.08% examples, 164344 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:27: EPOCH 6 - PROGRESS: at 77.60% examples, 161875 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:28: EPOCH 6 - PROGRESS: at 92.82% examples, 161285 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:29: EPOCH - 6 : training on 2056851 raw words (1062403 effective words) took 6.6s, 160704 effective words/s\n",
      "INFO - 11:31:30: EPOCH 7 - PROGRESS: at 15.01% examples, 156111 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:31: EPOCH 7 - PROGRESS: at 28.01% examples, 146928 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:32: EPOCH 7 - PROGRESS: at 42.95% examples, 149290 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:31:33: EPOCH 7 - PROGRESS: at 57.72% examples, 150661 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:31:34: EPOCH 7 - PROGRESS: at 71.39% examples, 148691 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:31:35: EPOCH 7 - PROGRESS: at 85.84% examples, 149480 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:36: EPOCH - 7 : training on 2056851 raw words (1063557 effective words) took 7.0s, 151597 effective words/s\n",
      "INFO - 11:31:37: EPOCH 8 - PROGRESS: at 13.09% examples, 137168 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:38: EPOCH 8 - PROGRESS: at 28.91% examples, 150450 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:31:39: EPOCH 8 - PROGRESS: at 41.99% examples, 145141 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:40: EPOCH 8 - PROGRESS: at 53.87% examples, 140778 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:41: EPOCH 8 - PROGRESS: at 66.96% examples, 139645 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:42: EPOCH 8 - PROGRESS: at 81.45% examples, 141857 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:43: EPOCH 8 - PROGRESS: at 94.25% examples, 140348 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:31:44: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:44: EPOCH - 8 : training on 2056851 raw words (1062504 effective words) took 7.5s, 141375 effective words/s\n",
      "INFO - 11:31:45: EPOCH 9 - PROGRESS: at 15.48% examples, 158400 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:46: EPOCH 9 - PROGRESS: at 31.38% examples, 162028 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:47: EPOCH 9 - PROGRESS: at 43.89% examples, 150502 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:48: EPOCH 9 - PROGRESS: at 56.78% examples, 147003 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:49: EPOCH 9 - PROGRESS: at 70.90% examples, 147427 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:50: EPOCH 9 - PROGRESS: at 81.97% examples, 142566 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:51: EPOCH 9 - PROGRESS: at 96.16% examples, 143357 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:51: EPOCH - 9 : training on 2056851 raw words (1062358 effective words) took 7.4s, 144359 effective words/s\n",
      "INFO - 11:31:52: EPOCH 10 - PROGRESS: at 12.15% examples, 125968 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:53: EPOCH 10 - PROGRESS: at 26.58% examples, 136755 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:54: EPOCH 10 - PROGRESS: at 40.49% examples, 140510 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:55: EPOCH 10 - PROGRESS: at 53.87% examples, 140118 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:56: EPOCH 10 - PROGRESS: at 70.37% examples, 146007 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:57: EPOCH 10 - PROGRESS: at 86.86% examples, 150629 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:31:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:31:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:31:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:31:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:31:58: EPOCH - 10 : training on 2056851 raw words (1062219 effective words) took 7.0s, 152576 effective words/s\n",
      "INFO - 11:31:59: EPOCH 11 - PROGRESS: at 14.51% examples, 152294 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:00: EPOCH 11 - PROGRESS: at 26.58% examples, 138368 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:01: EPOCH 11 - PROGRESS: at 41.48% examples, 144575 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:32:02: EPOCH 11 - PROGRESS: at 56.30% examples, 147721 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:03: EPOCH 11 - PROGRESS: at 66.96% examples, 140559 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:04: EPOCH 11 - PROGRESS: at 78.55% examples, 137384 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:05: EPOCH 11 - PROGRESS: at 94.25% examples, 141150 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:05: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:05: EPOCH - 11 : training on 2056851 raw words (1062415 effective words) took 7.5s, 142340 effective words/s\n",
      "INFO - 11:32:06: EPOCH 12 - PROGRESS: at 14.05% examples, 148508 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:07: EPOCH 12 - PROGRESS: at 28.46% examples, 149439 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:08: EPOCH 12 - PROGRESS: at 43.89% examples, 153435 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:09: EPOCH 12 - PROGRESS: at 60.17% examples, 158748 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:10: EPOCH 12 - PROGRESS: at 76.66% examples, 160968 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:32:11: EPOCH 12 - PROGRESS: at 93.75% examples, 163528 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:12: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:12: EPOCH - 12 : training on 2056851 raw words (1063730 effective words) took 6.5s, 164653 effective words/s\n",
      "INFO - 11:32:13: EPOCH 13 - PROGRESS: at 16.46% examples, 166088 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:32:14: EPOCH 13 - PROGRESS: at 33.34% examples, 170323 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:15: EPOCH 13 - PROGRESS: at 49.13% examples, 169772 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:32:16: EPOCH 13 - PROGRESS: at 64.56% examples, 168455 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:17: EPOCH 13 - PROGRESS: at 80.95% examples, 169291 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:18: EPOCH 13 - PROGRESS: at 97.58% examples, 169524 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:18: EPOCH - 13 : training on 2056851 raw words (1062839 effective words) took 6.3s, 169783 effective words/s\n",
      "INFO - 11:32:19: EPOCH 14 - PROGRESS: at 15.48% examples, 159857 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:20: EPOCH 14 - PROGRESS: at 32.38% examples, 167067 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:21: EPOCH 14 - PROGRESS: at 49.13% examples, 169119 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:22: EPOCH 14 - PROGRESS: at 65.55% examples, 169995 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:23: EPOCH 14 - PROGRESS: at 81.97% examples, 170850 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:24: EPOCH 14 - PROGRESS: at 98.56% examples, 170982 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:24: EPOCH - 14 : training on 2056851 raw words (1063112 effective words) took 6.2s, 171356 effective words/s\n",
      "INFO - 11:32:25: EPOCH 15 - PROGRESS: at 15.01% examples, 154977 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:26: EPOCH 15 - PROGRESS: at 31.89% examples, 166114 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:27: EPOCH 15 - PROGRESS: at 48.17% examples, 167899 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:28: EPOCH 15 - PROGRESS: at 64.05% examples, 166794 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:29: EPOCH 15 - PROGRESS: at 80.47% examples, 168070 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:30: EPOCH 15 - PROGRESS: at 96.64% examples, 167794 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:32:31: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:31: EPOCH - 15 : training on 2056851 raw words (1062814 effective words) took 6.3s, 168595 effective words/s\n",
      "INFO - 11:32:32: EPOCH 16 - PROGRESS: at 14.51% examples, 150569 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:32:33: EPOCH 16 - PROGRESS: at 30.92% examples, 161142 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:34: EPOCH 16 - PROGRESS: at 47.21% examples, 164623 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:35: EPOCH 16 - PROGRESS: at 63.08% examples, 165198 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 11:32:36: EPOCH 16 - PROGRESS: at 79.47% examples, 165912 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:37: EPOCH 16 - PROGRESS: at 96.64% examples, 167210 words/s, in_qsize 0, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:32:37: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:37: EPOCH - 16 : training on 2056851 raw words (1063293 effective words) took 6.3s, 167830 effective words/s\n",
      "INFO - 11:32:38: EPOCH 17 - PROGRESS: at 15.48% examples, 164079 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:39: EPOCH 17 - PROGRESS: at 31.89% examples, 167955 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:40: EPOCH 17 - PROGRESS: at 47.70% examples, 167282 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:41: EPOCH 17 - PROGRESS: at 64.05% examples, 169135 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:42: EPOCH 17 - PROGRESS: at 79.98% examples, 169264 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:43: EPOCH 17 - PROGRESS: at 97.12% examples, 170192 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:43: EPOCH - 17 : training on 2056851 raw words (1062952 effective words) took 6.2s, 170615 effective words/s\n",
      "INFO - 11:32:44: EPOCH 18 - PROGRESS: at 15.95% examples, 164022 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:45: EPOCH 18 - PROGRESS: at 31.89% examples, 163470 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:46: EPOCH 18 - PROGRESS: at 48.65% examples, 167421 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:47: EPOCH 18 - PROGRESS: at 62.60% examples, 161421 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:48: EPOCH 18 - PROGRESS: at 74.27% examples, 153393 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:49: EPOCH 18 - PROGRESS: at 86.86% examples, 149668 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:50: EPOCH 18 - PROGRESS: at 99.57% examples, 147259 words/s, in_qsize 2, out_qsize 2\n",
      "INFO - 11:32:50: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:50: EPOCH - 18 : training on 2056851 raw words (1063445 effective words) took 7.2s, 147372 effective words/s\n",
      "INFO - 11:32:51: EPOCH 19 - PROGRESS: at 10.66% examples, 112316 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:52: EPOCH 19 - PROGRESS: at 24.64% examples, 129035 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:53: EPOCH 19 - PROGRESS: at 35.72% examples, 124007 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:55: EPOCH 19 - PROGRESS: at 50.54% examples, 131919 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:56: EPOCH 19 - PROGRESS: at 65.55% examples, 137015 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:57: EPOCH 19 - PROGRESS: at 81.45% examples, 142289 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 11:32:58: EPOCH 19 - PROGRESS: at 96.64% examples, 144132 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:32:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:32:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:32:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:32:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:32:58: EPOCH - 19 : training on 2056851 raw words (1062696 effective words) took 7.4s, 144202 effective words/s\n",
      "INFO - 11:32:59: EPOCH 20 - PROGRESS: at 12.62% examples, 132588 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:00: EPOCH 20 - PROGRESS: at 27.07% examples, 142721 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 11:33:01: EPOCH 20 - PROGRESS: at 41.99% examples, 145867 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:02: EPOCH 20 - PROGRESS: at 52.44% examples, 137202 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:03: EPOCH 20 - PROGRESS: at 66.96% examples, 140428 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:04: EPOCH 20 - PROGRESS: at 78.55% examples, 137718 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:05: EPOCH 20 - PROGRESS: at 93.28% examples, 139722 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:05: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:05: EPOCH - 20 : training on 2056851 raw words (1062641 effective words) took 7.5s, 141716 effective words/s\n",
      "INFO - 11:33:06: EPOCH 21 - PROGRESS: at 15.95% examples, 164048 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:07: EPOCH 21 - PROGRESS: at 31.38% examples, 163204 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:08: EPOCH 21 - PROGRESS: at 45.34% examples, 156560 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:09: EPOCH 21 - PROGRESS: at 54.39% examples, 142093 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:10: EPOCH 21 - PROGRESS: at 68.48% examples, 142660 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:11: EPOCH 21 - PROGRESS: at 84.36% examples, 146457 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:12: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:12: EPOCH - 21 : training on 2056851 raw words (1062373 effective words) took 7.1s, 150519 effective words/s\n",
      "INFO - 11:33:13: EPOCH 22 - PROGRESS: at 15.48% examples, 161714 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:14: EPOCH 22 - PROGRESS: at 32.38% examples, 169087 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:15: EPOCH 22 - PROGRESS: at 48.65% examples, 170735 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:16: EPOCH 22 - PROGRESS: at 65.04% examples, 170656 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:17: EPOCH 22 - PROGRESS: at 80.95% examples, 170219 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:18: EPOCH 22 - PROGRESS: at 97.12% examples, 170116 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:19: EPOCH - 22 : training on 2056851 raw words (1062702 effective words) took 6.3s, 169798 effective words/s\n",
      "INFO - 11:33:20: EPOCH 23 - PROGRESS: at 15.01% examples, 158377 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:21: EPOCH 23 - PROGRESS: at 30.42% examples, 159186 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:22: EPOCH 23 - PROGRESS: at 45.34% examples, 157150 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:23: EPOCH 23 - PROGRESS: at 61.66% examples, 161169 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:24: EPOCH 23 - PROGRESS: at 77.15% examples, 161515 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:25: EPOCH 23 - PROGRESS: at 93.28% examples, 162681 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:25: EPOCH - 23 : training on 2056851 raw words (1063698 effective words) took 6.5s, 163203 effective words/s\n",
      "INFO - 11:33:26: EPOCH 24 - PROGRESS: at 15.48% examples, 162588 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:27: EPOCH 24 - PROGRESS: at 30.92% examples, 162406 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:28: EPOCH 24 - PROGRESS: at 47.21% examples, 163859 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:29: EPOCH 24 - PROGRESS: at 62.60% examples, 163712 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:30: EPOCH 24 - PROGRESS: at 73.77% examples, 153838 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:33:31: EPOCH 24 - PROGRESS: at 86.34% examples, 150167 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:32: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:32: EPOCH - 24 : training on 2056851 raw words (1063587 effective words) took 7.0s, 151267 effective words/s\n",
      "INFO - 11:33:33: EPOCH 25 - PROGRESS: at 10.66% examples, 110063 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:34: EPOCH 25 - PROGRESS: at 25.62% examples, 131057 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:35: EPOCH 25 - PROGRESS: at 40.05% examples, 137597 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:36: EPOCH 25 - PROGRESS: at 55.83% examples, 143934 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:37: EPOCH 25 - PROGRESS: at 72.33% examples, 149868 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:38: EPOCH 25 - PROGRESS: at 87.86% examples, 151723 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:39: EPOCH - 25 : training on 2056851 raw words (1063297 effective words) took 6.9s, 153439 effective words/s\n",
      "INFO - 11:33:40: EPOCH 26 - PROGRESS: at 15.95% examples, 166722 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:41: EPOCH 26 - PROGRESS: at 31.38% examples, 163465 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:42: EPOCH 26 - PROGRESS: at 46.71% examples, 163082 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:43: EPOCH 26 - PROGRESS: at 63.56% examples, 165710 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:44: EPOCH 26 - PROGRESS: at 79.98% examples, 167542 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:45: EPOCH 26 - PROGRESS: at 96.16% examples, 167812 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:33:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:45: EPOCH - 26 : training on 2056851 raw words (1063475 effective words) took 6.3s, 168496 effective words/s\n",
      "INFO - 11:33:47: EPOCH 27 - PROGRESS: at 15.95% examples, 169331 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:48: EPOCH 27 - PROGRESS: at 31.89% examples, 168827 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:49: EPOCH 27 - PROGRESS: at 47.70% examples, 167080 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:50: EPOCH 27 - PROGRESS: at 64.05% examples, 168264 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:51: EPOCH 27 - PROGRESS: at 79.98% examples, 168364 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:52: EPOCH 27 - PROGRESS: at 94.72% examples, 165177 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:52: EPOCH - 27 : training on 2056851 raw words (1063603 effective words) took 6.6s, 160638 effective words/s\n",
      "INFO - 11:33:53: EPOCH 28 - PROGRESS: at 12.62% examples, 130634 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:54: EPOCH 28 - PROGRESS: at 27.55% examples, 141715 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:55: EPOCH 28 - PROGRESS: at 42.49% examples, 146936 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:33:56: EPOCH 28 - PROGRESS: at 56.30% examples, 146318 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:57: EPOCH 28 - PROGRESS: at 68.48% examples, 142279 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:58: EPOCH 28 - PROGRESS: at 83.90% examples, 145442 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:33:59: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:33:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:33:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:33:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:33:59: EPOCH - 28 : training on 2056851 raw words (1062781 effective words) took 7.1s, 149397 effective words/s\n",
      "INFO - 11:34:00: EPOCH 29 - PROGRESS: at 14.51% examples, 152995 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:01: EPOCH 29 - PROGRESS: at 30.42% examples, 158848 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:02: EPOCH 29 - PROGRESS: at 46.27% examples, 160316 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:03: EPOCH 29 - PROGRESS: at 60.17% examples, 157249 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:04: EPOCH 29 - PROGRESS: at 70.90% examples, 148497 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:05: EPOCH 29 - PROGRESS: at 84.86% examples, 147998 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 11:34:06: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:34:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:34:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:34:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:34:06: EPOCH - 29 : training on 2056851 raw words (1062775 effective words) took 7.1s, 149946 effective words/s\n",
      "INFO - 11:34:07: EPOCH 30 - PROGRESS: at 13.53% examples, 141469 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:08: EPOCH 30 - PROGRESS: at 27.55% examples, 142067 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:09: EPOCH 30 - PROGRESS: at 41.99% examples, 145318 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:10: EPOCH 30 - PROGRESS: at 57.72% examples, 150223 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:11: EPOCH 30 - PROGRESS: at 73.26% examples, 152002 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:13: EPOCH 30 - PROGRESS: at 88.84% examples, 153942 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 11:34:13: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 11:34:13: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 11:34:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 11:34:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 11:34:13: EPOCH - 30 : training on 2056851 raw words (1063781 effective words) took 6.9s, 153677 effective words/s\n",
      "INFO - 11:34:13: training on a 61705530 raw words (31888033 effective words) took 208.2s, 153128 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 3.47 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 30, report_delay = 1)\n",
    "print('Time to build vocab: {} mins'.format(round((time()-t)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:34:24: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reconstruct': <gensim.models.keyedvectors.Vocab at 0x24aa65ffdf0>,\n",
       " 'subject': <gensim.models.keyedvectors.Vocab at 0x24aa65ffa90>,\n",
       " 'specif': <gensim.models.keyedvectors.Vocab at 0x24aa65ffbb0>,\n",
       " 'effect': <gensim.models.keyedvectors.Vocab at 0x24aa66fa310>,\n",
       " 'map': <gensim.models.keyedvectors.Vocab at 0x24aa66fa640>,\n",
       " 'predict': <gensim.models.keyedvectors.Vocab at 0x24aa66faf10>,\n",
       " 'model': <gensim.models.keyedvectors.Vocab at 0x24aa66fac40>,\n",
       " 'allow': <gensim.models.keyedvectors.Vocab at 0x24aa66fa940>,\n",
       " 'infer': <gensim.models.keyedvectors.Vocab at 0x24aa66faa60>,\n",
       " 'analyz': <gensim.models.keyedvectors.Vocab at 0x24aa66faa90>,\n",
       " 'diseas': <gensim.models.keyedvectors.Vocab at 0x24aa66fa0d0>,\n",
       " 'relat': <gensim.models.keyedvectors.Vocab at 0x24aa66fa3d0>,\n",
       " 'alter': <gensim.models.keyedvectors.Vocab at 0x24aa66fa220>,\n",
       " 'neuroimag': <gensim.models.keyedvectors.Vocab at 0x24aa66fa820>,\n",
       " 'data': <gensim.models.keyedvectors.Vocab at 0x24aa66fab20>,\n",
       " 'given': <gensim.models.keyedvectors.Vocab at 0x24aa66fa8e0>,\n",
       " 'made': <gensim.models.keyedvectors.Vocab at 0x24aa66faca0>,\n",
       " 'two': <gensim.models.keyedvectors.Vocab at 0x24aa66fa730>,\n",
       " 'level': <gensim.models.keyedvectors.Vocab at 0x24aa66fa760>,\n",
       " 'global': <gensim.models.keyedvectors.Vocab at 0x24aa66fa040>,\n",
       " 'e': <gensim.models.keyedvectors.Vocab at 0x24aa66fa3a0>,\n",
       " 'condit': <gensim.models.keyedvectors.Vocab at 0x24aa66fa520>,\n",
       " 'presenc': <gensim.models.keyedvectors.Vocab at 0x24aa66fa7c0>,\n",
       " 'local': <gensim.models.keyedvectors.Vocab at 0x24aa66fa700>,\n",
       " 'detect': <gensim.models.keyedvectors.Vocab at 0x24aa66fa370>,\n",
       " 'individu': <gensim.models.keyedvectors.Vocab at 0x24aa66fa610>,\n",
       " 'measur': <gensim.models.keyedvectors.Vocab at 0x24aa66fae20>,\n",
       " 'extract': <gensim.models.keyedvectors.Vocab at 0x24aa66fa250>,\n",
       " 'wide_use': <gensim.models.keyedvectors.Vocab at 0x24aa66fac10>,\n",
       " 'use': <gensim.models.keyedvectors.Vocab at 0x24aa66fa1c0>,\n",
       " 'form': <gensim.models.keyedvectors.Vocab at 0x24aa66fae80>,\n",
       " 'rare': <gensim.models.keyedvectors.Vocab at 0x24aa66fabb0>,\n",
       " 'exist': <gensim.models.keyedvectors.Vocab at 0x24aa66fa9d0>,\n",
       " 'often': <gensim.models.keyedvectors.Vocab at 0x24aa66faee0>,\n",
       " 'yield': <gensim.models.keyedvectors.Vocab at 0x24aa66facd0>,\n",
       " 'noisi': <gensim.models.keyedvectors.Vocab at 0x24aa66fa850>,\n",
       " 'compos': <gensim.models.keyedvectors.Vocab at 0x24aa66fa340>,\n",
       " 'dispers': <gensim.models.keyedvectors.Vocab at 0x24aa66fac70>,\n",
       " 'isol': <gensim.models.keyedvectors.Vocab at 0x24aa66fad90>,\n",
       " 'island': <gensim.models.keyedvectors.Vocab at 0x24aa66fa880>,\n",
       " 'articl': <gensim.models.keyedvectors.Vocab at 0x24aa66fa970>,\n",
       " 'propos': <gensim.models.keyedvectors.Vocab at 0x24aa66fa400>,\n",
       " 'method': <gensim.models.keyedvectors.Vocab at 0x24aa66fad60>,\n",
       " 'name': <gensim.models.keyedvectors.Vocab at 0x24aa66fa8b0>,\n",
       " 'rsm': <gensim.models.keyedvectors.Vocab at 0x24aa66fa4f0>,\n",
       " 'improv': <gensim.models.keyedvectors.Vocab at 0x24aa65fc160>,\n",
       " 'approach': <gensim.models.keyedvectors.Vocab at 0x24aa65fc970>,\n",
       " 'particular': <gensim.models.keyedvectors.Vocab at 0x24aa65fce50>,\n",
       " 'binari': <gensim.models.keyedvectors.Vocab at 0x24aa65fcfd0>,\n",
       " 'classifi': <gensim.models.keyedvectors.Vocab at 0x24aa65fceb0>,\n",
       " 'aim': <gensim.models.keyedvectors.Vocab at 0x24aa65fcbb0>,\n",
       " 'reduc': <gensim.models.keyedvectors.Vocab at 0x24aa65fcbe0>,\n",
       " 'nois': <gensim.models.keyedvectors.Vocab at 0x24aa65fcc10>,\n",
       " 'due': <gensim.models.keyedvectors.Vocab at 0x24aa65fcee0>,\n",
       " 'sampl': <gensim.models.keyedvectors.Vocab at 0x24aa65fcd90>,\n",
       " 'error': <gensim.models.keyedvectors.Vocab at 0x24aa65fce20>,\n",
       " 'associ': <gensim.models.keyedvectors.Vocab at 0x24aa65fcdf0>,\n",
       " 'finit': <gensim.models.keyedvectors.Vocab at 0x24aa65fcac0>,\n",
       " 'exampl': <gensim.models.keyedvectors.Vocab at 0x24aa65fc0d0>,\n",
       " 'train': <gensim.models.keyedvectors.Vocab at 0x24aa65fcf10>,\n",
       " 'wrapper': <gensim.models.keyedvectors.Vocab at 0x24aa65fca00>,\n",
       " 'type': <gensim.models.keyedvectors.Vocab at 0x24aa65fcdc0>,\n",
       " 'algorithm': <gensim.models.keyedvectors.Vocab at 0x24aa65fccd0>,\n",
       " 'differ': <gensim.models.keyedvectors.Vocab at 0x24aa844e040>,\n",
       " 'diagnost': <gensim.models.keyedvectors.Vocab at 0x24aa844e0a0>,\n",
       " 'manner': <gensim.models.keyedvectors.Vocab at 0x24aa844e100>,\n",
       " 'without': <gensim.models.keyedvectors.Vocab at 0x24aa844e160>,\n",
       " 'inform': <gensim.models.keyedvectors.Vocab at 0x24aa844e1c0>,\n",
       " 'pose': <gensim.models.keyedvectors.Vocab at 0x24aa844e220>,\n",
       " 'maximum': <gensim.models.keyedvectors.Vocab at 0x24aa844e280>,\n",
       " 'posteriori': <gensim.models.keyedvectors.Vocab at 0x24aa844e2e0>,\n",
       " 'problem': <gensim.models.keyedvectors.Vocab at 0x24aa844e340>,\n",
       " 'prior': <gensim.models.keyedvectors.Vocab at 0x24aa844e3a0>,\n",
       " 'whose': <gensim.models.keyedvectors.Vocab at 0x24aa844e400>,\n",
       " 'paramet': <gensim.models.keyedvectors.Vocab at 0x24aa844e460>,\n",
       " 'estim': <gensim.models.keyedvectors.Vocab at 0x24aa844e4c0>,\n",
       " 'fashion': <gensim.models.keyedvectors.Vocab at 0x24aa844e520>,\n",
       " 'experiment': <gensim.models.keyedvectors.Vocab at 0x24aa844e580>,\n",
       " 'evalu': <gensim.models.keyedvectors.Vocab at 0x24aa844e5e0>,\n",
       " 'perform': <gensim.models.keyedvectors.Vocab at 0x24aa844e640>,\n",
       " 'synthet': <gensim.models.keyedvectors.Vocab at 0x24aa844e6a0>,\n",
       " 'generat': <gensim.models.keyedvectors.Vocab at 0x24aa844e700>,\n",
       " 'alzheim': <gensim.models.keyedvectors.Vocab at 0x24aa844e760>,\n",
       " 'initi': <gensim.models.keyedvectors.Vocab at 0x24aa844e7c0>,\n",
       " 'adni': <gensim.models.keyedvectors.Vocab at 0x24aa844e820>,\n",
       " 'databas': <gensim.models.keyedvectors.Vocab at 0x24aa844e880>,\n",
       " 'result': <gensim.models.keyedvectors.Vocab at 0x24aa844e8e0>,\n",
       " 'synthet_data': <gensim.models.keyedvectors.Vocab at 0x24aa844e940>,\n",
       " 'demonstr': <gensim.models.keyedvectors.Vocab at 0x24aa844e9a0>,\n",
       " 'higher': <gensim.models.keyedvectors.Vocab at 0x24aa844ea00>,\n",
       " 'accuraci': <gensim.models.keyedvectors.Vocab at 0x24aa844ea60>,\n",
       " 'compar': <gensim.models.keyedvectors.Vocab at 0x24aa844eac0>,\n",
       " 'direct': <gensim.models.keyedvectors.Vocab at 0x24aa844eb20>,\n",
       " 'bootstrap': <gensim.models.keyedvectors.Vocab at 0x24aa844eb80>,\n",
       " 'averag': <gensim.models.keyedvectors.Vocab at 0x24aa844ebe0>,\n",
       " 'analys': <gensim.models.keyedvectors.Vocab at 0x24aa844ec40>,\n",
       " 'dataset': <gensim.models.keyedvectors.Vocab at 0x24aa844eca0>,\n",
       " 'show': <gensim.models.keyedvectors.Vocab at 0x24aa844ed00>,\n",
       " 'also': <gensim.models.keyedvectors.Vocab at 0x24aa844ed60>,\n",
       " 'correl': <gensim.models.keyedvectors.Vocab at 0x24aa844edc0>,\n",
       " 'cortic': <gensim.models.keyedvectors.Vocab at 0x24aa844ee20>,\n",
       " 'thick': <gensim.models.keyedvectors.Vocab at 0x24aa844ee80>,\n",
       " 'non': <gensim.models.keyedvectors.Vocab at 0x24aa844eee0>,\n",
       " 'imag': <gensim.models.keyedvectors.Vocab at 0x24aa844ef40>,\n",
       " 'marker': <gensim.models.keyedvectors.Vocab at 0x24aa844efa0>,\n",
       " 'ad': <gensim.models.keyedvectors.Vocab at 0x24aa8452040>,\n",
       " 'mini': <gensim.models.keyedvectors.Vocab at 0x24aa84520a0>,\n",
       " 'mental': <gensim.models.keyedvectors.Vocab at 0x24aa8452100>,\n",
       " 'state': <gensim.models.keyedvectors.Vocab at 0x24aa8452160>,\n",
       " 'examin': <gensim.models.keyedvectors.Vocab at 0x24aa84521c0>,\n",
       " 'score': <gensim.models.keyedvectors.Vocab at 0x24aa8452220>,\n",
       " 'fluid': <gensim.models.keyedvectors.Vocab at 0x24aa8452280>,\n",
       " 'amyloid': <gensim.models.keyedvectors.Vocab at 0x24aa84522e0>,\n",
       " 'beta': <gensim.models.keyedvectors.Vocab at 0x24aa8452340>,\n",
       " 'reliabl': <gensim.models.keyedvectors.Vocab at 0x24aa84523a0>,\n",
       " 'studi': <gensim.models.keyedvectors.Vocab at 0x24aa8452400>,\n",
       " 'longitudin': <gensim.models.keyedvectors.Vocab at 0x24aa8452460>,\n",
       " 'rotat': <gensim.models.keyedvectors.Vocab at 0x24aa84524c0>,\n",
       " 'invari': <gensim.models.keyedvectors.Vocab at 0x24aa8452520>,\n",
       " 'neural_network': <gensim.models.keyedvectors.Vocab at 0x24aa8452580>,\n",
       " 'translat': <gensim.models.keyedvectors.Vocab at 0x24aa84525e0>,\n",
       " 'great': <gensim.models.keyedvectors.Vocab at 0x24aa8452640>,\n",
       " 'valu': <gensim.models.keyedvectors.Vocab at 0x24aa84526a0>,\n",
       " 'recognit': <gensim.models.keyedvectors.Vocab at 0x24aa8452700>,\n",
       " 'task': <gensim.models.keyedvectors.Vocab at 0x24aa8452760>,\n",
       " 'paper': <gensim.models.keyedvectors.Vocab at 0x24aa84527c0>,\n",
       " 'bring': <gensim.models.keyedvectors.Vocab at 0x24aa8452820>,\n",
       " 'new': <gensim.models.keyedvectors.Vocab at 0x24aa8452880>,\n",
       " 'architectur': <gensim.models.keyedvectors.Vocab at 0x24aa84528e0>,\n",
       " 'convolut_neural': <gensim.models.keyedvectors.Vocab at 0x24aa8452940>,\n",
       " 'network_cnn': <gensim.models.keyedvectors.Vocab at 0x24aa84529a0>,\n",
       " 'cyclic': <gensim.models.keyedvectors.Vocab at 0x24aa8452a00>,\n",
       " 'convolut': <gensim.models.keyedvectors.Vocab at 0x24aa8452a60>,\n",
       " 'layer': <gensim.models.keyedvectors.Vocab at 0x24aa8452ac0>,\n",
       " 'achiev': <gensim.models.keyedvectors.Vocab at 0x24aa8452b20>,\n",
       " 'symbol': <gensim.models.keyedvectors.Vocab at 0x24aa8452b80>,\n",
       " 'get': <gensim.models.keyedvectors.Vocab at 0x24aa8452be0>,\n",
       " 'posit': <gensim.models.keyedvectors.Vocab at 0x24aa8452c40>,\n",
       " 'orient': <gensim.models.keyedvectors.Vocab at 0x24aa8452ca0>,\n",
       " 'network': <gensim.models.keyedvectors.Vocab at 0x24aa8452d00>,\n",
       " 'purpos': <gensim.models.keyedvectors.Vocab at 0x24aa8452d60>,\n",
       " 'multipl': <gensim.models.keyedvectors.Vocab at 0x24aa8452dc0>,\n",
       " 'overlap': <gensim.models.keyedvectors.Vocab at 0x24aa8452e20>,\n",
       " 'target': <gensim.models.keyedvectors.Vocab at 0x24aa8452e80>,\n",
       " 'last': <gensim.models.keyedvectors.Vocab at 0x24aa8452ee0>,\n",
       " 'least': <gensim.models.keyedvectors.Vocab at 0x24aa8452f40>,\n",
       " 'one': <gensim.models.keyedvectors.Vocab at 0x24aa8452fa0>,\n",
       " 'shot_learn': <gensim.models.keyedvectors.Vocab at 0x24aa8453040>,\n",
       " 'case': <gensim.models.keyedvectors.Vocab at 0x24aa84530a0>,\n",
       " 'spheric': <gensim.models.keyedvectors.Vocab at 0x24aa8453100>,\n",
       " 'polyharmon': <gensim.models.keyedvectors.Vocab at 0x24acf4c7a00>,\n",
       " 'poisson': <gensim.models.keyedvectors.Vocab at 0x24aa65ff7c0>,\n",
       " 'kernel': <gensim.models.keyedvectors.Vocab at 0x24aa65f9f10>,\n",
       " 'function': <gensim.models.keyedvectors.Vocab at 0x24aa8453160>,\n",
       " 'introduc': <gensim.models.keyedvectors.Vocab at 0x24aa84531c0>,\n",
       " 'develop': <gensim.models.keyedvectors.Vocab at 0x24aa8453220>,\n",
       " 'notion': <gensim.models.keyedvectors.Vocab at 0x24aa8453280>,\n",
       " 'natur': <gensim.models.keyedvectors.Vocab at 0x24aa84532e0>,\n",
       " 'generalis': <gensim.models.keyedvectors.Vocab at 0x24aa8453340>,\n",
       " 'harmon': <gensim.models.keyedvectors.Vocab at 0x24aa84533a0>,\n",
       " 'theori': <gensim.models.keyedvectors.Vocab at 0x24aa8453400>,\n",
       " 'zonal': <gensim.models.keyedvectors.Vocab at 0x24aa8453460>,\n",
       " 'allow_us': <gensim.models.keyedvectors.Vocab at 0x24aa84534c0>,\n",
       " 'analog': <gensim.models.keyedvectors.Vocab at 0x24aa8453520>,\n",
       " 'construct': <gensim.models.keyedvectors.Vocab at 0x24aa8453580>,\n",
       " 'union': <gensim.models.keyedvectors.Vocab at 0x24aa84535e0>,\n",
       " 'ball': <gensim.models.keyedvectors.Vocab at 0x24aa8453640>,\n",
       " 'find': <gensim.models.keyedvectors.Vocab at 0x24aa84536a0>,\n",
       " 'represent': <gensim.models.keyedvectors.Vocab at 0x24aa8453700>,\n",
       " 'term': <gensim.models.keyedvectors.Vocab at 0x24aa8453760>,\n",
       " 'gegenbau': <gensim.models.keyedvectors.Vocab at 0x24aa84537c0>,\n",
       " 'polynomi': <gensim.models.keyedvectors.Vocab at 0x24aa8453820>,\n",
       " 'connect': <gensim.models.keyedvectors.Vocab at 0x24aa8453880>,\n",
       " 'classic': <gensim.models.keyedvectors.Vocab at 0x24aa84538e0>,\n",
       " 'cauchi': <gensim.models.keyedvectors.Vocab at 0x24aa8453940>,\n",
       " 'holomorph': <gensim.models.keyedvectors.Vocab at 0x24aa84539a0>,\n",
       " 'lie': <gensim.models.keyedvectors.Vocab at 0x24aa8453a00>,\n",
       " 'finit_element': <gensim.models.keyedvectors.Vocab at 0x24aa8453a60>,\n",
       " 'approxim': <gensim.models.keyedvectors.Vocab at 0x24aa8453ac0>,\n",
       " 'stochast': <gensim.models.keyedvectors.Vocab at 0x24aa8453b20>,\n",
       " 'maxwel': <gensim.models.keyedvectors.Vocab at 0x24aa8453b80>,\n",
       " 'landau': <gensim.models.keyedvectors.Vocab at 0x24aa8453be0>,\n",
       " 'lifshitz': <gensim.models.keyedvectors.Vocab at 0x24aa8453c40>,\n",
       " 'gilbert': <gensim.models.keyedvectors.Vocab at 0x24aa8453ca0>,\n",
       " 'system': <gensim.models.keyedvectors.Vocab at 0x24aa8453d00>,\n",
       " 'equat': <gensim.models.keyedvectors.Vocab at 0x24aa8453d60>,\n",
       " 'coupl': <gensim.models.keyedvectors.Vocab at 0x24aa8453dc0>,\n",
       " 'call': <gensim.models.keyedvectors.Vocab at 0x24aa8453e20>,\n",
       " 'describ': <gensim.models.keyedvectors.Vocab at 0x24aa8453e80>,\n",
       " 'creation': <gensim.models.keyedvectors.Vocab at 0x24aa8453ee0>,\n",
       " 'domain_wall': <gensim.models.keyedvectors.Vocab at 0x24aa8453f40>,\n",
       " 'vortic': <gensim.models.keyedvectors.Vocab at 0x24aa8453fa0>,\n",
       " 'fundament': <gensim.models.keyedvectors.Vocab at 0x24aa8454040>,\n",
       " 'object': <gensim.models.keyedvectors.Vocab at 0x24aa84540a0>,\n",
       " 'novel': <gensim.models.keyedvectors.Vocab at 0x24aa8454100>,\n",
       " 'nanostructur': <gensim.models.keyedvectors.Vocab at 0x24aa8454160>,\n",
       " 'magnet': <gensim.models.keyedvectors.Vocab at 0x24aa84541c0>,\n",
       " 'memori': <gensim.models.keyedvectors.Vocab at 0x24aa8454220>,\n",
       " 'first': <gensim.models.keyedvectors.Vocab at 0x24aa8454280>,\n",
       " 'reformul': <gensim.models.keyedvectors.Vocab at 0x24aa84542e0>,\n",
       " 'time': <gensim.models.keyedvectors.Vocab at 0x24aa8454340>,\n",
       " 'differenti': <gensim.models.keyedvectors.Vocab at 0x24aa84543a0>,\n",
       " 'solut': <gensim.models.keyedvectors.Vocab at 0x24aa8454400>,\n",
       " 'converg': <gensim.models.keyedvectors.Vocab at 0x24aa8454460>,\n",
       " 'theta': <gensim.models.keyedvectors.Vocab at 0x24aa84544c0>,\n",
       " 'linear': <gensim.models.keyedvectors.Vocab at 0x24aa8454520>,\n",
       " 'scheme': <gensim.models.keyedvectors.Vocab at 0x24aa8454580>,\n",
       " 'consequ': <gensim.models.keyedvectors.Vocab at 0x24aa84545e0>,\n",
       " 'prove': <gensim.models.keyedvectors.Vocab at 0x24aa8454640>,\n",
       " 'minor': <gensim.models.keyedvectors.Vocab at 0x24aa84546a0>,\n",
       " 'space': <gensim.models.keyedvectors.Vocab at 0x24aa8454700>,\n",
       " 'step': <gensim.models.keyedvectors.Vocab at 0x24aa8454760>,\n",
       " 'depend': <gensim.models.keyedvectors.Vocab at 0x24aa84547c0>,\n",
       " 'henc': <gensim.models.keyedvectors.Vocab at 0x24aa8454820>,\n",
       " 'weak': <gensim.models.keyedvectors.Vocab at 0x24aa8454880>,\n",
       " 'martingal': <gensim.models.keyedvectors.Vocab at 0x24aa84548e0>,\n",
       " 'numer': <gensim.models.keyedvectors.Vocab at 0x24aa8454940>,\n",
       " 'present': <gensim.models.keyedvectors.Vocab at 0x24aa84549a0>,\n",
       " 'applic': <gensim.models.keyedvectors.Vocab at 0x24aa8454a00>,\n",
       " 'discret': <gensim.models.keyedvectors.Vocab at 0x24aa8454a60>,\n",
       " 'wavelet': <gensim.models.keyedvectors.Vocab at 0x24aa8454ac0>,\n",
       " 'transform': <gensim.models.keyedvectors.Vocab at 0x24aa8454b20>,\n",
       " 'tensor': <gensim.models.keyedvectors.Vocab at 0x24aa8454b80>,\n",
       " 'decomposit': <gensim.models.keyedvectors.Vocab at 0x24aa8454be0>,\n",
       " 'featur_extract': <gensim.models.keyedvectors.Vocab at 0x24aa8454c40>,\n",
       " 'ftir': <gensim.models.keyedvectors.Vocab at 0x24aa8454ca0>,\n",
       " 'medicin': <gensim.models.keyedvectors.Vocab at 0x24aa8454d00>,\n",
       " 'plant': <gensim.models.keyedvectors.Vocab at 0x24aa8454d60>,\n",
       " 'fourier_transform': <gensim.models.keyedvectors.Vocab at 0x24aa8454dc0>,\n",
       " 'infra': <gensim.models.keyedvectors.Vocab at 0x24aa8454e20>,\n",
       " 'red': <gensim.models.keyedvectors.Vocab at 0x24aa8454e80>,\n",
       " 'spectra': <gensim.models.keyedvectors.Vocab at 0x24aa8454ee0>,\n",
       " 'speci': <gensim.models.keyedvectors.Vocab at 0x24aa8454f40>,\n",
       " 'explor': <gensim.models.keyedvectors.Vocab at 0x24aa8454fa0>,\n",
       " 'influenc': <gensim.models.keyedvectors.Vocab at 0x24aa8455040>,\n",
       " 'preprocess': <gensim.models.keyedvectors.Vocab at 0x24aa84550a0>,\n",
       " 'effici': <gensim.models.keyedvectors.Vocab at 0x24aa8455100>,\n",
       " 'machin_learn': <gensim.models.keyedvectors.Vocab at 0x24aa8455160>,\n",
       " 'dwt': <gensim.models.keyedvectors.Vocab at 0x24aa84551c0>,\n",
       " 'techniqu': <gensim.models.keyedvectors.Vocab at 0x24aa8455220>,\n",
       " 'various': <gensim.models.keyedvectors.Vocab at 0x24aa8455280>,\n",
       " 'combin': <gensim.models.keyedvectors.Vocab at 0x24aa84552e0>,\n",
       " 'signal': <gensim.models.keyedvectors.Vocab at 0x24aa8455340>,\n",
       " 'process': <gensim.models.keyedvectors.Vocab at 0x24aa84553a0>,\n",
       " 'behavior': <gensim.models.keyedvectors.Vocab at 0x24aa8455400>,\n",
       " 'appli': <gensim.models.keyedvectors.Vocab at 0x24aa8455460>,\n",
       " 'classif': <gensim.models.keyedvectors.Vocab at 0x24aa84554c0>,\n",
       " 'cluster': <gensim.models.keyedvectors.Vocab at 0x24aa8455520>,\n",
       " 'best': <gensim.models.keyedvectors.Vocab at 0x24aa8455580>,\n",
       " 'found': <gensim.models.keyedvectors.Vocab at 0x24aa84555e0>,\n",
       " 'grid': <gensim.models.keyedvectors.Vocab at 0x24aa8455640>,\n",
       " 'search': <gensim.models.keyedvectors.Vocab at 0x24aa84556a0>,\n",
       " 'similar': <gensim.models.keyedvectors.Vocab at 0x24aa8455700>,\n",
       " 'signific_improv': <gensim.models.keyedvectors.Vocab at 0x24aa8455760>,\n",
       " 'qualiti': <gensim.models.keyedvectors.Vocab at 0x24aa84557c0>,\n",
       " 'well': <gensim.models.keyedvectors.Vocab at 0x24aa8455820>,\n",
       " 'classif_accuraci': <gensim.models.keyedvectors.Vocab at 0x24aa8455880>,\n",
       " 'tune': <gensim.models.keyedvectors.Vocab at 0x24aa84558e0>,\n",
       " 'logist_regress': <gensim.models.keyedvectors.Vocab at 0x24aa8455940>,\n",
       " 'comparison': <gensim.models.keyedvectors.Vocab at 0x24aa84559a0>,\n",
       " 'origin': <gensim.models.keyedvectors.Vocab at 0x24aa8455a00>,\n",
       " 'unlik': <gensim.models.keyedvectors.Vocab at 0x24aa8455a60>,\n",
       " 'rank': <gensim.models.keyedvectors.Vocab at 0x24aa8455ac0>,\n",
       " 'make': <gensim.models.keyedvectors.Vocab at 0x24aa8455b20>,\n",
       " 'versatil': <gensim.models.keyedvectors.Vocab at 0x24aa8455b80>,\n",
       " 'easier': <gensim.models.keyedvectors.Vocab at 0x24aa8455be0>,\n",
       " 'tool': <gensim.models.keyedvectors.Vocab at 0x24aa8455c40>,\n",
       " 'maxim': <gensim.models.keyedvectors.Vocab at 0x24aa8455ca0>,\n",
       " 'frequenc': <gensim.models.keyedvectors.Vocab at 0x24aa8455d00>,\n",
       " 'complement': <gensim.models.keyedvectors.Vocab at 0x24aa8455d60>,\n",
       " 'obstacl': <gensim.models.keyedvectors.Vocab at 0x24aa8455dc0>,\n",
       " 'let': <gensim.models.keyedvectors.Vocab at 0x24aa8455e20>,\n",
       " 'omega': <gensim.models.keyedvectors.Vocab at 0x24aa8455e80>,\n",
       " 'subset_mathbb': <gensim.models.keyedvectors.Vocab at 0x24aa8455ee0>,\n",
       " 'r_n': <gensim.models.keyedvectors.Vocab at 0x24aa8455f40>,\n",
       " 'bound': <gensim.models.keyedvectors.Vocab at 0x24aa8455fa0>,\n",
       " 'domain': <gensim.models.keyedvectors.Vocab at 0x24aa8456040>,\n",
       " 'satisfi': <gensim.models.keyedvectors.Vocab at 0x24aa84560a0>,\n",
       " 'asymmetri': <gensim.models.keyedvectors.Vocab at 0x24aa8456100>,\n",
       " 'arbitrari': <gensim.models.keyedvectors.Vocab at 0x24aa8456160>,\n",
       " 'refer': <gensim.models.keyedvectors.Vocab at 0x24aa84561c0>,\n",
       " 'interest': <gensim.models.keyedvectors.Vocab at 0x24aa8456220>,\n",
       " 'behaviour': <gensim.models.keyedvectors.Vocab at 0x24aa8456280>,\n",
       " 'dirichlet': <gensim.models.keyedvectors.Vocab at 0x24aa84562e0>,\n",
       " 'eigenvalu': <gensim.models.keyedvectors.Vocab at 0x24aa8456340>,\n",
       " 'lambda': <gensim.models.keyedvectors.Vocab at 0x24aa84563a0>,\n",
       " 'setminus': <gensim.models.keyedvectors.Vocab at 0x24aa8456400>,\n",
       " 'x': <gensim.models.keyedvectors.Vocab at 0x24aa8456460>,\n",
       " 'upper_bound': <gensim.models.keyedvectors.Vocab at 0x24aa84564c0>,\n",
       " 'distanc': <gensim.models.keyedvectors.Vocab at 0x24aa8456520>,\n",
       " 'set': <gensim.models.keyedvectors.Vocab at 0x24aa8456580>,\n",
       " 'point': <gensim.models.keyedvectors.Vocab at 0x24aa84565e0>,\n",
       " 'ground_state': <gensim.models.keyedvectors.Vocab at 0x24aa8456640>,\n",
       " 'phi': <gensim.models.keyedvectors.Vocab at 0x24aa84566a0>,\n",
       " 'short': <gensim.models.keyedvectors.Vocab at 0x24aa8456700>,\n",
       " 'corollari': <gensim.models.keyedvectors.Vocab at 0x24aa8456760>,\n",
       " 'begin': <gensim.models.keyedvectors.Vocab at 0x24aa84567c0>,\n",
       " 'mu': <gensim.models.keyedvectors.Vocab at 0x24aa8456820>,\n",
       " 'max': <gensim.models.keyedvectors.Vocab at 0x24aa8456880>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x24aa84568e0>,\n",
       " 'larg': <gensim.models.keyedvectors.Vocab at 0x24aa8456940>,\n",
       " 'enough': <gensim.models.keyedvectors.Vocab at 0x24aa84569a0>,\n",
       " 'close': <gensim.models.keyedvectors.Vocab at 0x24aa8456a00>,\n",
       " 'second': <gensim.models.keyedvectors.Vocab at 0x24aa8456a60>,\n",
       " 'discuss': <gensim.models.keyedvectors.Vocab at 0x24aa8456ac0>,\n",
       " 'distribut': <gensim.models.keyedvectors.Vocab at 0x24aa8456b20>,\n",
       " 'possibl': <gensim.models.keyedvectors.Vocab at 0x24aa8456b80>,\n",
       " 'inscrib': <gensim.models.keyedvectors.Vocab at 0x24aa8456be0>,\n",
       " 'wavelength': <gensim.models.keyedvectors.Vocab at 0x24aa8456c40>,\n",
       " 'final': <gensim.models.keyedvectors.Vocab at 0x24aa8456ca0>,\n",
       " 'specifi': <gensim.models.keyedvectors.Vocab at 0x24aa8456d00>,\n",
       " 'observ': <gensim.models.keyedvectors.Vocab at 0x24aa8456d60>,\n",
       " 'convex': <gensim.models.keyedvectors.Vocab at 0x24aa8456dc0>,\n",
       " 'suffici': <gensim.models.keyedvectors.Vocab at 0x24aa8456e20>,\n",
       " 'respect': <gensim.models.keyedvectors.Vocab at 0x24aa8456e80>,\n",
       " 'contain': <gensim.models.keyedvectors.Vocab at 0x24aa8456ee0>,\n",
       " 'period': <gensim.models.keyedvectors.Vocab at 0x24aa8456f40>,\n",
       " 'shape': <gensim.models.keyedvectors.Vocab at 0x24aa8456fa0>,\n",
       " 'hyperbol': <gensim.models.keyedvectors.Vocab at 0x24aa8457040>,\n",
       " 'asteroid': <gensim.models.keyedvectors.Vocab at 0x24aa84570a0>,\n",
       " 'oumuamua': <gensim.models.keyedvectors.Vocab at 0x24aa8457100>,\n",
       " 'u': <gensim.models.keyedvectors.Vocab at 0x24aa8457160>,\n",
       " 'lightcurv': <gensim.models.keyedvectors.Vocab at 0x24aa84571c0>,\n",
       " 'newli': <gensim.models.keyedvectors.Vocab at 0x24aa8457220>,\n",
       " 'discov': <gensim.models.keyedvectors.Vocab at 0x24aa8457280>,\n",
       " 'planet': <gensim.models.keyedvectors.Vocab at 0x24aa84572e0>,\n",
       " 'octob': <gensim.models.keyedvectors.Vocab at 0x24aa8457340>,\n",
       " 'observatori': <gensim.models.keyedvectors.Vocab at 0x24aa84573a0>,\n",
       " 'discoveri': <gensim.models.keyedvectors.Vocab at 0x24aa8457400>,\n",
       " 'channel': <gensim.models.keyedvectors.Vocab at 0x24aa8457460>,\n",
       " 'telescop': <gensim.models.keyedvectors.Vocab at 0x24aa84574c0>,\n",
       " 'deriv': <gensim.models.keyedvectors.Vocab at 0x24aa8457520>,\n",
       " 'partial': <gensim.models.keyedvectors.Vocab at 0x24aa8457580>,\n",
       " 'peak': <gensim.models.keyedvectors.Vocab at 0x24aa84575e0>,\n",
       " 'trough': <gensim.models.keyedvectors.Vocab at 0x24aa8457640>,\n",
       " 'amplitud': <gensim.models.keyedvectors.Vocab at 0x24aa84576a0>,\n",
       " 'mag': <gensim.models.keyedvectors.Vocab at 0x24aa8457700>,\n",
       " 'segment': <gensim.models.keyedvectors.Vocab at 0x24aa8457760>,\n",
       " 'rule': <gensim.models.keyedvectors.Vocab at 0x24aa84577c0>,\n",
       " 'less': <gensim.models.keyedvectors.Vocab at 0x24aa8457820>,\n",
       " 'hr': <gensim.models.keyedvectors.Vocab at 0x24aa8457880>,\n",
       " 'suggest': <gensim.models.keyedvectors.Vocab at 0x24aa84578e0>,\n",
       " 'assumpt': <gensim.models.keyedvectors.Vocab at 0x24aa8457940>,\n",
       " 'variabl': <gensim.models.keyedvectors.Vocab at 0x24aa84579a0>,\n",
       " 'chang': <gensim.models.keyedvectors.Vocab at 0x24aa8457a00>,\n",
       " 'cross_section': <gensim.models.keyedvectors.Vocab at 0x24aa8457a60>,\n",
       " 'axial': <gensim.models.keyedvectors.Vocab at 0x24aa8457ac0>,\n",
       " 'ratio': <gensim.models.keyedvectors.Vocab at 0x24aa8457b20>,\n",
       " 'saw': <gensim.models.keyedvectors.Vocab at 0x24aa8457b80>,\n",
       " 'evid': <gensim.models.keyedvectors.Vocab at 0x24aa8457be0>,\n",
       " 'coma': <gensim.models.keyedvectors.Vocab at 0x24aa8457c40>,\n",
       " 'tail': <gensim.models.keyedvectors.Vocab at 0x24aa8457ca0>,\n",
       " 'either': <gensim.models.keyedvectors.Vocab at 0x24aa8457d00>,\n",
       " 'stack': <gensim.models.keyedvectors.Vocab at 0x24aa8457d60>,\n",
       " 'equival': <gensim.models.keyedvectors.Vocab at 0x24aa8457dc0>,\n",
       " 'exposur': <gensim.models.keyedvectors.Vocab at 0x24aa8457e20>,\n",
       " 'advers': <gensim.models.keyedvectors.Vocab at 0x24aa8457e80>,\n",
       " 'polym': <gensim.models.keyedvectors.Vocab at 0x24aa8457ee0>,\n",
       " 'coat': <gensim.models.keyedvectors.Vocab at 0x24aa8457f40>,\n",
       " 'heat': <gensim.models.keyedvectors.Vocab at 0x24aa8457fa0>,\n",
       " 'transport': <gensim.models.keyedvectors.Vocab at 0x24aa8459040>,\n",
       " 'solid': <gensim.models.keyedvectors.Vocab at 0x24aa84590a0>,\n",
       " 'liquid': <gensim.models.keyedvectors.Vocab at 0x24aa8459100>,\n",
       " 'interfac': <gensim.models.keyedvectors.Vocab at 0x24aa8459160>,\n",
       " 'abil': <gensim.models.keyedvectors.Vocab at 0x24aa84591c0>,\n",
       " 'metal': <gensim.models.keyedvectors.Vocab at 0x24aa8459220>,\n",
       " 'nanoparticl': <gensim.models.keyedvectors.Vocab at 0x24aa8459280>,\n",
       " 'suppli': <gensim.models.keyedvectors.Vocab at 0x24aa84592e0>,\n",
       " 'environ': <gensim.models.keyedvectors.Vocab at 0x24aa8459340>,\n",
       " 'extern': <gensim.models.keyedvectors.Vocab at 0x24aa84593a0>,\n",
       " 'optic': <gensim.models.keyedvectors.Vocab at 0x24aa8459400>,\n",
       " 'field': <gensim.models.keyedvectors.Vocab at 0x24aa8459460>,\n",
       " 'attract': <gensim.models.keyedvectors.Vocab at 0x24aa84594c0>,\n",
       " 'grow': <gensim.models.keyedvectors.Vocab at 0x24aa8459520>,\n",
       " 'biomed': <gensim.models.keyedvectors.Vocab at 0x24aa8459580>,\n",
       " 'control': <gensim.models.keyedvectors.Vocab at 0x24aa84595e0>,\n",
       " 'thermal': <gensim.models.keyedvectors.Vocab at 0x24aa8459640>,\n",
       " 'properti': <gensim.models.keyedvectors.Vocab at 0x24aa84596a0>,\n",
       " 'appear': <gensim.models.keyedvectors.Vocab at 0x24aa8459700>,\n",
       " 'relev': <gensim.models.keyedvectors.Vocab at 0x24aa8459760>,\n",
       " 'work': <gensim.models.keyedvectors.Vocab at 0x24aa84597c0>,\n",
       " 'address': <gensim.models.keyedvectors.Vocab at 0x24aa8459820>,\n",
       " 'water': <gensim.models.keyedvectors.Vocab at 0x24aa8459880>,\n",
       " 'gold': <gensim.models.keyedvectors.Vocab at 0x24aa84598e0>,\n",
       " 'surfac': <gensim.models.keyedvectors.Vocab at 0x24aa8459940>,\n",
       " 'molecular_dynam': <gensim.models.keyedvectors.Vocab at 0x24aa84599a0>,\n",
       " 'simul': <gensim.models.keyedvectors.Vocab at 0x24aa8459a00>,\n",
       " 'increas': <gensim.models.keyedvectors.Vocab at 0x24aa8459a60>,\n",
       " 'densiti': <gensim.models.keyedvectors.Vocab at 0x24aa8459ac0>,\n",
       " 'displac': <gensim.models.keyedvectors.Vocab at 0x24aa8459b20>,\n",
       " 'resist': <gensim.models.keyedvectors.Vocab at 0x24aa8459b80>,\n",
       " 'flow': <gensim.models.keyedvectors.Vocab at 0x24aa8459be0>,\n",
       " 'affect': <gensim.models.keyedvectors.Vocab at 0x24aa8459c40>,\n",
       " 'amount': <gensim.models.keyedvectors.Vocab at 0x24aa8459ca0>,\n",
       " 'energi': <gensim.models.keyedvectors.Vocab at 0x24aa8459d00>,\n",
       " 'releas': <gensim.models.keyedvectors.Vocab at 0x24aa8459d60>,\n",
       " 'unexpect': <gensim.models.keyedvectors.Vocab at 0x24aa8459dc0>,\n",
       " 'trade': <gensim.models.keyedvectors.Vocab at 0x24aa8459e20>,\n",
       " 'establish': <gensim.models.keyedvectors.Vocab at 0x24aa8459e80>,\n",
       " 'sph': <gensim.models.keyedvectors.Vocab at 0x24aa8459ee0>,\n",
       " 'calcul': <gensim.models.keyedvectors.Vocab at 0x24aa8459f40>,\n",
       " 'mar': <gensim.models.keyedvectors.Vocab at 0x24aa8459fa0>,\n",
       " 'scale': <gensim.models.keyedvectors.Vocab at 0x24aa845a040>,\n",
       " 'collis': <gensim.models.keyedvectors.Vocab at 0x24aa845a0a0>,\n",
       " 'role': <gensim.models.keyedvectors.Vocab at 0x24aa845a100>,\n",
       " 'materi': <gensim.models.keyedvectors.Vocab at 0x24aa845a160>,\n",
       " 'rheolog': <gensim.models.keyedvectors.Vocab at 0x24aa845a1c0>,\n",
       " 'larg_scale': <gensim.models.keyedvectors.Vocab at 0x24aa845a220>,\n",
       " 'approx': <gensim.models.keyedvectors.Vocab at 0x24aa845a280>,\n",
       " 'km': <gensim.models.keyedvectors.Vocab at 0x24aa845a2e0>,\n",
       " 'impact': <gensim.models.keyedvectors.Vocab at 0x24aa845a340>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x24aa845a3a0>,\n",
       " 'smooth': <gensim.models.keyedvectors.Vocab at 0x24aa845a400>,\n",
       " 'particl': <gensim.models.keyedvectors.Vocab at 0x24aa845a460>,\n",
       " 'hydrodynam': <gensim.models.keyedvectors.Vocab at 0x24aa845a4c0>,\n",
       " 'code': <gensim.models.keyedvectors.Vocab at 0x24aa845a520>,\n",
       " 'strength': <gensim.models.keyedvectors.Vocab at 0x24aa845a580>,\n",
       " 'post': <gensim.models.keyedvectors.Vocab at 0x24aa845a5e0>,\n",
       " 'temperatur': <gensim.models.keyedvectors.Vocab at 0x24aa845a640>,\n",
       " 'investig': <gensim.models.keyedvectors.Vocab at 0x24aa845a6a0>,\n",
       " 'eject': <gensim.models.keyedvectors.Vocab at 0x24aa845a700>,\n",
       " 'escap': <gensim.models.keyedvectors.Vocab at 0x24aa845a760>,\n",
       " 'disc': <gensim.models.keyedvectors.Vocab at 0x24aa845a7c0>,\n",
       " 'mass': <gensim.models.keyedvectors.Vocab at 0x24aa845a820>,\n",
       " 'potenti': <gensim.models.keyedvectors.Vocab at 0x24aa845a880>,\n",
       " 'context': <gensim.models.keyedvectors.Vocab at 0x24aa845a8e0>,\n",
       " 'discontinu': <gensim.models.keyedvectors.Vocab at 0x24aa845a940>,\n",
       " 'rigid': <gensim.models.keyedvectors.Vocab at 0x24aa845a9a0>,\n",
       " 'bodi': <gensim.models.keyedvectors.Vocab at 0x24aa845aa00>,\n",
       " 'regim': <gensim.models.keyedvectors.Vocab at 0x24aa845aa60>,\n",
       " 'consid': <gensim.models.keyedvectors.Vocab at 0x24aa845aac0>,\n",
       " 'veloc': <gensim.models.keyedvectors.Vocab at 0x24aa845ab20>,\n",
       " 'substanti': <gensim.models.keyedvectors.Vocab at 0x24aa845ab80>,\n",
       " 'impactor': <gensim.models.keyedvectors.Vocab at 0x24aa845abe0>,\n",
       " 'subtl': <gensim.models.keyedvectors.Vocab at 0x24aa845ac40>,\n",
       " 'high': <gensim.models.keyedvectors.Vocab at 0x24aa845aca0>,\n",
       " 'mathcal': <gensim.models.keyedvectors.Vocab at 0x24aa845ad00>,\n",
       " 'r': <gensim.models.keyedvectors.Vocab at 0x24aa845ad60>,\n",
       " 'fail': <gensim.models.keyedvectors.Vocab at 0x24aa845adc0>,\n",
       " 'outbreak': <gensim.models.keyedvectors.Vocab at 0x24aa845ae20>,\n",
       " 'boost': <gensim.models.keyedvectors.Vocab at 0x24aa845ae80>,\n",
       " 'immun': <gensim.models.keyedvectors.Vocab at 0x24aa845aee0>,\n",
       " 'time_vari': <gensim.models.keyedvectors.Vocab at 0x24aa845af40>,\n",
       " 'suscept': <gensim.models.keyedvectors.Vocab at 0x24aa845afa0>,\n",
       " 'host': <gensim.models.keyedvectors.Vocab at 0x24aa845c040>,\n",
       " 'wane': <gensim.models.keyedvectors.Vocab at 0x24aa845c0a0>,\n",
       " 'known': <gensim.models.keyedvectors.Vocab at 0x24aa845c100>,\n",
       " 'induc': <gensim.models.keyedvectors.Vocab at 0x24aa845c160>,\n",
       " 'rich': <gensim.models.keyedvectors.Vocab at 0x24aa845c1c0>,\n",
       " 'long_term': <gensim.models.keyedvectors.Vocab at 0x24aa845c220>,\n",
       " 'transmiss': <gensim.models.keyedvectors.Vocab at 0x24aa845c280>,\n",
       " 'dynam': <gensim.models.keyedvectors.Vocab at 0x24aa845c2e0>,\n",
       " 'meanwhil': <gensim.models.keyedvectors.Vocab at 0x24aa845c340>,\n",
       " 'heterogen': <gensim.models.keyedvectors.Vocab at 0x24aa845c3a0>,\n",
       " 'shot': <gensim.models.keyedvectors.Vocab at 0x24aa845c400>,\n",
       " 'epidem': <gensim.models.keyedvectors.Vocab at 0x24aa845c460>,\n",
       " 'even_though': <gensim.models.keyedvectors.Vocab at 0x24aa845c4c0>,\n",
       " 'larg_amount': <gensim.models.keyedvectors.Vocab at 0x24aa845c520>,\n",
       " 'avail': <gensim.models.keyedvectors.Vocab at 0x24aa845c580>,\n",
       " 'epidemiolog': <gensim.models.keyedvectors.Vocab at 0x24aa845c5e0>,\n",
       " 'short_term': <gensim.models.keyedvectors.Vocab at 0x24aa845c640>,\n",
       " 'parsimoni': <gensim.models.keyedvectors.Vocab at 0x24aa845c6a0>,\n",
       " 'mathemat': <gensim.models.keyedvectors.Vocab at 0x24aa845c700>,\n",
       " 'take_account': <gensim.models.keyedvectors.Vocab at 0x24aa845c760>,\n",
       " 'obtain': <gensim.models.keyedvectors.Vocab at 0x24aa845c7c0>,\n",
       " 'explicit': <gensim.models.keyedvectors.Vocab at 0x24aa845c820>,\n",
       " 'delay': <gensim.models.keyedvectors.Vocab at 0x24aa845c880>,\n",
       " 'take': <gensim.models.keyedvectors.Vocab at 0x24aa845c8e0>,\n",
       " 'negat': <gensim.models.keyedvectors.Vocab at 0x24aa845c940>,\n",
       " 'slope': <gensim.models.keyedvectors.Vocab at 0x24aa845c9a0>,\n",
       " 'curv': <gensim.models.keyedvectors.Vocab at 0x24aa845ca00>,\n",
       " 'phase': <gensim.models.keyedvectors.Vocab at 0x24aa845ca60>,\n",
       " 'addit': <gensim.models.keyedvectors.Vocab at 0x24aa845cac0>,\n",
       " 'common': <gensim.models.keyedvectors.Vocab at 0x24aa845cb20>,\n",
       " 'standard': <gensim.models.keyedvectors.Vocab at 0x24aa845cb80>,\n",
       " 'sir': <gensim.models.keyedvectors.Vocab at 0x24aa845cbe0>,\n",
       " 'leq': <gensim.models.keyedvectors.Vocab at 0x24aa845cc40>,\n",
       " 'normal': <gensim.models.keyedvectors.Vocab at 0x24aa845cca0>,\n",
       " 'employ': <gensim.models.keyedvectors.Vocab at 0x24aa845cd00>,\n",
       " 'sensit': <gensim.models.keyedvectors.Vocab at 0x24aa845cd60>,\n",
       " 'analysi': <gensim.models.keyedvectors.Vocab at 0x24aa845cdc0>,\n",
       " 'order': <gensim.models.keyedvectors.Vocab at 0x24aa845ce20>,\n",
       " 'hydraul': <gensim.models.keyedvectors.Vocab at 0x24aa845ce80>,\n",
       " 'fractur': <gensim.models.keyedvectors.Vocab at 0x24aa845cee0>,\n",
       " 'horizont': <gensim.models.keyedvectors.Vocab at 0x24aa845cf40>,\n",
       " 'systemat': <gensim.models.keyedvectors.Vocab at 0x24aa845cfa0>,\n",
       " 'sobol': <gensim.models.keyedvectors.Vocab at 0x24aa845e040>,\n",
       " 'util': <gensim.models.keyedvectors.Vocab at 0x24aa845e0a0>,\n",
       " 'quantiti': <gensim.models.keyedvectors.Vocab at 0x24aa845e100>,\n",
       " 'pore': <gensim.models.keyedvectors.Vocab at 0x24aa845e160>,\n",
       " 'pressur': <gensim.models.keyedvectors.Vocab at 0x24aa845e1c0>,\n",
       " 'deplet': <gensim.models.keyedvectors.Vocab at 0x24aa845e220>,\n",
       " 'stress': <gensim.models.keyedvectors.Vocab at 0x24aa845e280>,\n",
       " 'around': <gensim.models.keyedvectors.Vocab at 0x24aa845e2e0>,\n",
       " 'base': <gensim.models.keyedvectors.Vocab at 0x24aa845e340>,\n",
       " 'degre': <gensim.models.keyedvectors.Vocab at 0x24aa845e3a0>,\n",
       " 'import': <gensim.models.keyedvectors.Vocab at 0x24aa845e400>,\n",
       " 'includ': <gensim.models.keyedvectors.Vocab at 0x24aa845e460>,\n",
       " 'rock': <gensim.models.keyedvectors.Vocab at 0x24aa845e4c0>,\n",
       " 'stimul': <gensim.models.keyedvectors.Vocab at 0x24aa845e520>,\n",
       " 'design': <gensim.models.keyedvectors.Vocab at 0x24aa845e580>,\n",
       " 'fulli': <gensim.models.keyedvectors.Vocab at 0x24aa845e5e0>,\n",
       " 'poroelast': <gensim.models.keyedvectors.Vocab at 0x24aa845e640>,\n",
       " 'account': <gensim.models.keyedvectors.Vocab at 0x24aa845e6a0>,\n",
       " 'product': <gensim.models.keyedvectors.Vocab at 0x24aa845e700>,\n",
       " 'eas': <gensim.models.keyedvectors.Vocab at 0x24aa845e760>,\n",
       " 'comput_cost': <gensim.models.keyedvectors.Vocab at 0x24aa845e7c0>,\n",
       " 'provid': <gensim.models.keyedvectors.Vocab at 0x24aa845e820>,\n",
       " 'rom': <gensim.models.keyedvectors.Vocab at 0x24aa845e880>,\n",
       " 'replac': <gensim.models.keyedvectors.Vocab at 0x24aa845e8e0>,\n",
       " 'complex': <gensim.models.keyedvectors.Vocab at 0x24aa845e940>,\n",
       " 'rather': <gensim.models.keyedvectors.Vocab at 0x24aa845e9a0>,\n",
       " 'simpl': <gensim.models.keyedvectors.Vocab at 0x24aa845ea00>,\n",
       " 'analyt': <gensim.models.keyedvectors.Vocab at 0x24aa845ea60>,\n",
       " 'locat': <gensim.models.keyedvectors.Vocab at 0x24aa845eac0>,\n",
       " 'main': <gensim.models.keyedvectors.Vocab at 0x24aa845eb20>,\n",
       " 'research': <gensim.models.keyedvectors.Vocab at 0x24aa845eb80>,\n",
       " 'mobil': <gensim.models.keyedvectors.Vocab at 0x24aa845ebe0>,\n",
       " 'half': <gensim.models.keyedvectors.Vocab at 0x24aa845ec40>,\n",
       " 'length': <gensim.models.keyedvectors.Vocab at 0x24aa845eca0>,\n",
       " 'contributor': <gensim.models.keyedvectors.Vocab at 0x24aa845ed00>,\n",
       " 'percentag': <gensim.models.keyedvectors.Vocab at 0x24aa845ed60>,\n",
       " 'contribut': <gensim.models.keyedvectors.Vocab at 0x24aa845edc0>,\n",
       " 'pre': <gensim.models.keyedvectors.Vocab at 0x24aa845ee20>,\n",
       " 'ii': <gensim.models.keyedvectors.Vocab at 0x24aa845ee80>,\n",
       " 'progress': <gensim.models.keyedvectors.Vocab at 0x24aa845eee0>,\n",
       " 'decreas': <gensim.models.keyedvectors.Vocab at 0x24aa845ef40>,\n",
       " 'iii': <gensim.models.keyedvectors.Vocab at 0x24aa845efa0>,\n",
       " 'domin': <gensim.models.keyedvectors.Vocab at 0x24aa8460040>,\n",
       " 'iv': <gensim.models.keyedvectors.Vocab at 0x24aa84600a0>,\n",
       " 'zone': <gensim.models.keyedvectors.Vocab at 0x24aa8460100>,\n",
       " 'tip': <gensim.models.keyedvectors.Vocab at 0x24aa8460160>,\n",
       " 'insid': <gensim.models.keyedvectors.Vocab at 0x24aa84601c0>,\n",
       " 'area': <gensim.models.keyedvectors.Vocab at 0x24aa8460220>,\n",
       " 'factor': <gensim.models.keyedvectors.Vocab at 0x24aa8460280>,\n",
       " 'minimum': <gensim.models.keyedvectors.Vocab at 0x24aa84602e0>,\n",
       " 'guidelin': <gensim.models.keyedvectors.Vocab at 0x24aa8460340>,\n",
       " 'legaci': <gensim.models.keyedvectors.Vocab at 0x24aa84603a0>,\n",
       " 'secondari': <gensim.models.keyedvectors.Vocab at 0x24aa8460400>,\n",
       " 'oper': <gensim.models.keyedvectors.Vocab at 0x24aa8460460>,\n",
       " 'drill': <gensim.models.keyedvectors.Vocab at 0x24aa84604c0>,\n",
       " 'separ': <gensim.models.keyedvectors.Vocab at 0x24aa8460520>,\n",
       " 'social': <gensim.models.keyedvectors.Vocab at 0x24aa8460580>,\n",
       " 'dilemma': <gensim.models.keyedvectors.Vocab at 0x24aa84605e0>,\n",
       " 'topolog': <gensim.models.keyedvectors.Vocab at 0x24aa8460640>,\n",
       " 'frustrat': <gensim.models.keyedvectors.Vocab at 0x24aa84606a0>,\n",
       " 'three': <gensim.models.keyedvectors.Vocab at 0x24aa8460700>,\n",
       " 'crowd': <gensim.models.keyedvectors.Vocab at 0x24aa8460760>,\n",
       " 'old': <gensim.models.keyedvectors.Vocab at 0x24aa84607c0>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x24aa8460820>,\n",
       " 'interact': <gensim.models.keyedvectors.Vocab at 0x24aa8460880>,\n",
       " 'configur': <gensim.models.keyedvectors.Vocab at 0x24aa84608e0>,\n",
       " 'statist': <gensim.models.keyedvectors.Vocab at 0x24aa8460940>,\n",
       " 'physic': <gensim.models.keyedvectors.Vocab at 0x24aa84609a0>,\n",
       " 'accord': <gensim.models.keyedvectors.Vocab at 0x24aa8460a00>,\n",
       " 'within': <gensim.models.keyedvectors.Vocab at 0x24aa8460a60>,\n",
       " 'triangl': <gensim.models.keyedvectors.Vocab at 0x24aa8460ac0>,\n",
       " 'deserv': <gensim.models.keyedvectors.Vocab at 0x24aa8460b20>,\n",
       " 'special': <gensim.models.keyedvectors.Vocab at 0x24aa8460b80>,\n",
       " 'attent': <gensim.models.keyedvectors.Vocab at 0x24aa8460be0>,\n",
       " 'motiv': <gensim.models.keyedvectors.Vocab at 0x24aa8460c40>,\n",
       " 'evolutionari': <gensim.models.keyedvectors.Vocab at 0x24aa8460ca0>,\n",
       " 'game': <gensim.models.keyedvectors.Vocab at 0x24aa8460d00>,\n",
       " 'triangular': <gensim.models.keyedvectors.Vocab at 0x24aa8460d60>,\n",
       " 'lattic': <gensim.models.keyedvectors.Vocab at 0x24aa8460dc0>,\n",
       " 'prevent': <gensim.models.keyedvectors.Vocab at 0x24aa8460e20>,\n",
       " 'anti': <gensim.models.keyedvectors.Vocab at 0x24aa8460e80>,\n",
       " 'coordin': <gensim.models.keyedvectors.Vocab at 0x24aa8460ee0>,\n",
       " 'compet': <gensim.models.keyedvectors.Vocab at 0x24aa8460f40>,\n",
       " 'strategi': <gensim.models.keyedvectors.Vocab at 0x24aa8460fa0>,\n",
       " 'would': <gensim.models.keyedvectors.Vocab at 0x24aa8462040>,\n",
       " 'need': <gensim.models.keyedvectors.Vocab at 0x24aa84620a0>,\n",
       " 'optim': <gensim.models.keyedvectors.Vocab at 0x24aa8462100>,\n",
       " 'outcom': <gensim.models.keyedvectors.Vocab at 0x24aa8462160>,\n",
       " 'updat': <gensim.models.keyedvectors.Vocab at 0x24aa84621c0>,\n",
       " 'protocol': <gensim.models.keyedvectors.Vocab at 0x24aa8462220>,\n",
       " 'spatial': <gensim.models.keyedvectors.Vocab at 0x24aa8462280>,\n",
       " 'pattern': <gensim.models.keyedvectors.Vocab at 0x24aa84622e0>,\n",
       " 'payoff': <gensim.models.keyedvectors.Vocab at 0x24aa8462340>,\n",
       " 'reminisc': <gensim.models.keyedvectors.Vocab at 0x24aa84623a0>,\n",
       " 'honeycomb': <gensim.models.keyedvectors.Vocab at 0x24aa8462400>,\n",
       " 'organ': <gensim.models.keyedvectors.Vocab at 0x24aa8462460>,\n",
       " 'help': <gensim.models.keyedvectors.Vocab at 0x24aa84624c0>,\n",
       " 'minim': <gensim.models.keyedvectors.Vocab at 0x24aa8462520>,\n",
       " 'emerg': <gensim.models.keyedvectors.Vocab at 0x24aa8462580>,\n",
       " 'microscop': <gensim.models.keyedvectors.Vocab at 0x24aa84625e0>,\n",
       " 'mean': <gensim.models.keyedvectors.Vocab at 0x24aa8462640>,\n",
       " 'mean_field': <gensim.models.keyedvectors.Vocab at 0x24aa84626a0>,\n",
       " 'mont_carlo': <gensim.models.keyedvectors.Vocab at 0x24aa8462700>,\n",
       " 'squar': <gensim.models.keyedvectors.Vocab at 0x24aa8462760>,\n",
       " 'cours': <gensim.models.keyedvectors.Vocab at 0x24aa84627c0>,\n",
       " 'absent': <gensim.models.keyedvectors.Vocab at 0x24aa8462820>,\n",
       " 'howev': <gensim.models.keyedvectors.Vocab at 0x24aa8462880>,\n",
       " 'delet': <gensim.models.keyedvectors.Vocab at 0x24aa84628e0>,\n",
       " 'diagon': <gensim.models.keyedvectors.Vocab at 0x24aa8462940>,\n",
       " 'link': <gensim.models.keyedvectors.Vocab at 0x24aa84629a0>,\n",
       " 'gradual': <gensim.models.keyedvectors.Vocab at 0x24aa8462a00>,\n",
       " 'bridg': <gensim.models.keyedvectors.Vocab at 0x24aa8462a60>,\n",
       " 'gap': <gensim.models.keyedvectors.Vocab at 0x24aa8462ac0>,\n",
       " 'cooper': <gensim.models.keyedvectors.Vocab at 0x24aa8462b20>,\n",
       " 'indic': <gensim.models.keyedvectors.Vocab at 0x24aa8462b80>,\n",
       " 'thus': <gensim.models.keyedvectors.Vocab at 0x24aa8462be0>,\n",
       " 'determin': <gensim.models.keyedvectors.Vocab at 0x24aa8462c40>,\n",
       " 'exciton': <gensim.models.keyedvectors.Vocab at 0x24aa8462ca0>,\n",
       " 'polaron': <gensim.models.keyedvectors.Vocab at 0x24aa8462d00>,\n",
       " 'quantum': <gensim.models.keyedvectors.Vocab at 0x24aa8462d60>,\n",
       " 'self': <gensim.models.keyedvectors.Vocab at 0x24aa8462dc0>,\n",
       " 'format': <gensim.models.keyedvectors.Vocab at 0x24aa8462e20>,\n",
       " 'cd': <gensim.models.keyedvectors.Vocab at 0x24aa8462e80>,\n",
       " 'mn': <gensim.models.keyedvectors.Vocab at 0x24aa8462ee0>,\n",
       " 'se': <gensim.models.keyedvectors.Vocab at 0x24aa8462f40>,\n",
       " 'mg': <gensim.models.keyedvectors.Vocab at 0x24aa8462fa0>,\n",
       " 'dilut': <gensim.models.keyedvectors.Vocab at 0x24aa8464040>,\n",
       " 'semiconductor': <gensim.models.keyedvectors.Vocab at 0x24aa84640a0>,\n",
       " 'resolv': <gensim.models.keyedvectors.Vocab at 0x24aa8464100>,\n",
       " 'photoluminesc': <gensim.models.keyedvectors.Vocab at 0x24aa8464160>,\n",
       " 'pl': <gensim.models.keyedvectors.Vocab at 0x24aa84641c0>,\n",
       " 'magnet_field': <gensim.models.keyedvectors.Vocab at 0x24aa8464220>,\n",
       " 'deduc': <gensim.models.keyedvectors.Vocab at 0x24aa8464280>,\n",
       " 'mev': <gensim.models.keyedvectors.Vocab at 0x24aa84642e0>,\n",
       " 'agreement': <gensim.models.keyedvectors.Vocab at 0x24aa8464340>,\n",
       " 'integr': <gensim.models.keyedvectors.Vocab at 0x24aa84643a0>,\n",
       " 'select': <gensim.models.keyedvectors.Vocab at 0x24aa8464400>,\n",
       " 'excit': <gensim.models.keyedvectors.Vocab at 0x24aa8464460>,\n",
       " 'circular': <gensim.models.keyedvectors.Vocab at 0x24aa84644c0>,\n",
       " 'polar': <gensim.models.keyedvectors.Vocab at 0x24aa8464520>,\n",
       " 'ps': <gensim.models.keyedvectors.Vocab at 0x24aa8464580>,\n",
       " 'signific': <gensim.models.keyedvectors.Vocab at 0x24aa84645e0>,\n",
       " 'longer': <gensim.models.keyedvectors.Vocab at 0x24aa8464640>,\n",
       " 'correspond': <gensim.models.keyedvectors.Vocab at 0x24aa84646a0>,\n",
       " 'report': <gensim.models.keyedvectors.Vocab at 0x24aa8464700>,\n",
       " 'earlier': <gensim.models.keyedvectors.Vocab at 0x24aa8464760>,\n",
       " 'strong': <gensim.models.keyedvectors.Vocab at 0x24aa84647c0>,\n",
       " 'accompani': <gensim.models.keyedvectors.Vocab at 0x24aa8464820>,\n",
       " 'squeez': <gensim.models.keyedvectors.Vocab at 0x24aa8464880>,\n",
       " 'heavi': <gensim.models.keyedvectors.Vocab at 0x24aa84648e0>,\n",
       " 'hole': <gensim.models.keyedvectors.Vocab at 0x24aa8464940>,\n",
       " 'envelop': <gensim.models.keyedvectors.Vocab at 0x24aa84649a0>,\n",
       " 'wavefunct': <gensim.models.keyedvectors.Vocab at 0x24aa8464a00>,\n",
       " 'conclus': <gensim.models.keyedvectors.Vocab at 0x24aa8464a60>,\n",
       " 'support': <gensim.models.keyedvectors.Vocab at 0x24aa8464ac0>,\n",
       " 'lifetim': <gensim.models.keyedvectors.Vocab at 0x24aa8464b20>,\n",
       " 'varieti': <gensim.models.keyedvectors.Vocab at 0x24aa8464b80>,\n",
       " 'automata': <gensim.models.keyedvectors.Vocab at 0x24aa8464be0>,\n",
       " 'eilenberg': <gensim.models.keyedvectors.Vocab at 0x24aa8464c40>,\n",
       " 'concept': <gensim.models.keyedvectors.Vocab at 0x24aa8464ca0>,\n",
       " 'syntact': <gensim.models.keyedvectors.Vocab at 0x24aa8464d00>,\n",
       " 'monoid': <gensim.models.keyedvectors.Vocab at 0x24aa8464d60>,\n",
       " 'regular': <gensim.models.keyedvectors.Vocab at 0x24aa8464dc0>,\n",
       " 'languag': <gensim.models.keyedvectors.Vocab at 0x24aa8464e20>,\n",
       " 'pseudovarieti': <gensim.models.keyedvectors.Vocab at 0x24aa8464e80>,\n",
       " 'modif': <gensim.models.keyedvectors.Vocab at 0x24aa8464ee0>,\n",
       " 'general': <gensim.models.keyedvectors.Vocab at 0x24aa8464f40>,\n",
       " 'class': <gensim.models.keyedvectors.Vocab at 0x24aa8464fa0>,\n",
       " 'hand': <gensim.models.keyedvectors.Vocab at 0x24aa8466040>,\n",
       " 'algebra': <gensim.models.keyedvectors.Vocab at 0x24aa84660a0>,\n",
       " 'structur': <gensim.models.keyedvectors.Vocab at 0x24aa8466100>,\n",
       " 'preimag': <gensim.models.keyedvectors.Vocab at 0x24aa8466160>,\n",
       " 'homomorph': <gensim.models.keyedvectors.Vocab at 0x24aa84661c0>,\n",
       " 'equip': <gensim.models.keyedvectors.Vocab at 0x24aa8466220>,\n",
       " 'compat': <gensim.models.keyedvectors.Vocab at 0x24aa8466280>,\n",
       " 'distinguish': <gensim.models.keyedvectors.Vocab at 0x24aa84662e0>,\n",
       " 'counterpart': <gensim.models.keyedvectors.Vocab at 0x24aa8466340>,\n",
       " 'previous': <gensim.models.keyedvectors.Vocab at 0x24aa84663a0>,\n",
       " 'c': <gensim.models.keyedvectors.Vocab at 0x24aa8466400>,\n",
       " 'spontan': <gensim.models.keyedvectors.Vocab at 0x24aa8466460>,\n",
       " 'abrikosov': <gensim.models.keyedvectors.Vocab at 0x24aa84664c0>,\n",
       " 'vortex': <gensim.models.keyedvectors.Vocab at 0x24aa8466520>,\n",
       " 'ferromagnet': <gensim.models.keyedvectors.Vocab at 0x24aa8466580>,\n",
       " 'superconductor': <gensim.models.keyedvectors.Vocab at 0x24aa84665e0>,\n",
       " 'euf': <gensim.models.keyedvectors.Vocab at 0x24aa8466640>,\n",
       " 'p': <gensim.models.keyedvectors.Vocab at 0x24aa84666a0>,\n",
       " 'x_x': <gensim.models.keyedvectors.Vocab at 0x24aa8466700>,\n",
       " 'low_temperatur': <gensim.models.keyedvectors.Vocab at 0x24aa8466760>,\n",
       " 'forc': <gensim.models.keyedvectors.Vocab at 0x24aa84667c0>,\n",
       " 'microscopi': <gensim.models.keyedvectors.Vocab at 0x24aa8466820>,\n",
       " 'mfm': <gensim.models.keyedvectors.Vocab at 0x24aa8466880>,\n",
       " 'svp': <gensim.models.keyedvectors.Vocab at 0x24aa84668e0>,\n",
       " 'singl_crystal': <gensim.models.keyedvectors.Vocab at 0x24aa8466940>,\n",
       " 'superconduct': <gensim.models.keyedvectors.Vocab at 0x24aa84669a0>,\n",
       " 'rm': <gensim.models.keyedvectors.Vocab at 0x24aa8466a00>,\n",
       " 'sc': <gensim.models.keyedvectors.Vocab at 0x24aa8466a60>,\n",
       " 'k': <gensim.models.keyedvectors.Vocab at 0x24aa8466ac0>,\n",
       " 'fm': <gensim.models.keyedvectors.Vocab at 0x24aa8466b20>,\n",
       " 'sim': <gensim.models.keyedvectors.Vocab at 0x24aa8466b80>,\n",
       " 'transit': <gensim.models.keyedvectors.Vocab at 0x24aa8466be0>,\n",
       " 'antivortex': <gensim.models.keyedvectors.Vocab at 0x24aa8466c40>,\n",
       " 'v': <gensim.models.keyedvectors.Vocab at 0x24aa8466ca0>,\n",
       " 'av': <gensim.models.keyedvectors.Vocab at 0x24aa8466d00>,\n",
       " 'pair': <gensim.models.keyedvectors.Vocab at 0x24aa8466d60>,\n",
       " 'vicin': <gensim.models.keyedvectors.Vocab at 0x24aa8466dc0>,\n",
       " 'upon': <gensim.models.keyedvectors.Vocab at 0x24aa8466e20>,\n",
       " 'cool': <gensim.models.keyedvectors.Vocab at 0x24aa8466e80>,\n",
       " 'cycl': <gensim.models.keyedvectors.Vocab at 0x24aa8466ee0>,\n",
       " 'near': <gensim.models.keyedvectors.Vocab at 0x24aa8466f40>,\n",
       " 'first_order': <gensim.models.keyedvectors.Vocab at 0x24aa8466fa0>,\n",
       " 'meissner': <gensim.models.keyedvectors.Vocab at 0x24aa8468040>,\n",
       " 'long': <gensim.models.keyedvectors.Vocab at 0x24aa84680a0>,\n",
       " 'scenario': <gensim.models.keyedvectors.Vocab at 0x24aa8468100>,\n",
       " 'character': <gensim.models.keyedvectors.Vocab at 0x24aa8468160>,\n",
       " 'larger': <gensim.models.keyedvectors.Vocab at 0x24aa84681c0>,\n",
       " 'peculiar': <gensim.models.keyedvectors.Vocab at 0x24aa8468220>,\n",
       " 'branch': <gensim.models.keyedvectors.Vocab at 0x24aa8468280>,\n",
       " 'stripe': <gensim.models.keyedvectors.Vocab at 0x24aa84682e0>,\n",
       " 'typic': <gensim.models.keyedvectors.Vocab at 0x24aa8468340>,\n",
       " 'uniaxi': <gensim.models.keyedvectors.Vocab at 0x24aa84683a0>,\n",
       " 'perpendicular': <gensim.models.keyedvectors.Vocab at 0x24aa8468400>,\n",
       " 'anisotropi': <gensim.models.keyedvectors.Vocab at 0x24aa8468460>,\n",
       " 'pma': <gensim.models.keyedvectors.Vocab at 0x24aa84684c0>,\n",
       " 'ware': <gensim.models.keyedvectors.Vocab at 0x24aa8468520>,\n",
       " 'sm': <gensim.models.keyedvectors.Vocab at 0x24aa8468580>,\n",
       " 'langl': <gensim.models.keyedvectors.Vocab at 0x24aa84685e0>,\n",
       " 'rangl': <gensim.models.keyedvectors.Vocab at 0x24aa8468640>,\n",
       " 'symmetri': <gensim.models.keyedvectors.Vocab at 0x24aa84686a0>,\n",
       " 'recent': <gensim.models.keyedvectors.Vocab at 0x24aa8468700>,\n",
       " 'expon': <gensim.models.keyedvectors.Vocab at 0x24aa8468760>,\n",
       " 'matrix': <gensim.models.keyedvectors.Vocab at 0x24aa84687c0>,\n",
       " 'symmetr': <gensim.models.keyedvectors.Vocab at 0x24aa8468820>,\n",
       " 'better_understand': <gensim.models.keyedvectors.Vocab at 0x24aa8468880>,\n",
       " 'group': <gensim.models.keyedvectors.Vocab at 0x24aa84688e0>,\n",
       " 'pdbi': <gensim.models.keyedvectors.Vocab at 0x24aa8468940>,\n",
       " 'arcsecond': <gensim.models.keyedvectors.Vocab at 0x24aa84689a0>,\n",
       " 'survey': <gensim.models.keyedvectors.Vocab at 0x24aa8468a00>,\n",
       " 'paw': <gensim.models.keyedvectors.Vocab at 0x24aa8468a60>,\n",
       " 'spiral': <gensim.models.keyedvectors.Vocab at 0x24aa8468ac0>,\n",
       " 'arm': <gensim.models.keyedvectors.Vocab at 0x24aa8468b20>,\n",
       " 'cloud': <gensim.models.keyedvectors.Vocab at 0x24aa8468b80>,\n",
       " 'star_format': <gensim.models.keyedvectors.Vocab at 0x24aa8468be0>,\n",
       " 'lead': <gensim.models.keyedvectors.Vocab at 0x24aa8468c40>,\n",
       " 'bright': <gensim.models.keyedvectors.Vocab at 0x24aa8468ca0>,\n",
       " 'star_form': <gensim.models.keyedvectors.Vocab at 0x24aa8468d00>,\n",
       " 'site': <gensim.models.keyedvectors.Vocab at 0x24aa8468d60>,\n",
       " 'along': <gensim.models.keyedvectors.Vocab at 0x24aa8468dc0>,\n",
       " 'promin': <gensim.models.keyedvectors.Vocab at 0x24aa8468e20>,\n",
       " 'remain': <gensim.models.keyedvectors.Vocab at 0x24aa8468e80>,\n",
       " 'elus': <gensim.models.keyedvectors.Vocab at 0x24aa8468ee0>,\n",
       " 'multi': <gensim.models.keyedvectors.Vocab at 0x24aa8468f40>,\n",
       " 'nearbi': <gensim.models.keyedvectors.Vocab at 0x24aa8468fa0>,\n",
       " 'grand': <gensim.models.keyedvectors.Vocab at 0x24aa846a040>,\n",
       " 'galaxi': <gensim.models.keyedvectors.Vocab at 0x24aa846a0a0>,\n",
       " 'belong': <gensim.models.keyedvectors.Vocab at 0x24aa846a100>,\n",
       " 'wave': <gensim.models.keyedvectors.Vocab at 0x24aa846a160>,\n",
       " 'exhibit': <gensim.models.keyedvectors.Vocab at 0x24aa846a1c0>,\n",
       " 'nine': <gensim.models.keyedvectors.Vocab at 0x24aa846a220>,\n",
       " 'gas': <gensim.models.keyedvectors.Vocab at 0x24aa846a280>,\n",
       " 'spur': <gensim.models.keyedvectors.Vocab at 0x24aa846a2e0>,\n",
       " 'ioniz': <gensim.models.keyedvectors.Vocab at 0x24aa846a340>,\n",
       " 'atom': <gensim.models.keyedvectors.Vocab at 0x24aa846a3a0>,\n",
       " 'molecular': <gensim.models.keyedvectors.Vocab at 0x24aa846a400>,\n",
       " 'dusti': <gensim.models.keyedvectors.Vocab at 0x24aa846a460>,\n",
       " 'interstellar': <gensim.models.keyedvectors.Vocab at 0x24aa846a4c0>,\n",
       " 'medium': <gensim.models.keyedvectors.Vocab at 0x24aa846a520>,\n",
       " 'ism': <gensim.models.keyedvectors.Vocab at 0x24aa846a580>,\n",
       " 'tracer': <gensim.models.keyedvectors.Vocab at 0x24aa846a5e0>,\n",
       " 'hii': <gensim.models.keyedvectors.Vocab at 0x24aa846a640>,\n",
       " 'region': <gensim.models.keyedvectors.Vocab at 0x24aa846a6a0>,\n",
       " 'young': <gensim.models.keyedvectors.Vocab at 0x24aa846a700>,\n",
       " 'myr': <gensim.models.keyedvectors.Vocab at 0x24aa846a760>,\n",
       " 'stellar': <gensim.models.keyedvectors.Vocab at 0x24aa846a7c0>,\n",
       " 'variat': <gensim.models.keyedvectors.Vocab at 0x24aa846a820>,\n",
       " 'giant': <gensim.models.keyedvectors.Vocab at 0x24aa846a880>,\n",
       " 'gmc': <gensim.models.keyedvectors.Vocab at 0x24aa846a8e0>,\n",
       " 'extinct': <gensim.models.keyedvectors.Vocab at 0x24aa846a940>,\n",
       " 'aris': <gensim.models.keyedvectors.Vocab at 0x24aa846a9a0>,\n",
       " 'ongo': <gensim.models.keyedvectors.Vocab at 0x24aa846aa00>,\n",
       " 'despit': <gensim.models.keyedvectors.Vocab at 0x24aa846aa60>,\n",
       " 'trend': <gensim.models.keyedvectors.Vocab at 0x24aa846aac0>,\n",
       " 'age': <gensim.models.keyedvectors.Vocab at 0x24aa846ab20>,\n",
       " 'feedback': <gensim.models.keyedvectors.Vocab at 0x24aa846ab80>,\n",
       " 'tentat': <gensim.models.keyedvectors.Vocab at 0x24aa846abe0>,\n",
       " 'trigger': <gensim.models.keyedvectors.Vocab at 0x24aa846ac40>,\n",
       " 'entiti': <gensim.models.keyedvectors.Vocab at 0x24aa846aca0>,\n",
       " 'blend': <gensim.models.keyedvectors.Vocab at 0x24aa846ad00>,\n",
       " 'lower': <gensim.models.keyedvectors.Vocab at 0x24aa846ad60>,\n",
       " 'resolut': <gensim.models.keyedvectors.Vocab at 0x24aa846adc0>,\n",
       " 'conclud': <gensim.models.keyedvectors.Vocab at 0x24aa846ae20>,\n",
       " 'coher': <gensim.models.keyedvectors.Vocab at 0x24aa846ae80>,\n",
       " 'onset': <gensim.models.keyedvectors.Vocab at 0x24aa846aee0>,\n",
       " 'mechan': <gensim.models.keyedvectors.Vocab at 0x24aa846af40>,\n",
       " 'sole': <gensim.models.keyedvectors.Vocab at 0x24aa846afa0>,\n",
       " 'occur': <gensim.models.keyedvectors.Vocab at 0x24aa846c040>,\n",
       " 'proceed': <gensim.models.keyedvectors.Vocab at 0x24aa846c0a0>,\n",
       " 'sever': <gensim.models.keyedvectors.Vocab at 0x24aa846c100>,\n",
       " 'million': <gensim.models.keyedvectors.Vocab at 0x24aa846c160>,\n",
       " 'year': <gensim.models.keyedvectors.Vocab at 0x24aa846c1c0>,\n",
       " 'impli': <gensim.models.keyedvectors.Vocab at 0x24aa846c220>,\n",
       " 'act': <gensim.models.keyedvectors.Vocab at 0x24aa846c280>,\n",
       " 'sustain': <gensim.models.keyedvectors.Vocab at 0x24aa846c2e0>,\n",
       " 'unstabl': <gensim.models.keyedvectors.Vocab at 0x24aa846c340>,\n",
       " 'adam': <gensim.models.keyedvectors.Vocab at 0x24aa846c3a0>,\n",
       " 'spectral': <gensim.models.keyedvectors.Vocab at 0x24aa846c400>,\n",
       " 'sequenc': <gensim.models.keyedvectors.Vocab at 0x24aa846c460>,\n",
       " 'variant': <gensim.models.keyedvectors.Vocab at 0x24aa846c4c0>,\n",
       " 'free': <gensim.models.keyedvectors.Vocab at 0x24aa846c520>,\n",
       " 'simplici': <gensim.models.keyedvectors.Vocab at 0x24aa846c580>,\n",
       " 'h': <gensim.models.keyedvectors.Vocab at 0x24aa846c5e0>,\n",
       " 'mathbb_f': <gensim.models.keyedvectors.Vocab at 0x24aa846c640>,\n",
       " 'mathbb_q': <gensim.models.keyedvectors.Vocab at 0x24aa846c6a0>,\n",
       " 'filtrat': <gensim.models.keyedvectors.Vocab at 0x24aa846c700>,\n",
       " 'appropri': <gensim.models.keyedvectors.Vocab at 0x24aa846c760>,\n",
       " 'cohomolog': <gensim.models.keyedvectors.Vocab at 0x24aa846c7c0>,\n",
       " 'covari': <gensim.models.keyedvectors.Vocab at 0x24aa846c820>,\n",
       " 'priorit': <gensim.models.keyedvectors.Vocab at 0x24aa846c880>,\n",
       " 'via': <gensim.models.keyedvectors.Vocab at 0x24aa846c8e0>,\n",
       " 'match': <gensim.models.keyedvectors.Vocab at 0x24aa846c940>,\n",
       " 'causal': <gensim.models.keyedvectors.Vocab at 0x24aa846c9a0>,\n",
       " 'five': <gensim.models.keyedvectors.Vocab at 0x24aa846ca00>,\n",
       " 'empir': <gensim.models.keyedvectors.Vocab at 0x24aa846ca60>,\n",
       " 'seek': <gensim.models.keyedvectors.Vocab at 0x24aa846cac0>,\n",
       " 'assum': <gensim.models.keyedvectors.Vocab at 0x24aa846cb20>,\n",
       " 'treatment': <gensim.models.keyedvectors.Vocab at 0x24aa846cb80>,\n",
       " 'identif': <gensim.models.keyedvectors.Vocab at 0x24aa846cbe0>,\n",
       " 'analyst': <gensim.models.keyedvectors.Vocab at 0x24aa846cc40>,\n",
       " 'must': <gensim.models.keyedvectors.Vocab at 0x24aa846cca0>,\n",
       " 'adjust': <gensim.models.keyedvectors.Vocab at 0x24aa846cd00>,\n",
       " 'confound': <gensim.models.keyedvectors.Vocab at 0x24aa846cd60>,\n",
       " 'basic': <gensim.models.keyedvectors.Vocab at 0x24aa846cdc0>,\n",
       " 'regress': <gensim.models.keyedvectors.Vocab at 0x24aa846ce20>,\n",
       " 'robust': <gensim.models.keyedvectors.Vocab at 0x24aa846ce80>,\n",
       " 'weight': <gensim.models.keyedvectors.Vocab at 0x24aa846cee0>,\n",
       " 'becom': <gensim.models.keyedvectors.Vocab at 0x24aa846cf40>,\n",
       " 'late': <gensim.models.keyedvectors.Vocab at 0x24aa846cfa0>,\n",
       " 'even': <gensim.models.keyedvectors.Vocab at 0x24aa846e040>,\n",
       " 'flexibl': <gensim.models.keyedvectors.Vocab at 0x24aa846e0a0>,\n",
       " 'black_box': <gensim.models.keyedvectors.Vocab at 0x24aa846e100>,\n",
       " 'littl': <gensim.models.keyedvectors.Vocab at 0x24aa846e160>,\n",
       " 'input': <gensim.models.keyedvectors.Vocab at 0x24aa846e1c0>,\n",
       " 'competit': <gensim.models.keyedvectors.Vocab at 0x24aa846e220>,\n",
       " 'substant': <gensim.models.keyedvectors.Vocab at 0x24aa846e280>,\n",
       " 'contrast': <gensim.models.keyedvectors.Vocab at 0x24aa846e2e0>,\n",
       " 'black': <gensim.models.keyedvectors.Vocab at 0x24aa846e340>,\n",
       " 'replic': <gensim.models.keyedvectors.Vocab at 0x24aa846e3a0>,\n",
       " 'custom': <gensim.models.keyedvectors.Vocab at 0x24aa846e400>,\n",
       " 'respons': <gensim.models.keyedvectors.Vocab at 0x24aa846e460>,\n",
       " 'expertis': <gensim.models.keyedvectors.Vocab at 0x24aa846e4c0>,\n",
       " 'across': <gensim.models.keyedvectors.Vocab at 0x24aa846e520>,\n",
       " 'advic': <gensim.models.keyedvectors.Vocab at 0x24aa846e580>,\n",
       " 'acoust': <gensim.models.keyedvectors.Vocab at 0x24aa846e5e0>,\n",
       " 'imped': <gensim.models.keyedvectors.Vocab at 0x24aa846e640>,\n",
       " 'invers': <gensim.models.keyedvectors.Vocab at 0x24aa846e6a0>,\n",
       " 'helmholtz': <gensim.models.keyedvectors.Vocab at 0x24aa846e700>,\n",
       " 'assign': <gensim.models.keyedvectors.Vocab at 0x24aa846e760>,\n",
       " 'homogen': <gensim.models.keyedvectors.Vocab at 0x24aa846e7c0>,\n",
       " 'boundari_condit': <gensim.models.keyedvectors.Vocab at 0x24aa846e820>,\n",
       " 'navier_stoke': <gensim.models.keyedvectors.Vocab at 0x24aa846e880>,\n",
       " 'solver': <gensim.models.keyedvectors.Vocab at 0x24aa846e8e0>,\n",
       " 'output': <gensim.models.keyedvectors.Vocab at 0x24aa846e940>,\n",
       " 'eigenfunct': <gensim.models.keyedvectors.Vocab at 0x24aa846e9a0>,\n",
       " 'ih': <gensim.models.keyedvectors.Vocab at 0x24aa846ea00>,\n",
       " 'revers': <gensim.models.keyedvectors.Vocab at 0x24aa846ea60>,\n",
       " 'procedur': <gensim.models.keyedvectors.Vocab at 0x24aa846eac0>,\n",
       " 'return': <gensim.models.keyedvectors.Vocab at 0x24aa846eb20>,\n",
       " 'unknown': <gensim.models.keyedvectors.Vocab at 0x24aa846eb80>,\n",
       " 'boundari': <gensim.models.keyedvectors.Vocab at 0x24aa846ebe0>,\n",
       " 'ib': <gensim.models.keyedvectors.Vocab at 0x24aa846ec40>,\n",
       " 'real': <gensim.models.keyedvectors.Vocab at 0x24aa846eca0>,\n",
       " 'second_order': <gensim.models.keyedvectors.Vocab at 0x24aa846ed00>,\n",
       " 'unstructur': <gensim.models.keyedvectors.Vocab at 0x24aa846ed60>,\n",
       " 'stagger': <gensim.models.keyedvectors.Vocab at 0x24aa846edc0>,\n",
       " 'arrang': <gensim.models.keyedvectors.Vocab at 0x24aa846ee20>,\n",
       " 'momentum': <gensim.models.keyedvectors.Vocab at 0x24aa846ee80>,\n",
       " 'extend': <gensim.models.keyedvectors.Vocab at 0x24aa846eee0>,\n",
       " 'center': <gensim.models.keyedvectors.Vocab at 0x24aa846ef40>,\n",
       " 'face': <gensim.models.keyedvectors.Vocab at 0x24aa846efa0>,\n",
       " 'compon': <gensim.models.keyedvectors.Vocab at 0x24aa8470040>,\n",
       " 'co': <gensim.models.keyedvectors.Vocab at 0x24aa84700a0>,\n",
       " 'treat': <gensim.models.keyedvectors.Vocab at 0x24aa8470100>,\n",
       " 'closur': <gensim.models.keyedvectors.Vocab at 0x24aa8470160>,\n",
       " 'gradient': <gensim.models.keyedvectors.Vocab at 0x24aa84701c0>,\n",
       " 'waveform': <gensim.models.keyedvectors.Vocab at 0x24aa8470220>,\n",
       " 'carri': <gensim.models.keyedvectors.Vocab at 0x24aa8470280>,\n",
       " 'independ': <gensim.models.keyedvectors.Vocab at 0x24aa84702e0>,\n",
       " 'complet': <gensim.models.keyedvectors.Vocab at 0x24aa8470340>,\n",
       " 'broadband': <gensim.models.keyedvectors.Vocab at 0x24aa84703a0>,\n",
       " 'desir': <gensim.models.keyedvectors.Vocab at 0x24aa8470400>,\n",
       " 'rang': <gensim.models.keyedvectors.Vocab at 0x24aa8470460>,\n",
       " 'valid': <gensim.models.keyedvectors.Vocab at 0x24aa84704c0>,\n",
       " 'inviscid': <gensim.models.keyedvectors.Vocab at 0x24aa8470520>,\n",
       " 'viscous': <gensim.models.keyedvectors.Vocab at 0x24aa8470580>,\n",
       " 'rectangular': <gensim.models.keyedvectors.Vocab at 0x24aa84705e0>,\n",
       " 'duct': <gensim.models.keyedvectors.Vocab at 0x24aa8470640>,\n",
       " 'geometr': <gensim.models.keyedvectors.Vocab at 0x24aa84706a0>,\n",
       " 'toy': <gensim.models.keyedvectors.Vocab at 0x24aa8470700>,\n",
       " 'caviti': <gensim.models.keyedvectors.Vocab at 0x24aa8470760>,\n",
       " 'verifi': <gensim.models.keyedvectors.Vocab at 0x24aa84707c0>,\n",
       " 'companion': <gensim.models.keyedvectors.Vocab at 0x24aa8470820>,\n",
       " 'full': <gensim.models.keyedvectors.Vocab at 0x24aa8470880>,\n",
       " 'compress': <gensim.models.keyedvectors.Vocab at 0x24aa84708e0>,\n",
       " 'geometri': <gensim.models.keyedvectors.Vocab at 0x24aa8470940>,\n",
       " 'one_dimension': <gensim.models.keyedvectors.Vocab at 0x24aa84709a0>,\n",
       " 'test': <gensim.models.keyedvectors.Vocab at 0x24aa8470a00>,\n",
       " 'tube': <gensim.models.keyedvectors.Vocab at 0x24aa8470a60>,\n",
       " 'methodolog': <gensim.models.keyedvectors.Vocab at 0x24aa8470ac0>,\n",
       " 'shown': <gensim.models.keyedvectors.Vocab at 0x24aa8470b20>,\n",
       " 'captur': <gensim.models.keyedvectors.Vocab at 0x24aa8470b80>,\n",
       " 'quantit': <gensim.models.keyedvectors.Vocab at 0x24aa8470be0>,\n",
       " 'growth_rate': <gensim.models.keyedvectors.Vocab at 0x24aa8470c40>,\n",
       " 'deciph': <gensim.models.keyedvectors.Vocab at 0x24aa8470ca0>,\n",
       " 'amplif': <gensim.models.keyedvectors.Vocab at 0x24aa8470d00>,\n",
       " 'reduct': <gensim.models.keyedvectors.Vocab at 0x24aa8470d60>,\n",
       " 'open': <gensim.models.keyedvectors.Vocab at 0x24aa8470dc0>,\n",
       " 'chemic': <gensim.models.keyedvectors.Vocab at 0x24aa8470e20>,\n",
       " 'reaction': <gensim.models.keyedvectors.Vocab at 0x24aa8470e80>,\n",
       " 'random': <gensim.models.keyedvectors.Vocab at 0x24aa8470ee0>,\n",
       " 'fluctuat': <gensim.models.keyedvectors.Vocab at 0x24aa8470f40>,\n",
       " 'biolog': <gensim.models.keyedvectors.Vocab at 0x24aa8470fa0>,\n",
       " 'longstand': <gensim.models.keyedvectors.Vocab at 0x24aa8472040>,\n",
       " 'issu': <gensim.models.keyedvectors.Vocab at 0x24aa84720a0>,\n",
       " 'understand': <gensim.models.keyedvectors.Vocab at 0x24aa8472100>,\n",
       " 'shed_light': <gensim.models.keyedvectors.Vocab at 0x24aa8472160>,\n",
       " 'impos': <gensim.models.keyedvectors.Vocab at 0x24aa84721c0>,\n",
       " 'intrins': <gensim.models.keyedvectors.Vocab at 0x24aa8472220>,\n",
       " 'ration': <gensim.models.keyedvectors.Vocab at 0x24aa8472280>,\n",
       " 'differenti_equat': <gensim.models.keyedvectors.Vocab at 0x24aa84722e0>,\n",
       " 'formal': <gensim.models.keyedvectors.Vocab at 0x24aa8472340>,\n",
       " 'contact': <gensim.models.keyedvectors.Vocab at 0x24aa84723a0>,\n",
       " 'action': <gensim.models.keyedvectors.Vocab at 0x24aa8472400>,\n",
       " 'kinet': <gensim.models.keyedvectors.Vocab at 0x24aa8472460>,\n",
       " 'repres': <gensim.models.keyedvectors.Vocab at 0x24aa84724c0>,\n",
       " 'biomolecular': <gensim.models.keyedvectors.Vocab at 0x24aa8472520>,\n",
       " 'breakag': <gensim.models.keyedvectors.Vocab at 0x24aa8472580>,\n",
       " 'cascad': <gensim.models.keyedvectors.Vocab at 0x24aa84725e0>,\n",
       " 'metabol': <gensim.models.keyedvectors.Vocab at 0x24aa8472640>,\n",
       " 'zero': <gensim.models.keyedvectors.Vocab at 0x24aa84726a0>,\n",
       " 'defici': <gensim.models.keyedvectors.Vocab at 0x24aa8472700>,\n",
       " 'admit': <gensim.models.keyedvectors.Vocab at 0x24aa8472760>,\n",
       " 'detail': <gensim.models.keyedvectors.Vocab at 0x24aa84727c0>,\n",
       " 'balanc': <gensim.models.keyedvectors.Vocab at 0x24aa8472820>,\n",
       " 'steadi_state': <gensim.models.keyedvectors.Vocab at 0x24aa8472880>,\n",
       " 'uncorrel': <gensim.models.keyedvectors.Vocab at 0x24aa84728e0>,\n",
       " 'number': <gensim.models.keyedvectors.Vocab at 0x24aa8472940>,\n",
       " 'molecul': <gensim.models.keyedvectors.Vocab at 0x24aa84729a0>,\n",
       " 'follow': <gensim.models.keyedvectors.Vocab at 0x24aa8472a00>,\n",
       " 'fano': <gensim.models.keyedvectors.Vocab at 0x24aa8472a60>,\n",
       " 'equal': <gensim.models.keyedvectors.Vocab at 0x24aa8472ac0>,\n",
       " 'unbalanc': <gensim.models.keyedvectors.Vocab at 0x24aa8472b20>,\n",
       " 'non_equilibrium': <gensim.models.keyedvectors.Vocab at 0x24aa8472b80>,\n",
       " 'non_zero': <gensim.models.keyedvectors.Vocab at 0x24aa8472be0>,\n",
       " 'flux': <gensim.models.keyedvectors.Vocab at 0x24aa8472c40>,\n",
       " 'defin': <gensim.models.keyedvectors.Vocab at 0x24aa8472ca0>,\n",
       " 'multipli': <gensim.models.keyedvectors.Vocab at 0x24aa8472d00>,\n",
       " 'adequ': <gensim.models.keyedvectors.Vocab at 0x24aa8472d60>,\n",
       " 'stoichiometr': <gensim.models.keyedvectors.Vocab at 0x24aa8472dc0>,\n",
       " 'coeffici': <gensim.models.keyedvectors.Vocab at 0x24aa8472e20>,\n",
       " 'lowest': <gensim.models.keyedvectors.Vocab at 0x24aa8472e80>,\n",
       " 'highest': <gensim.models.keyedvectors.Vocab at 0x24aa8472ee0>,\n",
       " 'amplifi': <gensim.models.keyedvectors.Vocab at 0x24aa8472f40>,\n",
       " 'goe': <gensim.models.keyedvectors.Vocab at 0x24aa8472fa0>,\n",
       " 'opposit': <gensim.models.keyedvectors.Vocab at 0x24aa8474040>,\n",
       " 'possess': <gensim.models.keyedvectors.Vocab at 0x24aa84740a0>,\n",
       " 'vanish': <gensim.models.keyedvectors.Vocab at 0x24aa8474100>,\n",
       " 'conjectur': <gensim.models.keyedvectors.Vocab at 0x24aa8474160>,\n",
       " 'hold': <gensim.models.keyedvectors.Vocab at 0x24aa84741c0>,\n",
       " 'mani_bodi': <gensim.models.keyedvectors.Vocab at 0x24aa8474220>,\n",
       " 'stabil': <gensim.models.keyedvectors.Vocab at 0x24aa8474280>,\n",
       " 'instabl': <gensim.models.keyedvectors.Vocab at 0x24aa84742e0>,\n",
       " 'disord': <gensim.models.keyedvectors.Vocab at 0x24aa8474340>,\n",
       " 'griffith': <gensim.models.keyedvectors.Vocab at 0x24aa84743a0>,\n",
       " 'spoil': <gensim.models.keyedvectors.Vocab at 0x24aa8474400>,\n",
       " 'perturb': <gensim.models.keyedvectors.Vocab at 0x24aa8474460>,\n",
       " 'motion': <gensim.models.keyedvectors.Vocab at 0x24aa84744c0>,\n",
       " 'spin_chain': <gensim.models.keyedvectors.Vocab at 0x24aa8474520>,\n",
       " 'dimens': <gensim.models.keyedvectors.Vocab at 0x24aa8474580>,\n",
       " 'reason': <gensim.models.keyedvectors.Vocab at 0x24aa84745e0>,\n",
       " 'idea': <gensim.models.keyedvectors.Vocab at 0x24aa8474640>,\n",
       " 'situat': <gensim.models.keyedvectors.Vocab at 0x24aa84746a0>,\n",
       " 'ensur': <gensim.models.keyedvectors.Vocab at 0x24aa8474700>,\n",
       " 'involv': <gensim.models.keyedvectors.Vocab at 0x24aa8474760>,\n",
       " 'much_smaller': <gensim.models.keyedvectors.Vocab at 0x24aa84747c0>,\n",
       " 'argu': <gensim.models.keyedvectors.Vocab at 0x24aa8474820>,\n",
       " 'ergod': <gensim.models.keyedvectors.Vocab at 0x24aa8474880>,\n",
       " 'restor': <gensim.models.keyedvectors.Vocab at 0x24aa84748e0>,\n",
       " 'although': <gensim.models.keyedvectors.Vocab at 0x24aa8474940>,\n",
       " 'equilibr': <gensim.models.keyedvectors.Vocab at 0x24aa84749a0>,\n",
       " 'extrem': <gensim.models.keyedvectors.Vocab at 0x24aa8474a00>,\n",
       " 'slow': <gensim.models.keyedvectors.Vocab at 0x24aa8474a60>,\n",
       " 'glass': <gensim.models.keyedvectors.Vocab at 0x24aa8474ac0>,\n",
       " 'fault': <gensim.models.keyedvectors.Vocab at 0x24aa8474b20>,\n",
       " 'user': <gensim.models.keyedvectors.Vocab at 0x24aa8474b80>,\n",
       " 'guid': <gensim.models.keyedvectors.Vocab at 0x24aa8474be0>,\n",
       " 'collect': <gensim.models.keyedvectors.Vocab at 0x24aa8474c40>,\n",
       " 'matlab': <gensim.models.keyedvectors.Vocab at 0x24aa8474ca0>,\n",
       " 'implement': <gensim.models.keyedvectors.Vocab at 0x24aa8474d00>,\n",
       " 'comput': <gensim.models.keyedvectors.Vocab at 0x24aa8474d60>,\n",
       " 'chapter': <gensim.models.keyedvectors.Vocab at 0x24aa8474dc0>,\n",
       " 'book': <gensim.models.keyedvectors.Vocab at 0x24aa8474e20>,\n",
       " 'solv': <gensim.models.keyedvectors.Vocab at 0x24aa8474e80>,\n",
       " 'diagnosi': <gensim.models.keyedvectors.Vocab at 0x24aa8474ee0>,\n",
       " 'synthesi': <gensim.models.keyedvectors.Vocab at 0x24aa8474f40>,\n",
       " 'springer': <gensim.models.keyedvectors.Vocab at 0x24aa8474fa0>,\n",
       " 'document': <gensim.models.keyedvectors.Vocab at 0x24aa8477040>,\n",
       " 'version': <gensim.models.keyedvectors.Vocab at 0x24aa84770a0>,\n",
       " 'background': <gensim.models.keyedvectors.Vocab at 0x24aa8477100>,\n",
       " 'exact': <gensim.models.keyedvectors.Vocab at 0x24aa8477160>,\n",
       " 'filter': <gensim.models.keyedvectors.Vocab at 0x24aa84771c0>,\n",
       " 'give': <gensim.models.keyedvectors.Vocab at 0x24aa8477220>,\n",
       " 'depth': <gensim.models.keyedvectors.Vocab at 0x24aa8477280>,\n",
       " 'command': <gensim.models.keyedvectors.Vocab at 0x24aa84772e0>,\n",
       " 'syntax': <gensim.models.keyedvectors.Vocab at 0x24aa8477340>,\n",
       " 'illustr': <gensim.models.keyedvectors.Vocab at 0x24aa84773a0>,\n",
       " ...}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "word_vectors.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "    \n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "    \n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(wv,text_list):\n",
    "    return np.vstack([word_averaging(wv,post) for post in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15729, 500)\n",
      "(5243, 500)\n"
     ]
    }
   ],
   "source": [
    "# Split the train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df['Computer Science'], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "\n",
    "train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "\n",
    "xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "\n",
    "print(xtrain_wa.shape)\n",
    "print(xtest_wa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008773</td>\n",
       "      <td>-0.017558</td>\n",
       "      <td>-0.014966</td>\n",
       "      <td>0.074738</td>\n",
       "      <td>0.032327</td>\n",
       "      <td>-0.005938</td>\n",
       "      <td>-0.041153</td>\n",
       "      <td>-0.069079</td>\n",
       "      <td>-0.051088</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016749</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>-0.027267</td>\n",
       "      <td>-0.058535</td>\n",
       "      <td>0.007023</td>\n",
       "      <td>-0.005435</td>\n",
       "      <td>-0.008344</td>\n",
       "      <td>-0.010225</td>\n",
       "      <td>-0.021100</td>\n",
       "      <td>-0.124291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037156</td>\n",
       "      <td>0.084054</td>\n",
       "      <td>-0.017546</td>\n",
       "      <td>0.088451</td>\n",
       "      <td>-0.045776</td>\n",
       "      <td>-0.057364</td>\n",
       "      <td>-0.051368</td>\n",
       "      <td>-0.081209</td>\n",
       "      <td>0.022541</td>\n",
       "      <td>-0.063956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022733</td>\n",
       "      <td>-0.073145</td>\n",
       "      <td>-0.024903</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.037835</td>\n",
       "      <td>-0.048836</td>\n",
       "      <td>-0.094771</td>\n",
       "      <td>0.071984</td>\n",
       "      <td>-0.027724</td>\n",
       "      <td>-0.004475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039981</td>\n",
       "      <td>0.044136</td>\n",
       "      <td>0.053685</td>\n",
       "      <td>0.104825</td>\n",
       "      <td>-0.027556</td>\n",
       "      <td>-0.030685</td>\n",
       "      <td>-0.048549</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.014587</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>-0.067751</td>\n",
       "      <td>0.008118</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>0.055134</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>-0.075216</td>\n",
       "      <td>0.019505</td>\n",
       "      <td>-0.033770</td>\n",
       "      <td>-0.073987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.024030</td>\n",
       "      <td>0.075454</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.063421</td>\n",
       "      <td>-0.039212</td>\n",
       "      <td>-0.039182</td>\n",
       "      <td>0.052440</td>\n",
       "      <td>-0.072764</td>\n",
       "      <td>0.086223</td>\n",
       "      <td>-0.012484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035055</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>0.037790</td>\n",
       "      <td>-0.026623</td>\n",
       "      <td>-0.028276</td>\n",
       "      <td>-0.041030</td>\n",
       "      <td>-0.037071</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>-0.049297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.081890</td>\n",
       "      <td>0.050731</td>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.149374</td>\n",
       "      <td>-0.044353</td>\n",
       "      <td>-0.050988</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-0.038763</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.064335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>-0.053701</td>\n",
       "      <td>0.063147</td>\n",
       "      <td>0.032049</td>\n",
       "      <td>-0.013313</td>\n",
       "      <td>0.025958</td>\n",
       "      <td>-0.048271</td>\n",
       "      <td>-0.030183</td>\n",
       "      <td>-0.047121</td>\n",
       "      <td>-0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15724</th>\n",
       "      <td>0.024405</td>\n",
       "      <td>0.068945</td>\n",
       "      <td>-0.040952</td>\n",
       "      <td>0.093245</td>\n",
       "      <td>-0.016080</td>\n",
       "      <td>-0.003609</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>-0.026245</td>\n",
       "      <td>-0.032424</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006013</td>\n",
       "      <td>-0.092882</td>\n",
       "      <td>-0.002878</td>\n",
       "      <td>-0.031686</td>\n",
       "      <td>0.023343</td>\n",
       "      <td>-0.014281</td>\n",
       "      <td>-0.094264</td>\n",
       "      <td>0.054955</td>\n",
       "      <td>-0.047740</td>\n",
       "      <td>-0.001219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>-0.065551</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>-0.080563</td>\n",
       "      <td>0.108847</td>\n",
       "      <td>0.056295</td>\n",
       "      <td>0.035416</td>\n",
       "      <td>-0.006369</td>\n",
       "      <td>-0.082752</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047534</td>\n",
       "      <td>-0.055977</td>\n",
       "      <td>-0.023527</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>-0.041622</td>\n",
       "      <td>-0.062967</td>\n",
       "      <td>0.030662</td>\n",
       "      <td>-0.034561</td>\n",
       "      <td>0.052897</td>\n",
       "      <td>-0.048324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15726</th>\n",
       "      <td>0.027851</td>\n",
       "      <td>0.108637</td>\n",
       "      <td>-0.020858</td>\n",
       "      <td>0.061606</td>\n",
       "      <td>0.024265</td>\n",
       "      <td>0.059007</td>\n",
       "      <td>-0.063916</td>\n",
       "      <td>-0.012227</td>\n",
       "      <td>-0.065004</td>\n",
       "      <td>-0.090870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055292</td>\n",
       "      <td>-0.027657</td>\n",
       "      <td>-0.051533</td>\n",
       "      <td>-0.023564</td>\n",
       "      <td>-0.041488</td>\n",
       "      <td>-0.054770</td>\n",
       "      <td>-0.052391</td>\n",
       "      <td>0.022434</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>-0.077410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15727</th>\n",
       "      <td>-0.010995</td>\n",
       "      <td>0.022113</td>\n",
       "      <td>0.010424</td>\n",
       "      <td>0.122031</td>\n",
       "      <td>-0.081740</td>\n",
       "      <td>-0.066552</td>\n",
       "      <td>-0.018385</td>\n",
       "      <td>-0.028067</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>-0.006074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054260</td>\n",
       "      <td>-0.005339</td>\n",
       "      <td>0.060367</td>\n",
       "      <td>0.054964</td>\n",
       "      <td>-0.049919</td>\n",
       "      <td>0.024668</td>\n",
       "      <td>-0.012224</td>\n",
       "      <td>0.116266</td>\n",
       "      <td>-0.010424</td>\n",
       "      <td>-0.008453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15728</th>\n",
       "      <td>-0.014360</td>\n",
       "      <td>0.035485</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>0.129206</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>-0.088011</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>-0.018697</td>\n",
       "      <td>0.036779</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032614</td>\n",
       "      <td>-0.065346</td>\n",
       "      <td>0.077003</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.040547</td>\n",
       "      <td>-0.059981</td>\n",
       "      <td>0.024218</td>\n",
       "      <td>-0.052617</td>\n",
       "      <td>-0.025563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15729 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.008773 -0.017558 -0.014966  0.074738  0.032327 -0.005938 -0.041153   \n",
       "1     -0.037156  0.084054 -0.017546  0.088451 -0.045776 -0.057364 -0.051368   \n",
       "2     -0.039981  0.044136  0.053685  0.104825 -0.027556 -0.030685 -0.048549   \n",
       "3     -0.024030  0.075454  0.025006  0.063421 -0.039212 -0.039182  0.052440   \n",
       "4     -0.081890  0.050731  0.027753  0.149374 -0.044353 -0.050988  0.013249   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "15724  0.024405  0.068945 -0.040952  0.093245 -0.016080 -0.003609  0.000768   \n",
       "15725 -0.065551  0.033206 -0.080563  0.108847  0.056295  0.035416 -0.006369   \n",
       "15726  0.027851  0.108637 -0.020858  0.061606  0.024265  0.059007 -0.063916   \n",
       "15727 -0.010995  0.022113  0.010424  0.122031 -0.081740 -0.066552 -0.018385   \n",
       "15728 -0.014360  0.035485 -0.000074  0.129206  0.016949 -0.088011  0.002382   \n",
       "\n",
       "            7         8         9    ...       490       491       492  \\\n",
       "0     -0.069079 -0.051088  0.005554  ...  0.016749 -0.012772 -0.027267   \n",
       "1     -0.081209  0.022541 -0.063956  ... -0.022733 -0.073145 -0.024903   \n",
       "2      0.000113 -0.014587  0.002255  ...  0.018677 -0.067751  0.008118   \n",
       "3     -0.072764  0.086223 -0.012484  ...  0.035055 -0.040039  0.037790   \n",
       "4     -0.038763 -0.000135 -0.064335  ...  0.003003 -0.053701  0.063147   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "15724 -0.026245 -0.032424 -0.025602  ... -0.006013 -0.092882 -0.002878   \n",
       "15725 -0.082752  0.029485  0.003059  ...  0.047534 -0.055977 -0.023527   \n",
       "15726 -0.012227 -0.065004 -0.090870  ...  0.055292 -0.027657 -0.051533   \n",
       "15727 -0.028067  0.020064 -0.006074  ... -0.054260 -0.005339  0.060367   \n",
       "15728 -0.018697  0.036779  0.011996  ... -0.032614 -0.065346  0.077003   \n",
       "\n",
       "            493       494       495       496       497       498       499  \n",
       "0     -0.058535  0.007023 -0.005435 -0.008344 -0.010225 -0.021100 -0.124291  \n",
       "1      0.006073  0.037835 -0.048836 -0.094771  0.071984 -0.027724 -0.004475  \n",
       "2      0.019448  0.055134  0.000230 -0.075216  0.019505 -0.033770 -0.073987  \n",
       "3     -0.026623 -0.028276 -0.041030 -0.037071  0.011737  0.012102 -0.049297  \n",
       "4      0.032049 -0.013313  0.025958 -0.048271 -0.030183 -0.047121 -0.055100  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "15724 -0.031686  0.023343 -0.014281 -0.094264  0.054955 -0.047740 -0.001219  \n",
       "15725 -0.073593 -0.041622 -0.062967  0.030662 -0.034561  0.052897 -0.048324  \n",
       "15726 -0.023564 -0.041488 -0.054770 -0.052391  0.022434  0.007858 -0.077410  \n",
       "15727  0.054964 -0.049919  0.024668 -0.012224  0.116266 -0.010424 -0.008453  \n",
       "15728  0.017582  0.018828  0.040547 -0.059981  0.024218 -0.052617 -0.025563  \n",
       "\n",
       "[15729 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(xtrain_wa.shape[1])\n",
    "xtrain_wa_df = pd.DataFrame(xtrain_wa)\n",
    "display(xtrain_wa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sigmoid function from keras backend and using it\n",
    "from keras.backend import sigmoid\n",
    "def swish(x,beta = 1):\n",
    "    return (x*sigmoid(beta*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the custom object and updating them\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8633185  0.86745071 0.87857598 0.86681503 0.86486489]\n",
      "Baseline: 86.82050228%  (0.53868807%)\n"
     ]
    }
   ],
   "source": [
    "# using SVM\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "n_cols = xtrain_wa_df.shape[1]\n",
    "n_rows = xtrain_wa_df.shape[0]\n",
    "\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50,activation = 'swish',input_dim = n_cols))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model\n",
    "estimator = KerasClassifier(build_fn = create_baseline, epochs = 15, batch_size = 5, verbose = 0)\n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "results = cross_val_score(estimator, xtrain_wa_df, ytrain, cv = kfold)\n",
    "print(results)\n",
    "print(\"Baseline: %.8f%%  (%.8f%%)\" % (results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 11:38:46: From C:\\Users\\ujjwal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting on first dataset\n",
    "estimator = KerasClassifier(build_fn = create_baseline, epochs = 10, batch_size = 5, verbose = 0)\n",
    "estimator.fit(xtrain_wa,ytrain)\n",
    "y_pred = estimator.predict(xtest_wa)\n",
    "display(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.8670608430288003\n",
      "f1_score: 0.8670608430288003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      3127\n",
      "           1       0.79      0.91      0.85      2116\n",
      "\n",
      "    accuracy                           0.87      5243\n",
      "   macro avg       0.86      0.87      0.86      5243\n",
      "weighted avg       0.87      0.87      0.87      5243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluating f1-score\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "print(\"accuracy_score:\", accuracy_score(ytest,y_pred))\n",
    "print(\"f1_score:\", f1_score(ytest,y_pred,average = 'micro'))\n",
    "print(classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8989, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing on the final test set\n",
    "x_test_2 = df_test_2['title_abstract_combined_processed']\n",
    "test_2_tokenized = x_test_2.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "\n",
    "xtest_2_wa = word_averaging_list(word_vectors,test_2_tokenized)\n",
    "\n",
    "print(xtest_2_wa.shape)\n",
    "y_pred_2 = estimator.predict(xtest_2_wa)\n",
    "display(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Computer Science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8984</th>\n",
       "      <td>29957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8985</th>\n",
       "      <td>29958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8986</th>\n",
       "      <td>29959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8987</th>\n",
       "      <td>29960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8988</th>\n",
       "      <td>29961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Computer Science\n",
       "0     20973                 0\n",
       "1     20974                 0\n",
       "2     20975                 1\n",
       "3     20976                 0\n",
       "4     20977                 1\n",
       "...     ...               ...\n",
       "8984  29957                 1\n",
       "8985  29958                 1\n",
       "8986  29959                 1\n",
       "8987  29960                 0\n",
       "8988  29961                 1\n",
       "\n",
       "[8989 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adding the results in the final dataframe\n",
    "df_test_fs = pd.read_csv('test.csv')\n",
    "df_test_fs = df_test_fs.drop(labels = ['TITLE','ABSTRACT'],axis = 1)\n",
    "y_pred_list = np.array(y_pred_2).tolist()\n",
    "df_pred = pd.DataFrame(y_pred_list)\n",
    "df_test_fs['Computer Science'] = df_pred\n",
    "display(df_test_fs)\n",
    "#df_test_fs.to_csv('submission_u_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93579149 0.93006992 0.93261284 0.93706292 0.93322736]\n",
      "Baseline: 93.37529063%  (0.24597699%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90050858 0.90495867 0.91004449 0.90781945 0.89443558]\n",
      "Baseline: 90.35533547%  (0.55626197%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88175464 0.88652259 0.89446914 0.88652259 0.89666134]\n",
      "Baseline: 88.91860604%  (0.55354308%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97361732 0.97584236 0.97043866 0.97584236 0.97774243]\n",
      "Baseline: 97.46966243%  (0.24979005%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98982835 0.98919261 0.99046409 0.98887473 0.99077904]\n",
      "Baseline: 98.98277640%  (0.07240908%)\n"
     ]
    }
   ],
   "source": [
    "# Looping through other labels\n",
    "for i in range(len(tagnames)-1):\n",
    "    xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df[tagnames[i+1]], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "    \n",
    "    train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    \n",
    "    xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "    xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "    # evaluate model\n",
    "    estimator = KerasClassifier(build_fn = create_baseline, epochs = 15, batch_size = 5, verbose = 0)\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "    results = cross_val_score(estimator, xtrain_wa_df, ytrain, cv = kfold)\n",
    "    print(results)\n",
    "    print(\"Baseline: %.8f%%  (%.8f%%)\" % (results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9313370207896242\n",
      "f1_score: 0.9313370207896242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95      3716\n",
      "           1       0.91      0.85      0.88      1527\n",
      "\n",
      "    accuracy                           0.93      5243\n",
      "   macro avg       0.92      0.91      0.92      5243\n",
      "weighted avg       0.93      0.93      0.93      5243\n",
      "\n",
      "(8989, 500)\n",
      "accuracy_score: 0.9029181766164409\n",
      "f1_score: 0.9029181766164409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94      3815\n",
      "           1       0.90      0.72      0.80      1428\n",
      "\n",
      "    accuracy                           0.90      5243\n",
      "   macro avg       0.90      0.85      0.87      5243\n",
      "weighted avg       0.90      0.90      0.90      5243\n",
      "\n",
      "(8989, 500)\n",
      "accuracy_score: 0.8899485027655922\n",
      "f1_score: 0.8899485027655922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93      3937\n",
      "           1       0.77      0.79      0.78      1306\n",
      "\n",
      "    accuracy                           0.89      5243\n",
      "   macro avg       0.85      0.86      0.85      5243\n",
      "weighted avg       0.89      0.89      0.89      5243\n",
      "\n",
      "(8989, 500)\n",
      "accuracy_score: 0.9702460423421705\n",
      "f1_score: 0.9702460423421705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      5076\n",
      "           1       0.57      0.26      0.36       167\n",
      "\n",
      "    accuracy                           0.97      5243\n",
      "   macro avg       0.77      0.63      0.67      5243\n",
      "weighted avg       0.96      0.97      0.96      5243\n",
      "\n",
      "(8989, 500)\n"
     ]
    }
   ],
   "source": [
    "# Looping through other labels\n",
    "for i in range(len(tagnames)-1):\n",
    "    # Split the train and test dataset\n",
    "    xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df[tagnames[i+1]], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "    \n",
    "    train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    \n",
    "    xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "    xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "    \n",
    "    # predicting on first dataset\n",
    "    estimator = KerasClassifier(build_fn = create_baseline, epochs = 15, batch_size = 5, verbose = 0)\n",
    "    estimator.fit(xtrain_wa,ytrain)\n",
    "    y_pred = estimator.predict(xtest_wa)\n",
    "    print(\"accuracy_score:\", accuracy_score(ytest,y_pred))\n",
    "    print(\"f1_score:\", f1_score(ytest,y_pred,average = 'micro'))\n",
    "    print(classification_report(ytest,y_pred))\n",
    "    \n",
    "    x_test_2 = df_test_2['title_abstract_combined_processed']\n",
    "    test_2_tokenized = x_test_2.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    xtest_2_wa = word_averaging_list(word_vectors,test_2_tokenized)\n",
    "    print(xtest_2_wa.shape)\n",
    "    y_pred_2 = estimator.predict(xtest_2_wa)\n",
    "    \n",
    "    y_pred_list = np.array(y_pred_2).tolist()\n",
    "    df_pred = pd.DataFrame(y_pred_list)\n",
    "    df_test_fs[tagnames[i+1]] = df_pred\n",
    "display(df_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the final results in proper format\n",
    "df_test_fs.to_csv('submission_u_keras_w2v.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
