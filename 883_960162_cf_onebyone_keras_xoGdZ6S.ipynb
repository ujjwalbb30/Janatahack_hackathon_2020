{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>We present novel understandings of the Gamma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
       "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>Milky Way open clusters are very diverse in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>Proving that a cryptographic protocol is cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              TITLE  \\\n",
       "0  20973  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  20974  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  20975         Case For Static AMSDU Aggregation in WLANs   \n",
       "3  20976  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  20977  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                                            ABSTRACT  \n",
       "0    We present novel understandings of the Gamma...  \n",
       "1    Meteorites contain minerals from Solar Syste...  \n",
       "2    Frame aggregation is a mechanism by which mu...  \n",
       "3    Milky Way open clusters are very diverse in ...  \n",
       "4    Proving that a cryptographic protocol is cor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import pandas and numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#read the train csv file and explore\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test_2 = pd.read_csv('test.csv')\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shape of the dataframe is:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20972, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'column names are as follows:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'TITLE', 'ABSTRACT', 'Computer Science', 'Physics', 'Mathematics',\n",
       "       'Statistics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#perform EDA on the dataframe\n",
    "display(\"shape of the dataframe is:\",df.shape)\n",
    "display(\"column names are as follows:\",df.columns)\n",
    "index_final = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20972 entries, 0 to 20971\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   ID                    20972 non-null  int64 \n",
      " 1   TITLE                 20972 non-null  object\n",
      " 2   ABSTRACT              20972 non-null  object\n",
      " 3   Computer Science      20972 non-null  int64 \n",
      " 4   Physics               20972 non-null  int64 \n",
      " 5   Mathematics           20972 non-null  int64 \n",
      " 6   Statistics            20972 non-null  int64 \n",
      " 7   Quantitative Biology  20972 non-null  int64 \n",
      " 8   Quantitative Finance  20972 non-null  int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10486.500000</td>\n",
       "      <td>0.409784</td>\n",
       "      <td>0.286716</td>\n",
       "      <td>0.267881</td>\n",
       "      <td>0.248236</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>0.011873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6054.239259</td>\n",
       "      <td>0.491806</td>\n",
       "      <td>0.452238</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.164947</td>\n",
       "      <td>0.108317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5243.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10486.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15729.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20972.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  Computer Science       Physics   Mathematics  \\\n",
       "count  20972.000000      20972.000000  20972.000000  20972.000000   \n",
       "mean   10486.500000          0.409784      0.286716      0.267881   \n",
       "std     6054.239259          0.491806      0.452238      0.442866   \n",
       "min        1.000000          0.000000      0.000000      0.000000   \n",
       "25%     5243.750000          0.000000      0.000000      0.000000   \n",
       "50%    10486.500000          0.000000      0.000000      0.000000   \n",
       "75%    15729.250000          1.000000      1.000000      1.000000   \n",
       "max    20972.000000          1.000000      1.000000      1.000000   \n",
       "\n",
       "         Statistics  Quantitative Biology  Quantitative Finance  \n",
       "count  20972.000000          20972.000000          20972.000000  \n",
       "mean       0.248236              0.027990              0.011873  \n",
       "std        0.432000              0.164947              0.108317  \n",
       "min        0.000000              0.000000              0.000000  \n",
       "25%        0.000000              0.000000              0.000000  \n",
       "50%        0.000000              0.000000              0.000000  \n",
       "75%        0.000000              0.000000              0.000000  \n",
       "max        1.000000              1.000000              1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer Science',\n",
       " 'Physics',\n",
       " 'Mathematics',\n",
       " 'Statistics',\n",
       " 'Quantitative Biology',\n",
       " 'Quantitative Finance']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the number of research paper under each tag\n",
    "#tag names\n",
    "tagnames = df.drop(['ID','TITLE','ABSTRACT'],axis=1).columns.tolist()\n",
    "display(tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Computer Science': 8594,\n",
       " 'Physics': 6013,\n",
       " 'Mathematics': 5618,\n",
       " 'Statistics': 5206,\n",
       " 'Quantitative Biology': 587,\n",
       " 'Quantitative Finance': 249}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagno = {}\n",
    "for name in tagnames:\n",
    "    tagno.update({name:df[name].sum()})\n",
    "display(tagno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib and plot number of research paper under each tag\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(tagno)),list(tagno.values()),align ='center')\n",
    "plt.xticks(range(len(tagno)),list(tagno.keys()),rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum tags that belong to a research paper are:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18336f30640>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the maximum number of tags present for a research paper\n",
    "df['combinations'] = df['Computer Science'] + df['Physics'] + df['Mathematics'] + df['Statistics'] + df['Quantitative Biology'] + df['Quantitative Finance']\n",
    "display(\"maximum tags that belong to a research paper are:\", df['combinations'].max())\n",
    "df['combinations'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Computer Science', 'Physics'],\n",
       " ['Computer Science', 'Mathematics'],\n",
       " ['Computer Science', 'Statistics'],\n",
       " ['Computer Science', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Quantitative Finance'],\n",
       " ['Physics', 'Mathematics'],\n",
       " ['Physics', 'Statistics'],\n",
       " ['Physics', 'Quantitative Biology'],\n",
       " ['Physics', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Statistics'],\n",
       " ['Mathematics', 'Quantitative Biology'],\n",
       " ['Mathematics', 'Quantitative Finance'],\n",
       " ['Statistics', 'Quantitative Biology'],\n",
       " ['Statistics', 'Quantitative Finance'],\n",
       " ['Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Physics', 'Mathematics'],\n",
       " ['Computer Science', 'Physics', 'Statistics'],\n",
       " ['Computer Science', 'Physics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Physics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Mathematics', 'Statistics'],\n",
       " ['Computer Science', 'Mathematics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Mathematics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Statistics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Statistics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Physics', 'Mathematics', 'Statistics'],\n",
       " ['Physics', 'Mathematics', 'Quantitative Biology'],\n",
       " ['Physics', 'Mathematics', 'Quantitative Finance'],\n",
       " ['Physics', 'Statistics', 'Quantitative Biology'],\n",
       " ['Physics', 'Statistics', 'Quantitative Finance'],\n",
       " ['Physics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Statistics', 'Quantitative Biology'],\n",
       " ['Mathematics', 'Statistics', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Statistics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Computer Science'],\n",
       " ['Physics'],\n",
       " ['Mathematics'],\n",
       " ['Statistics'],\n",
       " ['Quantitative Biology'],\n",
       " ['Quantitative Finance']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'total tag combinations that can be possibly present:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find all unique combinations of 3 tags\n",
    "from itertools import combinations\n",
    "comb2,comb3 = list(combinations(tagnames,2)),list(combinations(tagnames,3))\n",
    "totcomb = [list(ele) for ele in comb2+comb3]\n",
    "totcomb = totcomb + [[el] for el in tagnames]\n",
    "display(totcomb)\n",
    "display(\"total tag combinations that can be possibly present:\", len(totcomb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Computer Science',): 4910,\n",
       "         ('Mathematics',): 3610,\n",
       "         ('Computer Science', 'Statistics'): 2285,\n",
       "         ('Physics',): 5120,\n",
       "         ('Quantitative Biology',): 443,\n",
       "         ('Statistics',): 1636,\n",
       "         ('Physics', 'Mathematics'): 293,\n",
       "         ('Mathematics', 'Statistics'): 825,\n",
       "         ('Computer Science', 'Mathematics'): 682,\n",
       "         ('Quantitative Finance',): 209,\n",
       "         ('Computer Science', 'Physics'): 437,\n",
       "         ('Computer Science', 'Mathematics', 'Statistics'): 179,\n",
       "         ('Physics', 'Statistics'): 99,\n",
       "         ('Computer Science', 'Physics', 'Statistics'): 36,\n",
       "         ('Computer Science', 'Quantitative Biology'): 30,\n",
       "         ('Statistics', 'Quantitative Biology'): 105,\n",
       "         ('Statistics', 'Quantitative Finance'): 24,\n",
       "         ('Physics', 'Mathematics', 'Statistics'): 9,\n",
       "         ('Computer Science', 'Quantitative Finance'): 9,\n",
       "         ('Quantitative Biology', 'Quantitative Finance'): 4,\n",
       "         ('Computer Science', 'Statistics', 'Quantitative Biology'): 5,\n",
       "         ('Computer Science', 'Physics', 'Mathematics'): 19,\n",
       "         ('Computer Science', 'Statistics', 'Quantitative Finance'): 2,\n",
       "         ('Mathematics', 'Statistics', 'Quantitative Finance'): 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'number of unique combinations'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the counts of all unique combinations of tags\n",
    "from collections import Counter\n",
    "co = []\n",
    "for index, row in df.iterrows():\n",
    "    l = []\n",
    "    for name in tagnames:\n",
    "        if row[name] == 1:\n",
    "            l.append(name)\n",
    "    co.append(l)\n",
    "Ot = Counter([tuple(i) for i in co])\n",
    "display(Ot,\"number of unique combinations\",len(Ot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reconstructing',\n",
       " 'Subject-Specific',\n",
       " 'Effect',\n",
       " 'Maps',\n",
       " 'Rotation',\n",
       " 'Invariance',\n",
       " 'Neural',\n",
       " 'Network',\n",
       " 'Spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'A',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'Maxwell--Landau--Lifshitz--Gilbert',\n",
       " 'system',\n",
       " 'Comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'Discrete',\n",
       " 'Wavelet',\n",
       " 'Transforms',\n",
       " 'and',\n",
       " 'Wavelet',\n",
       " 'Tensor',\n",
       " 'Train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'FTIR',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'On',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'On',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " '1I/`Oumuamua',\n",
       " '(2017)',\n",
       " 'U1',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'Adverse',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'polymer',\n",
       " 'coating',\n",
       " 'on',\n",
       " 'heat',\n",
       " 'transport',\n",
       " 'at',\n",
       " 'solid-liquid',\n",
       " 'interface',\n",
       " 'SPH',\n",
       " 'calculations',\n",
       " 'of',\n",
       " 'Mars-scale',\n",
       " 'collisions:',\n",
       " 'the',\n",
       " 'role',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Equation',\n",
       " 'of',\n",
       " 'State,',\n",
       " 'material',\n",
       " 'rheologies,',\n",
       " 'and',\n",
       " 'numerical',\n",
       " 'effects',\n",
       " '$\\\\mathcal{R}_{0}$',\n",
       " 'fails',\n",
       " 'to',\n",
       " 'predict',\n",
       " 'the',\n",
       " 'outbreak',\n",
       " 'potential',\n",
       " 'in',\n",
       " 'the',\n",
       " 'presence',\n",
       " 'of',\n",
       " 'natural-boosting',\n",
       " 'immunity',\n",
       " 'A',\n",
       " 'global',\n",
       " 'sensitivity',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'reduced',\n",
       " 'order',\n",
       " 'models',\n",
       " 'for',\n",
       " 'hydraulically-fractured',\n",
       " 'horizontal',\n",
       " 'wells',\n",
       " 'Role-separating',\n",
       " 'ordering',\n",
       " 'in',\n",
       " 'social',\n",
       " 'dilemmas',\n",
       " 'controlled',\n",
       " 'by',\n",
       " 'topological',\n",
       " 'frustration',\n",
       " 'Dynamics',\n",
       " 'of',\n",
       " 'exciton',\n",
       " 'magnetic',\n",
       " 'polarons',\n",
       " 'in',\n",
       " 'CdMnSe/CdMgSe',\n",
       " 'quantum',\n",
       " 'wells:',\n",
       " 'the',\n",
       " 'effect',\n",
       " 'of',\n",
       " 'self-localization',\n",
       " 'On',\n",
       " 'Varieties',\n",
       " 'of',\n",
       " 'Ordered',\n",
       " 'Automata',\n",
       " 'Direct',\n",
       " 'Evidence',\n",
       " 'of',\n",
       " 'Spontaneous',\n",
       " 'Abrikosov',\n",
       " 'Vortex',\n",
       " 'State',\n",
       " 'in',\n",
       " 'Ferromagnetic',\n",
       " 'Superconductor',\n",
       " 'EuFe$_2$(As$_{1-x}$P$_x$)$_2$',\n",
       " 'with',\n",
       " '$x=0.21$',\n",
       " 'A',\n",
       " 'rank',\n",
       " '18',\n",
       " 'Waring',\n",
       " 'decomposition',\n",
       " 'of',\n",
       " '$sM_{\\\\langle',\n",
       " '3\\\\rangle}$',\n",
       " 'with',\n",
       " '432',\n",
       " 'symmetries',\n",
       " 'The',\n",
       " 'PdBI',\n",
       " 'Arcsecond',\n",
       " 'Whirlpool',\n",
       " 'Survey',\n",
       " '(PAWS).',\n",
       " 'The',\n",
       " 'Role',\n",
       " 'of',\n",
       " 'Spiral',\n",
       " 'Arms',\n",
       " 'in',\n",
       " 'Cloud',\n",
       " 'and',\n",
       " 'Star',\n",
       " 'Formation',\n",
       " 'Higher',\n",
       " 'structure',\n",
       " 'in',\n",
       " 'the',\n",
       " 'unstable',\n",
       " 'Adams',\n",
       " 'spectral',\n",
       " 'sequence',\n",
       " 'Comparing',\n",
       " 'Covariate',\n",
       " 'Prioritization',\n",
       " 'via',\n",
       " 'Matching',\n",
       " 'to',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Methods',\n",
       " 'for',\n",
       " 'Causal',\n",
       " 'Inference',\n",
       " 'using',\n",
       " 'Five',\n",
       " 'Empirical',\n",
       " 'Applications',\n",
       " 'Acoustic',\n",
       " 'Impedance',\n",
       " 'Calculation',\n",
       " 'via',\n",
       " 'Numerical',\n",
       " 'Solution',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Inverse',\n",
       " 'Helmholtz',\n",
       " 'Problem',\n",
       " 'Deciphering',\n",
       " 'noise',\n",
       " 'amplification',\n",
       " 'and',\n",
       " 'reduction',\n",
       " 'in',\n",
       " 'open',\n",
       " 'chemical',\n",
       " 'reaction',\n",
       " 'networks',\n",
       " 'Many-Body',\n",
       " 'Localization:',\n",
       " 'Stability',\n",
       " 'and',\n",
       " 'Instability',\n",
       " 'Fault',\n",
       " 'Detection',\n",
       " 'and',\n",
       " 'Isolation',\n",
       " 'Tools',\n",
       " '(FDITOOLS)',\n",
       " \"User's\",\n",
       " 'Guide',\n",
       " 'Complexity',\n",
       " 'of',\n",
       " 'Deciding',\n",
       " 'Detectability',\n",
       " 'in',\n",
       " 'Discrete',\n",
       " 'Event',\n",
       " 'Systems',\n",
       " 'The',\n",
       " 'Knaster-Tarski',\n",
       " 'theorem',\n",
       " 'versus',\n",
       " 'monotone',\n",
       " 'nonexpansive',\n",
       " 'mappings',\n",
       " 'Efficient',\n",
       " 'methods',\n",
       " 'for',\n",
       " 'computing',\n",
       " 'integrals',\n",
       " 'in',\n",
       " 'electronic',\n",
       " 'structure',\n",
       " 'calculations',\n",
       " 'Diffraction-Aware',\n",
       " 'Sound',\n",
       " 'Localization',\n",
       " 'for',\n",
       " 'a',\n",
       " 'Non-Line-of-Sight',\n",
       " 'Source',\n",
       " \"Jacob's\",\n",
       " 'ladders,',\n",
       " 'crossbreeding',\n",
       " 'in',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " '$$-factorization',\n",
       " 'formulas',\n",
       " 'and',\n",
       " 'selection',\n",
       " 'of',\n",
       " 'families',\n",
       " 'of',\n",
       " '$$-kindred',\n",
       " 'real',\n",
       " 'continuous',\n",
       " 'functions',\n",
       " 'Minimax',\n",
       " 'Estimation',\n",
       " 'of',\n",
       " 'the',\n",
       " '$L_1$',\n",
       " 'Distance',\n",
       " 'Density',\n",
       " 'large',\n",
       " 'deviations',\n",
       " 'for',\n",
       " 'multidimensional',\n",
       " 'stochastic',\n",
       " 'hyperbolic',\n",
       " 'conservation',\n",
       " 'laws',\n",
       " 'mixup:',\n",
       " 'Beyond',\n",
       " 'Empirical',\n",
       " 'Risk',\n",
       " 'Minimization',\n",
       " 'Equality',\n",
       " 'of',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'definitions',\n",
       " 'of',\n",
       " 'Brakke',\n",
       " 'flow',\n",
       " 'Dynamic',\n",
       " 'Base',\n",
       " 'Station',\n",
       " 'Repositioning',\n",
       " 'to',\n",
       " 'Improve',\n",
       " 'Spectral',\n",
       " 'Efficiency',\n",
       " 'of',\n",
       " 'Drone',\n",
       " 'Small',\n",
       " 'Cells',\n",
       " 'An',\n",
       " 'Unsupervised',\n",
       " 'Homogenization',\n",
       " 'Pipeline',\n",
       " 'for',\n",
       " 'Clustering',\n",
       " 'Similar',\n",
       " 'Patients',\n",
       " 'using',\n",
       " 'Electronic',\n",
       " 'Health',\n",
       " 'Record',\n",
       " 'Data',\n",
       " 'Deep',\n",
       " 'Neural',\n",
       " 'Network',\n",
       " 'Optimized',\n",
       " 'to',\n",
       " 'Resistive',\n",
       " 'Memory',\n",
       " 'with',\n",
       " 'Nonlinear',\n",
       " 'Current-Voltage',\n",
       " 'Characteristics',\n",
       " 'Rate-Distortion',\n",
       " 'Region',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Gray-Wyner',\n",
       " 'Model',\n",
       " 'with',\n",
       " 'Side',\n",
       " 'Information',\n",
       " 'Fourier-based',\n",
       " 'numerical',\n",
       " 'approximation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Weertman',\n",
       " 'equation',\n",
       " 'for',\n",
       " 'moving',\n",
       " 'dislocations',\n",
       " 'Design',\n",
       " 'Decisions',\n",
       " 'for',\n",
       " 'Weave:',\n",
       " 'A',\n",
       " 'Real-Time',\n",
       " 'Web-based',\n",
       " 'Collaborative',\n",
       " 'Visualization',\n",
       " 'Framework',\n",
       " 'Suzaku',\n",
       " 'Analysis',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Supernova',\n",
       " 'Remnant',\n",
       " 'G306.3-0.9',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Gamma-ray',\n",
       " 'View',\n",
       " 'of',\n",
       " 'Its',\n",
       " 'Neighborhood',\n",
       " 'Japanese',\n",
       " 'Sentiment',\n",
       " 'Classification',\n",
       " 'using',\n",
       " 'a',\n",
       " 'Tree-Structured',\n",
       " 'Long',\n",
       " 'Short-Term',\n",
       " 'Memory',\n",
       " 'with',\n",
       " 'Attention',\n",
       " 'Covariances,',\n",
       " 'Robustness,',\n",
       " 'and',\n",
       " 'Variational',\n",
       " 'Bayes',\n",
       " 'Are',\n",
       " 'multi-factor',\n",
       " 'Gaussian',\n",
       " 'term',\n",
       " 'structure',\n",
       " 'models',\n",
       " 'still',\n",
       " 'useful?',\n",
       " 'An',\n",
       " 'empirical',\n",
       " 'analysis',\n",
       " 'on',\n",
       " 'Italian',\n",
       " 'BTPs',\n",
       " 'Probing',\n",
       " 'valley',\n",
       " 'filtering',\n",
       " 'effect',\n",
       " 'by',\n",
       " 'Andreev',\n",
       " 'reflection',\n",
       " 'in',\n",
       " 'zigzag',\n",
       " 'graphene',\n",
       " 'nanoribbon',\n",
       " 'Generalized',\n",
       " 'Approximate',\n",
       " 'Message-Passing',\n",
       " 'Decoder',\n",
       " 'for',\n",
       " 'Universal',\n",
       " 'Sparse',\n",
       " 'Superposition',\n",
       " 'Codes',\n",
       " 'LAAIR:',\n",
       " 'A',\n",
       " 'Layered',\n",
       " 'Architecture',\n",
       " 'for',\n",
       " 'Autonomous',\n",
       " 'Interactive',\n",
       " 'Robots',\n",
       " '3D',\n",
       " 'Human',\n",
       " 'Pose',\n",
       " 'Estimation',\n",
       " 'in',\n",
       " 'RGBD',\n",
       " 'Images',\n",
       " 'for',\n",
       " 'Robotic',\n",
       " 'Task',\n",
       " 'Learning',\n",
       " 'Simultaneous',\n",
       " 'non-vanishing',\n",
       " 'for',\n",
       " 'Dirichlet',\n",
       " 'L-functions',\n",
       " 'Wehrl',\n",
       " 'Entropy',\n",
       " 'Based',\n",
       " 'Quantification',\n",
       " 'of',\n",
       " 'Nonclassicality',\n",
       " 'for',\n",
       " 'Single',\n",
       " 'Mode',\n",
       " 'Quantum',\n",
       " 'Optical',\n",
       " 'States',\n",
       " 'Attention-based',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Person',\n",
       " 'Retrieval',\n",
       " 'Large',\n",
       " 'Scale',\n",
       " 'Automated',\n",
       " 'Forecasting',\n",
       " 'for',\n",
       " 'Monitoring',\n",
       " 'Network',\n",
       " 'Safety',\n",
       " 'and',\n",
       " 'Security',\n",
       " 'Contextual',\n",
       " 'Regression:',\n",
       " 'An',\n",
       " 'Accurate',\n",
       " 'and',\n",
       " 'Conveniently',\n",
       " 'Interpretable',\n",
       " 'Nonlinear',\n",
       " 'Model',\n",
       " 'for',\n",
       " 'Mining',\n",
       " 'Discovery',\n",
       " 'from',\n",
       " 'Scientific',\n",
       " 'Data',\n",
       " 'Multi-time',\n",
       " 'correlators',\n",
       " 'in',\n",
       " 'continuous',\n",
       " 'measurement',\n",
       " 'of',\n",
       " 'qubit',\n",
       " 'observables',\n",
       " 'Parallelism,',\n",
       " 'Concurrency',\n",
       " 'and',\n",
       " 'Distribution',\n",
       " 'in',\n",
       " 'Constraint',\n",
       " 'Handling',\n",
       " 'Rules:',\n",
       " 'A',\n",
       " 'Survey',\n",
       " 'Robustness',\n",
       " 'against',\n",
       " 'the',\n",
       " 'channel',\n",
       " 'effect',\n",
       " 'in',\n",
       " 'pathological',\n",
       " 'voice',\n",
       " 'detection',\n",
       " 'An',\n",
       " 'Effective',\n",
       " 'Framework',\n",
       " 'for',\n",
       " 'Constructing',\n",
       " 'Exponent',\n",
       " 'Lattice',\n",
       " 'Basis',\n",
       " 'of',\n",
       " 'Nonzero',\n",
       " 'Algebraic',\n",
       " 'Numbers',\n",
       " 'Competing',\n",
       " 'evolutionary',\n",
       " 'paths',\n",
       " 'in',\n",
       " 'growing',\n",
       " 'populations',\n",
       " 'with',\n",
       " 'applications',\n",
       " 'to',\n",
       " 'multidrug',\n",
       " 'resistance',\n",
       " 'Transient',\n",
       " 'flows',\n",
       " 'in',\n",
       " 'active',\n",
       " 'porous',\n",
       " 'media',\n",
       " 'An',\n",
       " 'information',\n",
       " 'model',\n",
       " 'for',\n",
       " 'modular',\n",
       " 'robots:',\n",
       " 'the',\n",
       " 'Hardware',\n",
       " 'Robot',\n",
       " 'Information',\n",
       " 'Model',\n",
       " '(HRIM)',\n",
       " 'Detecting',\n",
       " 'Adversarial',\n",
       " 'Samples',\n",
       " 'Using',\n",
       " 'Density',\n",
       " 'Ratio',\n",
       " 'Estimates',\n",
       " 'The',\n",
       " 'Query',\n",
       " 'Complexity',\n",
       " 'of',\n",
       " 'Cake',\n",
       " 'Cutting',\n",
       " 'Stacked',\n",
       " 'Convolutional',\n",
       " 'and',\n",
       " 'Recurrent',\n",
       " 'Neural',\n",
       " 'Networks',\n",
       " 'for',\n",
       " 'Music',\n",
       " 'Emotion',\n",
       " 'Recognition',\n",
       " 'Timed',\n",
       " 'Automata',\n",
       " 'with',\n",
       " 'Polynomial',\n",
       " 'Delay',\n",
       " 'and',\n",
       " 'their',\n",
       " 'Expressiveness',\n",
       " 'Superconducting',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'Cu',\n",
       " 'intercalated',\n",
       " 'Bi$_2$Se$_3$',\n",
       " 'studied',\n",
       " 'by',\n",
       " 'Muon',\n",
       " 'Spin',\n",
       " 'Spectroscopy',\n",
       " 'Time-domain',\n",
       " 'THz',\n",
       " 'spectroscopy',\n",
       " 'reveals',\n",
       " 'coupled',\n",
       " 'protein-hydration',\n",
       " 'dielectric',\n",
       " 'response',\n",
       " 'in',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'native',\n",
       " 'and',\n",
       " 'fibrils',\n",
       " 'of',\n",
       " 'human',\n",
       " 'lyso-zyme',\n",
       " 'Inversion',\n",
       " 'of',\n",
       " 'Qubit',\n",
       " 'Energy',\n",
       " 'Levels',\n",
       " 'in',\n",
       " 'Qubit-Oscillator',\n",
       " 'Circuits',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Deep-Strong-Coupling',\n",
       " 'Regime',\n",
       " 'Deep',\n",
       " 'Multiple',\n",
       " 'Instance',\n",
       " 'Feature',\n",
       " 'Learning',\n",
       " 'via',\n",
       " 'Variational',\n",
       " 'Autoencoder',\n",
       " 'Regularity',\n",
       " 'of',\n",
       " 'envelopes',\n",
       " 'in',\n",
       " 'Khler',\n",
       " 'classes',\n",
       " '$S^1$-equivariant',\n",
       " 'Index',\n",
       " 'theorems',\n",
       " 'and',\n",
       " 'Morse',\n",
       " 'inequalities',\n",
       " 'on',\n",
       " 'complex',\n",
       " 'manifolds',\n",
       " 'with',\n",
       " 'boundary',\n",
       " 'Internal',\n",
       " 'Model',\n",
       " 'from',\n",
       " 'Observations',\n",
       " 'for',\n",
       " 'Reward',\n",
       " 'Shaping',\n",
       " 'Characterizations',\n",
       " 'of',\n",
       " 'quasitrivial',\n",
       " 'symmetric',\n",
       " 'nondecreasing',\n",
       " 'associative',\n",
       " 'operations',\n",
       " 'Multivariate',\n",
       " 'Dependency',\n",
       " 'Measure',\n",
       " 'based',\n",
       " 'on',\n",
       " 'Copula',\n",
       " 'and',\n",
       " 'Gaussian',\n",
       " 'Kernel',\n",
       " 'The',\n",
       " 'nature',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tensor',\n",
       " 'order',\n",
       " 'in',\n",
       " 'Cd2Re2O7',\n",
       " 'Efficient',\n",
       " 'and',\n",
       " 'consistent',\n",
       " 'inference',\n",
       " 'of',\n",
       " 'ancestral',\n",
       " 'sequences',\n",
       " 'in',\n",
       " 'an',\n",
       " 'evolutionary',\n",
       " 'model',\n",
       " 'with',\n",
       " 'insertions',\n",
       " 'and',\n",
       " 'deletions',\n",
       " 'under',\n",
       " 'dense',\n",
       " 'taxon',\n",
       " 'sampling',\n",
       " 'Flow',\n",
       " 'Characteristics',\n",
       " 'and',\n",
       " 'Cores',\n",
       " 'of',\n",
       " 'Complex',\n",
       " 'Network',\n",
       " 'and',\n",
       " 'Multiplex',\n",
       " 'Type',\n",
       " 'Systems',\n",
       " 'Pattern-forming',\n",
       " 'fronts',\n",
       " 'in',\n",
       " 'a',\n",
       " 'Swift-Hohenberg',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'directional',\n",
       " 'quenching',\n",
       " '-',\n",
       " 'parallel',\n",
       " 'and',\n",
       " 'oblique',\n",
       " 'stripes',\n",
       " 'Generalized',\n",
       " 'Minimum',\n",
       " 'Distance',\n",
       " 'Estimators',\n",
       " 'in',\n",
       " 'Linear',\n",
       " 'Regression',\n",
       " 'with',\n",
       " 'Dependent',\n",
       " 'Errors',\n",
       " 'Live',\n",
       " 'Service',\n",
       " 'Migration',\n",
       " 'in',\n",
       " 'Mobile',\n",
       " 'Edge',\n",
       " 'Clouds',\n",
       " 'Induced',\n",
       " 'density',\n",
       " 'correlations',\n",
       " 'in',\n",
       " 'a',\n",
       " 'sonic',\n",
       " 'black',\n",
       " 'hole',\n",
       " 'condensate',\n",
       " 'Genus',\n",
       " 'growth',\n",
       " 'in',\n",
       " '$\\\\mathbb{Z}_p$-towers',\n",
       " 'of',\n",
       " 'function',\n",
       " 'fields',\n",
       " 'Topological',\n",
       " 'Phases',\n",
       " 'emerging',\n",
       " 'from',\n",
       " 'Spin-Orbital',\n",
       " 'Physics',\n",
       " 'Accurate',\n",
       " 'and',\n",
       " 'Diverse',\n",
       " 'Sampling',\n",
       " 'of',\n",
       " 'Sequences',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " '\"Best',\n",
       " 'of',\n",
       " 'Many\"',\n",
       " 'Sample',\n",
       " 'Objective',\n",
       " 'Exploring',\n",
       " 'RNN-Transducer',\n",
       " 'for',\n",
       " 'Chinese',\n",
       " 'Speech',\n",
       " 'Recognition',\n",
       " 'A',\n",
       " 'Debt-Aware',\n",
       " 'Learning',\n",
       " 'Approach',\n",
       " 'for',\n",
       " 'Resource',\n",
       " 'Adaptations',\n",
       " 'in',\n",
       " 'Cloud',\n",
       " 'Elasticity',\n",
       " 'Management',\n",
       " 'Semi-simplicial',\n",
       " 'spaces',\n",
       " 'Constraints,',\n",
       " 'Lazy',\n",
       " 'Constraints,',\n",
       " 'or',\n",
       " 'Propagators',\n",
       " 'in',\n",
       " 'ASP',\n",
       " 'Solving:',\n",
       " 'An',\n",
       " 'Empirical',\n",
       " 'Analysis',\n",
       " 'A',\n",
       " 'Unified',\n",
       " 'Approach',\n",
       " 'to',\n",
       " 'Nonlinear',\n",
       " 'Transformation',\n",
       " 'Materials',\n",
       " 'Stationary',\n",
       " 'crack',\n",
       " 'propagation',\n",
       " 'in',\n",
       " 'a',\n",
       " 'two-dimensional',\n",
       " 'visco-elastic',\n",
       " 'network',\n",
       " 'model',\n",
       " 'A',\n",
       " 'note',\n",
       " 'on',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'group',\n",
       " 'of',\n",
       " 'Kodaira',\n",
       " 'fibrations',\n",
       " 'Photo-Chemically',\n",
       " 'Directed',\n",
       " 'Self-Assembly',\n",
       " 'of',\n",
       " 'Carbon',\n",
       " 'Nanotubes',\n",
       " 'on',\n",
       " 'Surfaces',\n",
       " 'Split-and-augmented',\n",
       " 'Gibbs',\n",
       " 'sampler',\n",
       " '-',\n",
       " 'Application',\n",
       " 'to',\n",
       " 'large-scale',\n",
       " 'inference',\n",
       " 'problems',\n",
       " 'Does',\n",
       " 'a',\n",
       " 'generalized',\n",
       " 'Chaplygin',\n",
       " 'gas',\n",
       " 'correctly',\n",
       " 'describe',\n",
       " 'the',\n",
       " 'cosmological',\n",
       " 'dark',\n",
       " 'sector?',\n",
       " 'The',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'subdiffusion',\n",
       " 'on',\n",
       " 'the',\n",
       " 'NTA',\n",
       " 'size',\n",
       " 'measurements',\n",
       " 'of',\n",
       " 'extracellular',\n",
       " 'vesicles',\n",
       " 'in',\n",
       " 'biological',\n",
       " 'samples',\n",
       " 'Empirical',\n",
       " 'regression',\n",
       " 'quantile',\n",
       " 'process',\n",
       " 'with',\n",
       " 'possible',\n",
       " 'application',\n",
       " 'to',\n",
       " 'risk',\n",
       " 'analysis',\n",
       " 'Primordial',\n",
       " 'perturbations',\n",
       " 'from',\n",
       " 'inflation',\n",
       " 'with',\n",
       " 'a',\n",
       " 'hyperbolic',\n",
       " 'field-space',\n",
       " 'Role',\n",
       " 'of',\n",
       " 'Vanadyl',\n",
       " 'Oxygen',\n",
       " 'in',\n",
       " 'Understanding',\n",
       " 'Metallic',\n",
       " 'Behavior',\n",
       " 'of',\n",
       " 'V2O5(001)',\n",
       " 'Nanorods',\n",
       " 'Graph',\n",
       " 'Convolution:',\n",
       " 'A',\n",
       " 'High-Order',\n",
       " 'and',\n",
       " 'Adaptive',\n",
       " 'Approach',\n",
       " 'Learning',\n",
       " 'Sparse',\n",
       " 'Representations',\n",
       " 'in',\n",
       " 'Reinforcement',\n",
       " 'Learning',\n",
       " 'with',\n",
       " 'Sparse',\n",
       " 'Coding',\n",
       " 'Almost',\n",
       " 'euclidean',\n",
       " 'Isoperimetric',\n",
       " 'Inequalities',\n",
       " 'in',\n",
       " 'spaces',\n",
       " 'satisfying',\n",
       " 'local',\n",
       " 'Ricci',\n",
       " 'curvature',\n",
       " 'lower',\n",
       " 'bounds',\n",
       " 'Exponential',\n",
       " 'Sums',\n",
       " 'and',\n",
       " 'Riesz',\n",
       " 'energies',\n",
       " 'One',\n",
       " 'dimensionalization',\n",
       " 'in',\n",
       " 'the',\n",
       " 'spin-1',\n",
       " 'Heisenberg',\n",
       " 'model',\n",
       " 'on',\n",
       " 'the',\n",
       " 'anisotropic',\n",
       " 'triangular',\n",
       " 'lattice',\n",
       " 'Memory',\n",
       " 'Aware',\n",
       " 'Synapses:',\n",
       " 'Learning',\n",
       " 'what',\n",
       " '(not)',\n",
       " 'to',\n",
       " 'forget',\n",
       " 'Uniform',\n",
       " 'Spectral',\n",
       " 'Convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Stochastic',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject-specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data.',\n",
       " 'Given',\n",
       " 'a',\n",
       " \"subject's\",\n",
       " 'data,',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels:',\n",
       " 'global,',\n",
       " 'i.e.',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject,',\n",
       " 'and',\n",
       " 'local,',\n",
       " 'i.e.',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " \"subject's\",\n",
       " 'data.',\n",
       " 'While',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used,',\n",
       " 'local',\n",
       " 'inference,',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject-specific',\n",
       " 'effect',\n",
       " 'maps,',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'article,',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method,',\n",
       " 'named',\n",
       " 'RSM,',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject-specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular,',\n",
       " 'binary',\n",
       " 'classifiers.',\n",
       " 'RSM',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers.',\n",
       " 'The',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper-type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner,',\n",
       " 'i.e.',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence.',\n",
       " 'Reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'Maximum-A-Posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier-specific',\n",
       " 'fashion.',\n",
       " 'Experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " \"Alzheimer's\",\n",
       " 'Disease',\n",
       " 'Neuroimaging',\n",
       " 'Initiative',\n",
       " '(ADNI)',\n",
       " 'database.',\n",
       " 'Results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'RSM',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging.',\n",
       " 'Analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ADNI',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'RSM',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject-specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non-imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " \"Alzheimer's\",\n",
       " 'Disease',\n",
       " '(AD),',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Mini',\n",
       " 'Mental',\n",
       " 'State',\n",
       " 'Examination',\n",
       " 'Score',\n",
       " 'and',\n",
       " 'Cerebrospinal',\n",
       " 'Fluid',\n",
       " 'amyloid-$\\\\beta$',\n",
       " 'levels.',\n",
       " 'Further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'ADNI',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'RSM',\n",
       " 'is',\n",
       " 'used.',\n",
       " 'Rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'paper,',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " '(CNN)',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " '2-D',\n",
       " 'symbol',\n",
       " 'recognition.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " '2-D',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non-overlap',\n",
       " 'target.',\n",
       " 'Last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least,',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one-shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance.',\n",
       " 'We',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics,',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics.',\n",
       " 'In',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics,',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us,',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics,',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls.',\n",
       " 'We',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Gegenbauer',\n",
       " 'polynomials.',\n",
       " 'We',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'Poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball,',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls,',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Cauchy-Hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Lie',\n",
       " 'ball.',\n",
       " 'The',\n",
       " 'stochastic',\n",
       " 'Landau--Lifshitz--Gilbert',\n",
       " '(LLG)',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Maxwell',\n",
       " 'equations',\n",
       " '(the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'MLLG',\n",
       " 'system)',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " '(fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories).',\n",
       " 'We',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'LLG',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time-differentiable',\n",
       " 'solutions.',\n",
       " 'We',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " '$\\\\theta$-linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'consequence,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions,',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " '(depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " '$\\\\theta$).',\n",
       " 'Hence,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'MLLG',\n",
       " 'system.',\n",
       " 'Numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method.',\n",
       " 'Fourier-transform',\n",
       " 'infra-red',\n",
       " '(FTIR)',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " '7',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms.',\n",
       " 'Wavelet',\n",
       " 'Tensor',\n",
       " 'Train',\n",
       " '(WTT)',\n",
       " 'and',\n",
       " 'Discrete',\n",
       " 'Wavelet',\n",
       " 'Transforms',\n",
       " '(DWT)',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'FTIR',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants.',\n",
       " 'Various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks.',\n",
       " 'Best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'WTT',\n",
       " 'and',\n",
       " 'DWT',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar,',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra.',\n",
       " 'Unlike',\n",
       " 'DWT,',\n",
       " 'WTT',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " '(rank),',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications.',\n",
       " 'Let',\n",
       " '$\\\\Omega',\n",
       " '\\\\subset',\n",
       " '\\\\mathbb{R}^n$',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'Hayman-type',\n",
       " 'asymmetry',\n",
       " 'condition,',\n",
       " 'and',\n",
       " 'let',\n",
       " '$',\n",
       " 'D',\n",
       " '$',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " '\"obstacle\".',\n",
       " 'We',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'Dirichlet',\n",
       " 'eigenvalue',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '$.',\n",
       " 'First,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '$',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'Dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1}',\n",
       " '>',\n",
       " '0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\Omega',\n",
       " '$.',\n",
       " 'In',\n",
       " 'short,',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\mu_\\\\Omega',\n",
       " ':=',\n",
       " '\\\\max_{x}\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '\\\\end{equation}',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega)',\n",
       " '$,',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1}',\n",
       " '$.',\n",
       " 'Second,',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1(\\\\Omega)}',\n",
       " '$',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " '$',\n",
       " '\\\\Omega',\n",
       " '$.',\n",
       " 'Finally,',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " '$',\n",
       " 'D',\n",
       " '$',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega)',\n",
       " '$,',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1(\\\\Omega)}',\n",
       " '$.',\n",
       " 'We',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " '1I/`Oumuamua',\n",
       " '(2017',\n",
       " 'U1)',\n",
       " 'on',\n",
       " '2017',\n",
       " 'October',\n",
       " '30',\n",
       " 'with',\n",
       " 'Lowell',\n",
       " \"Observatory's\",\n",
       " '4.3-m',\n",
       " 'Discovery',\n",
       " 'Channel',\n",
       " 'Telescope.',\n",
       " 'From',\n",
       " 'these',\n",
       " 'observations,',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak-to-trough',\n",
       " 'amplitude',\n",
       " 'of',\n",
       " 'at',\n",
       " 'least',\n",
       " '1.2',\n",
       " 'mag.',\n",
       " 'This',\n",
       " 'lightcurve',\n",
       " 'segment',\n",
       " 'rules',\n",
       " 'out',\n",
       " 'rotation',\n",
       " 'periods',\n",
       " 'less',\n",
       " 'than',\n",
       " '3',\n",
       " 'hr',\n",
       " 'and',\n",
       " 'suggests',\n",
       " 'that',\n",
       " 'the',\n",
       " 'period',\n",
       " 'is',\n",
       " 'at',\n",
       " 'least',\n",
       " '5',\n",
       " 'hr.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'assumption',\n",
       " 'that',\n",
       " 'the',\n",
       " 'variability',\n",
       " 'is',\n",
       " 'due',\n",
       " 'to',\n",
       " 'a',\n",
       " 'changing',\n",
       " 'cross',\n",
       " 'section,',\n",
       " 'the',\n",
       " 'axial',\n",
       " 'ratio',\n",
       " 'is',\n",
       " 'at',\n",
       " 'least',\n",
       " '3:1.',\n",
       " 'We',\n",
       " 'saw',\n",
       " 'no',\n",
       " 'evidence',\n",
       " 'for',\n",
       " 'a',\n",
       " 'coma',\n",
       " 'or',\n",
       " 'tail',\n",
       " 'in',\n",
       " 'either',\n",
       " 'individual',\n",
       " 'images',\n",
       " 'or',\n",
       " 'in',\n",
       " 'a',\n",
       " 'stacked',\n",
       " 'image',\n",
       " 'having',\n",
       " 'an',\n",
       " 'equivalent',\n",
       " 'exposure',\n",
       " 'time',\n",
       " 'of',\n",
       " '9000',\n",
       " 's.',\n",
       " 'The',\n",
       " 'ability',\n",
       " 'of',\n",
       " 'metallic',\n",
       " 'nanoparticles',\n",
       " 'to',\n",
       " 'supply',\n",
       " 'heat',\n",
       " 'to',\n",
       " 'a',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analysing the words in titles and abstracts\n",
    "titles = []\n",
    "abstracts = []\n",
    "for index,row in df.iterrows():\n",
    "    l = row['TITLE'].split()\n",
    "    m = row['ABSTRACT'].split()\n",
    "    for el in l:\n",
    "        titles.append(el)\n",
    "    for ele in m:\n",
    "        abstracts.append(ele)\n",
    "display(titles)\n",
    "display(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>combinations</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance  combinations  \\\n",
       "0                     0             1   \n",
       "1                     0             1   \n",
       "2                     0             1   \n",
       "3                     0             1   \n",
       "4                     0             2   \n",
       "\n",
       "                             title_abstract_combined  \n",
       "0  Reconstructing Subject-Specific Effect Maps   ...  \n",
       "1  Rotation Invariance Neural Network   Rotation ...  \n",
       "2  Spherical polyharmonics and Poisson kernels fo...  \n",
       "3  A finite element approximation for the stochas...  \n",
       "4  Comparative study of Discrete Wavelet Transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>We present novel understandings of the Gamma...</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
       "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>Milky Way open clusters are very diverse in ...</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>Proving that a cryptographic protocol is cor...</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              TITLE  \\\n",
       "0  20973  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  20974  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  20975         Case For Static AMSDU Aggregation in WLANs   \n",
       "3  20976  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  20977  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0    We present novel understandings of the Gamma...   \n",
       "1    Meteorites contain minerals from Solar Syste...   \n",
       "2    Frame aggregation is a mechanism by which mu...   \n",
       "3    Milky Way open clusters are very diverse in ...   \n",
       "4    Proving that a cryptographic protocol is cor...   \n",
       "\n",
       "                             title_abstract_combined  \n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...  \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...  \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...  \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...  \n",
       "4  Witness-Functions versus Interpretation-Functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine title and abstract\n",
    "df['title_abstract_combined'] = df['TITLE'] + ' ' + df['ABSTRACT']\n",
    "df_test_2['title_abstract_combined'] = df_test_2['TITLE'] + ' ' + df_test_2['ABSTRACT']\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...  \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...  \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...  \n",
       "3                     0  A finite element approximation for the stochas...  \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove ID, TITLE, ABSTRACT, combinations, title_processed, and abstract_processed columns\n",
    "df = df.drop(labels = ['ID','TITLE','ABSTRACT','combinations'], axis = 1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...\n",
       "1  Laboratory mid-IR spectra of equilibrated and ...\n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...\n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...\n",
       "4  Witness-Functions versus Interpretation-Functi..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test_2 = df_test_2.drop(labels = ['ID','TITLE','ABSTRACT'], axis = 1)\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstructing subject specific effect maps pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotation invariance neural network rotation in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spherical polyharmonics and poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>a finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>comparative study of discrete wavelet transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstructing subject specific effect maps pr...  \n",
       "1  rotation invariance neural network rotation in...  \n",
       "2  spherical polyharmonics and poisson kernels fo...  \n",
       "3  a finite element approximation for the stochas...  \n",
       "4  comparative study of discrete wavelet transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>closed form marginal likelihood in gamma poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratory mid ir spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case for static amsdu aggregation in wlans fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>the gaia eso survey the inner disk intermediat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>witness functions versus interpretation functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  closed form marginal likelihood in gamma poiss...  \n",
       "1  laboratory mid ir spectra of equilibrated and ...  \n",
       "2  case for static amsdu aggregation in wlans fra...  \n",
       "3  the gaia eso survey the inner disk intermediat...  \n",
       "4  witness functions versus interpretation functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the regular expressions library\n",
    "import re\n",
    "\n",
    "#remove punctuation from title and abstract\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined'].map(lambda x: re.sub('[,\\.!?0-9]', ' ',x))\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined'].map(lambda x: re.sub('[,\\.!?0-9]', ' ',x))\n",
    "\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined_processed'].map(lambda x: re.sub('[\\W_]+', ' ',x))\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined_processed'].map(lambda x: re.sub('[\\W_]+', ' ',x))\n",
    "\n",
    "#convert the title and abstract to lowercase\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined_processed'].map(lambda x: x.lower())\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined_processed'].map(lambda x: x.lower())\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reconstructing',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data',\n",
       " 'given',\n",
       " 'a',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels',\n",
       " 'global',\n",
       " 'i',\n",
       " 'e',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'and',\n",
       " 'local',\n",
       " 'i',\n",
       " 'e',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'while',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'local',\n",
       " 'inference',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands',\n",
       " 'in',\n",
       " 'this',\n",
       " 'article',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method',\n",
       " 'named',\n",
       " 'rsm',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'rsm',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper',\n",
       " 'type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner',\n",
       " 'i',\n",
       " 'e',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'maximum',\n",
       " 'a',\n",
       " 'posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier',\n",
       " 'specific',\n",
       " 'fashion',\n",
       " 'experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'neuroimaging',\n",
       " 'initiative',\n",
       " 'adni',\n",
       " 'database',\n",
       " 'results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'rsm',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging',\n",
       " 'analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'rsm',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non',\n",
       " 'imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'ad',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mini',\n",
       " 'mental',\n",
       " 'state',\n",
       " 'examination',\n",
       " 'score',\n",
       " 'and',\n",
       " 'cerebrospinal',\n",
       " 'fluid',\n",
       " 'amyloid',\n",
       " 'beta',\n",
       " 'levels',\n",
       " 'further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'rsm',\n",
       " 'is',\n",
       " 'used',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'cnn',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'recognition',\n",
       " 'we',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non',\n",
       " 'overlap',\n",
       " 'target',\n",
       " 'last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'we',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'we',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gegenbauer',\n",
       " 'polynomials',\n",
       " 'we',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cauchy',\n",
       " 'hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lie',\n",
       " 'ball',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'maxwell',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'system',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'maxwell',\n",
       " 'equations',\n",
       " 'the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " 'fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories',\n",
       " 'we',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time',\n",
       " 'differentiable',\n",
       " 'solutions',\n",
       " 'we',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " 'theta',\n",
       " 'linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system',\n",
       " 'as',\n",
       " 'a',\n",
       " 'consequence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'theta',\n",
       " 'hence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method',\n",
       " 'comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'and',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'fourier',\n",
       " 'transform',\n",
       " 'infra',\n",
       " 'red',\n",
       " 'ftir',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'dwt',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks',\n",
       " 'best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'dwt',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra',\n",
       " 'unlike',\n",
       " 'dwt',\n",
       " 'wtt',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " 'rank',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications',\n",
       " 'on',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'let',\n",
       " 'omega',\n",
       " 'subset',\n",
       " 'mathbb',\n",
       " 'r',\n",
       " 'n',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'hayman',\n",
       " 'type',\n",
       " 'asymmetry',\n",
       " 'condition',\n",
       " 'and',\n",
       " 'let',\n",
       " 'd',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'obstacle',\n",
       " 'we',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'eigenvalue',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'first',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " 'x',\n",
       " 'd',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'of',\n",
       " 'omega',\n",
       " 'in',\n",
       " 'short',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " 'begin',\n",
       " 'equation',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'max',\n",
       " 'x',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'end',\n",
       " 'equation',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'second',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " 'omega',\n",
       " 'finally',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " 'd',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'we',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'on',\n",
       " 'october',\n",
       " 'with',\n",
       " 'lowell',\n",
       " 'observatory',\n",
       " 's',\n",
       " 'm',\n",
       " 'discovery',\n",
       " 'channel',\n",
       " 'telescope',\n",
       " 'from',\n",
       " 'these',\n",
       " 'observations',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['reconstructing',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data',\n",
       " 'given',\n",
       " 'a',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels',\n",
       " 'global',\n",
       " 'i',\n",
       " 'e',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'and',\n",
       " 'local',\n",
       " 'i',\n",
       " 'e',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'while',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'local',\n",
       " 'inference',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands',\n",
       " 'in',\n",
       " 'this',\n",
       " 'article',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method',\n",
       " 'named',\n",
       " 'rsm',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'rsm',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper',\n",
       " 'type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner',\n",
       " 'i',\n",
       " 'e',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'maximum',\n",
       " 'a',\n",
       " 'posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier',\n",
       " 'specific',\n",
       " 'fashion',\n",
       " 'experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'neuroimaging',\n",
       " 'initiative',\n",
       " 'adni',\n",
       " 'database',\n",
       " 'results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'rsm',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging',\n",
       " 'analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'rsm',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non',\n",
       " 'imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'ad',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mini',\n",
       " 'mental',\n",
       " 'state',\n",
       " 'examination',\n",
       " 'score',\n",
       " 'and',\n",
       " 'cerebrospinal',\n",
       " 'fluid',\n",
       " 'amyloid',\n",
       " 'beta',\n",
       " 'levels',\n",
       " 'further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'rsm',\n",
       " 'is',\n",
       " 'used',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'cnn',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'recognition',\n",
       " 'we',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non',\n",
       " 'overlap',\n",
       " 'target',\n",
       " 'last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'we',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'we',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gegenbauer',\n",
       " 'polynomials',\n",
       " 'we',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cauchy',\n",
       " 'hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lie',\n",
       " 'ball',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'maxwell',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'system',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'maxwell',\n",
       " 'equations',\n",
       " 'the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " 'fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories',\n",
       " 'we',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time',\n",
       " 'differentiable',\n",
       " 'solutions',\n",
       " 'we',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " 'theta',\n",
       " 'linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system',\n",
       " 'as',\n",
       " 'a',\n",
       " 'consequence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'theta',\n",
       " 'hence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method',\n",
       " 'comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'and',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'fourier',\n",
       " 'transform',\n",
       " 'infra',\n",
       " 'red',\n",
       " 'ftir',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'dwt',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks',\n",
       " 'best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'dwt',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra',\n",
       " 'unlike',\n",
       " 'dwt',\n",
       " 'wtt',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " 'rank',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications',\n",
       " 'on',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'let',\n",
       " 'omega',\n",
       " 'subset',\n",
       " 'mathbb',\n",
       " 'r',\n",
       " 'n',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'hayman',\n",
       " 'type',\n",
       " 'asymmetry',\n",
       " 'condition',\n",
       " 'and',\n",
       " 'let',\n",
       " 'd',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'obstacle',\n",
       " 'we',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'eigenvalue',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'first',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " 'x',\n",
       " 'd',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'of',\n",
       " 'omega',\n",
       " 'in',\n",
       " 'short',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " 'begin',\n",
       " 'equation',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'max',\n",
       " 'x',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'end',\n",
       " 'equation',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'second',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " 'omega',\n",
       " 'finally',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " 'd',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'we',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'on',\n",
       " 'october',\n",
       " 'with',\n",
       " 'lowell',\n",
       " 'observatory',\n",
       " 's',\n",
       " 'm',\n",
       " 'discovery',\n",
       " 'channel',\n",
       " 'telescope',\n",
       " 'from',\n",
       " 'these',\n",
       " 'observations',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analysing the words in titles and abstracts\n",
    "titles_pr = []\n",
    "abstracts_pr = []\n",
    "for index,row in df.iterrows():\n",
    "    l = row['title_abstract_combined_processed'].split()\n",
    "    m = row['title_abstract_combined_processed'].split()\n",
    "    for el in l:\n",
    "        titles_pr.append(el)\n",
    "    for ele in m:\n",
    "        abstracts_pr.append(ele)\n",
    "display(titles_pr)\n",
    "display(abstracts_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstructing subject specific effect maps pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotation invariance neural network rotation in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spherical polyharmonics poisson kernels polyha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>finite element approximation stochastic maxwel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>comparative study discrete wavelet transforms ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstructing subject specific effect maps pr...  \n",
       "1  rotation invariance neural network rotation in...  \n",
       "2  spherical polyharmonics poisson kernels polyha...  \n",
       "3  finite element approximation stochastic maxwel...  \n",
       "4  comparative study discrete wavelet transforms ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>closed form marginal likelihood gamma poisson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratory mid ir spectra equilibrated igneous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case static amsdu aggregation wlans frame aggr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>gaia eso survey inner disk intermediate age op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>witness functions versus interpretation functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  closed form marginal likelihood gamma poisson ...  \n",
       "1  laboratory mid ir spectra equilibrated igneous...  \n",
       "2  case static amsdu aggregation wlans frame aggr...  \n",
       "3  gaia eso survey inner disk intermediate age op...  \n",
       "4  witness functions versus interpretation functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#remove stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\"\",sentence)\n",
    "df['title_abstract_combined_processed']=df['title_abstract_combined_processed'].apply(removeStopWords)\n",
    "df_test_2['title_abstract_combined_processed']=df_test_2['title_abstract_combined_processed'].apply(removeStopWords)\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstruct subject specif effect map predict ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotat invari neural network rotat invari trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spheric polyharmon poisson kernel polyharmon f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>finit element approxim stochast maxwel landau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>compar studi discret wavelet transform wavelet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstruct subject specif effect map predict ...  \n",
       "1  rotat invari neural network rotat invari trans...  \n",
       "2  spheric polyharmon poisson kernel polyharmon f...  \n",
       "3  finit element approxim stochast maxwel landau ...  \n",
       "4  compar studi discret wavelet transform wavelet...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>close form margin likelihood gamma poisson mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratori mid ir spectra equilibr igneous met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case static amsdu aggreg wlan frame aggreg mec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>gaia eso survey inner disk intermedi age open ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>wit function versus interpret function secreci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  close form margin likelihood gamma poisson mat...  \n",
       "1  laboratori mid ir spectra equilibr igneous met...  \n",
       "2  case static amsdu aggreg wlan frame aggreg mec...  \n",
       "3  gaia eso survey inner disk intermedi age open ...  \n",
       "4  wit function versus interpret function secreci...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#do stemming on title and abstract\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "df['title_abstract_combined_processed']=df['title_abstract_combined_processed'].apply(stemming)\n",
    "df_test_2['title_abstract_combined_processed']=df_test_2['title_abstract_combined_processed'].apply(stemming)\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import logging\n",
    "logging.basicConfig(format = \"%(levelname)s - %(asctime)s: %(message)s\", datefmt = '%H:%M:%S', level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:39:18: collecting all words and their counts\n",
      "INFO - 22:39:18: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 22:39:20: PROGRESS: at sentence #10000, processed 1020749 words and 570778 word types\n",
      "INFO - 22:39:22: PROGRESS: at sentence #20000, processed 2035896 words and 962748 word types\n",
      "INFO - 22:39:23: collected 996639 word types from a corpus of 2134263 words (unigram + bigrams) and 20972 sentences\n",
      "INFO - 22:39:23: using 996639 counts as vocab in Phrases<0 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 22:39:23: collecting all words and their counts\n",
      "INFO - 22:39:23: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 22:39:24: collected 523371 word types from a corpus of 915660 words (unigram + bigrams) and 8989 sentences\n",
      "INFO - 22:39:24: using 523371 counts as vocab in Phrases<0 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [row.split() for row in df['title_abstract_combined_processed']]\n",
    "sent_test_2 = [row.split() for row in df_test_2['title_abstract_combined_processed']]\n",
    "phrases = Phrases(sent, min_count = 50, progress_per = 10000)\n",
    "phrases_test_2 = Phrases(sent_test_2, min_count = 50, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:39:42: source_vocab length 996639\n",
      "INFO - 22:39:53: Phraser built with 480 phrasegrams\n",
      "INFO - 22:39:53: source_vocab length 523371\n",
      "INFO - 22:40:00: Phraser built with 166 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "bigram_test_2 = Phraser(phrases_test_2)\n",
    "sentences_test_2 = bigram[sent_test_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36007"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'use',\n",
       " 'method',\n",
       " 'data',\n",
       " 'result',\n",
       " 'system',\n",
       " 'base',\n",
       " 'show',\n",
       " 'network',\n",
       " 'problem']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23180"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences_test_2:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'use',\n",
       " 'method',\n",
       " 'data',\n",
       " 'system',\n",
       " 'result',\n",
       " 'base',\n",
       " 'network',\n",
       " 'show',\n",
       " 'algorithm']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "display(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count = 5, window = 2, size = 300, sample = 6e-5, alpha = 0.05, min_alpha = 0.00001, negative = 5, workers = cores-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:40:32: collecting all words and their counts\n",
      "INFO - 22:40:32: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 22:40:35: PROGRESS: at sentence #10000, processed 983758 words, keeping 24853 word types\n",
      "INFO - 22:40:38: PROGRESS: at sentence #20000, processed 1962056 words, keeping 35161 word types\n",
      "INFO - 22:40:39: collected 36007 word types from a corpus of 2056851 raw words and 20972 sentences\n",
      "INFO - 22:40:39: Loading a fresh vocabulary\n",
      "INFO - 22:40:39: effective_min_count=5 retains 13262 unique words (36% of original 36007, drops 22745)\n",
      "INFO - 22:40:39: effective_min_count=5 leaves 2016739 word corpus (98% of original 2056851, drops 40112)\n",
      "INFO - 22:40:39: deleting the raw counts dictionary of 36007 items\n",
      "INFO - 22:40:39: sample=6e-05 downsamples 1245 most-common words\n",
      "INFO - 22:40:39: downsampling leaves estimated 1063097 word corpus (52.7% of prior 2016739)\n",
      "INFO - 22:40:39: estimated required memory for 13262 words and 300 dimensions: 38459800 bytes\n",
      "INFO - 22:40:39: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.16 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per = 10000)\n",
    "print('Time to build vocab: {} mins'.format(round((time()-t)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:40:45: training model with 4 workers on 13262 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=5 window=2\n",
      "INFO - 22:40:46: EPOCH 1 - PROGRESS: at 15.01% examples, 151586 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:40:47: EPOCH 1 - PROGRESS: at 29.95% examples, 153307 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:40:48: EPOCH 1 - PROGRESS: at 45.81% examples, 157550 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:40:49: EPOCH 1 - PROGRESS: at 60.65% examples, 157931 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:40:50: EPOCH 1 - PROGRESS: at 77.15% examples, 160780 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:40:51: EPOCH 1 - PROGRESS: at 92.82% examples, 160230 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:40:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:40:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:40:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:40:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:40:52: EPOCH - 1 : training on 2056851 raw words (1062349 effective words) took 6.6s, 160926 effective words/s\n",
      "INFO - 22:40:53: EPOCH 2 - PROGRESS: at 15.01% examples, 157990 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 22:40:54: EPOCH 2 - PROGRESS: at 31.38% examples, 164807 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:40:55: EPOCH 2 - PROGRESS: at 46.77% examples, 164208 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:40:56: EPOCH 2 - PROGRESS: at 63.56% examples, 167502 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:40:57: EPOCH 2 - PROGRESS: at 78.05% examples, 163482 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:40:58: EPOCH 2 - PROGRESS: at 91.81% examples, 159268 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:40:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:40:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:40:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:40:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:40:58: EPOCH - 2 : training on 2056851 raw words (1062397 effective words) took 6.6s, 161458 effective words/s\n",
      "INFO - 22:40:59: EPOCH 3 - PROGRESS: at 15.01% examples, 158724 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:00: EPOCH 3 - PROGRESS: at 29.95% examples, 156596 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:01: EPOCH 3 - PROGRESS: at 43.89% examples, 153960 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:02: EPOCH 3 - PROGRESS: at 59.68% examples, 156744 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:03: EPOCH 3 - PROGRESS: at 76.21% examples, 160198 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:04: EPOCH 3 - PROGRESS: at 93.28% examples, 162672 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:41:05: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:05: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:05: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:05: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:05: EPOCH - 3 : training on 2056851 raw words (1061728 effective words) took 6.5s, 163656 effective words/s\n",
      "INFO - 22:41:06: EPOCH 4 - PROGRESS: at 15.95% examples, 168197 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:41:07: EPOCH 4 - PROGRESS: at 31.38% examples, 165324 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:08: EPOCH 4 - PROGRESS: at 46.71% examples, 164092 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:09: EPOCH 4 - PROGRESS: at 62.60% examples, 163694 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:10: EPOCH 4 - PROGRESS: at 78.05% examples, 163447 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:11: EPOCH 4 - PROGRESS: at 93.75% examples, 162682 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:11: EPOCH - 4 : training on 2056851 raw words (1062838 effective words) took 6.5s, 162459 effective words/s\n",
      "INFO - 22:41:12: EPOCH 5 - PROGRESS: at 15.95% examples, 167573 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:41:13: EPOCH 5 - PROGRESS: at 32.38% examples, 169269 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:14: EPOCH 5 - PROGRESS: at 49.13% examples, 171660 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:15: EPOCH 5 - PROGRESS: at 65.04% examples, 171172 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:16: EPOCH 5 - PROGRESS: at 81.97% examples, 171973 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:17: EPOCH 5 - PROGRESS: at 98.56% examples, 172457 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:17: EPOCH - 5 : training on 2056851 raw words (1063671 effective words) took 6.2s, 172441 effective words/s\n",
      "INFO - 22:41:18: EPOCH 6 - PROGRESS: at 15.95% examples, 167087 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:19: EPOCH 6 - PROGRESS: at 32.38% examples, 170487 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:20: EPOCH 6 - PROGRESS: at 49.13% examples, 171040 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:21: EPOCH 6 - PROGRESS: at 65.55% examples, 171340 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:23: EPOCH 6 - PROGRESS: at 81.97% examples, 171124 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:24: EPOCH 6 - PROGRESS: at 97.58% examples, 169359 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:24: EPOCH - 6 : training on 2056851 raw words (1062403 effective words) took 6.3s, 169041 effective words/s\n",
      "INFO - 22:41:25: EPOCH 7 - PROGRESS: at 15.49% examples, 162216 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:26: EPOCH 7 - PROGRESS: at 31.89% examples, 167469 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:27: EPOCH 7 - PROGRESS: at 47.70% examples, 167533 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:41:28: EPOCH 7 - PROGRESS: at 64.05% examples, 167962 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:29: EPOCH 7 - PROGRESS: at 79.47% examples, 167185 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:30: EPOCH 7 - PROGRESS: at 96.16% examples, 168343 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:30: EPOCH - 7 : training on 2056851 raw words (1063557 effective words) took 6.3s, 168376 effective words/s\n",
      "INFO - 22:41:31: EPOCH 8 - PROGRESS: at 15.95% examples, 165153 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:32: EPOCH 8 - PROGRESS: at 32.86% examples, 167175 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:33: EPOCH 8 - PROGRESS: at 49.13% examples, 168783 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:34: EPOCH 8 - PROGRESS: at 65.04% examples, 168017 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:35: EPOCH 8 - PROGRESS: at 81.45% examples, 169213 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:41:36: EPOCH 8 - PROGRESS: at 98.07% examples, 170097 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:41:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:36: EPOCH - 8 : training on 2056851 raw words (1062504 effective words) took 6.2s, 170171 effective words/s\n",
      "INFO - 22:41:37: EPOCH 9 - PROGRESS: at 15.95% examples, 166326 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:41:38: EPOCH 9 - PROGRESS: at 32.38% examples, 167206 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:39: EPOCH 9 - PROGRESS: at 47.70% examples, 163847 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:40: EPOCH 9 - PROGRESS: at 64.05% examples, 166247 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:41: EPOCH 9 - PROGRESS: at 80.47% examples, 167834 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:42: EPOCH 9 - PROGRESS: at 96.15% examples, 166959 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:43: EPOCH - 9 : training on 2056851 raw words (1062358 effective words) took 6.3s, 167706 effective words/s\n",
      "INFO - 22:41:44: EPOCH 10 - PROGRESS: at 15.48% examples, 159770 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:45: EPOCH 10 - PROGRESS: at 32.38% examples, 166322 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:46: EPOCH 10 - PROGRESS: at 49.13% examples, 170583 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:47: EPOCH 10 - PROGRESS: at 65.04% examples, 169826 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 22:41:48: EPOCH 10 - PROGRESS: at 81.93% examples, 171276 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:49: EPOCH 10 - PROGRESS: at 98.56% examples, 171077 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:49: EPOCH - 10 : training on 2056851 raw words (1062219 effective words) took 6.2s, 171433 effective words/s\n",
      "INFO - 22:41:50: EPOCH 11 - PROGRESS: at 16.46% examples, 169987 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:41:51: EPOCH 11 - PROGRESS: at 33.34% examples, 172536 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:52: EPOCH 11 - PROGRESS: at 49.59% examples, 171285 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:53: EPOCH 11 - PROGRESS: at 66.48% examples, 173149 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:54: EPOCH 11 - PROGRESS: at 80.95% examples, 169414 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:55: EPOCH 11 - PROGRESS: at 96.64% examples, 167822 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:41:55: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:41:55: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:41:55: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:41:55: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:41:55: EPOCH - 11 : training on 2056851 raw words (1062415 effective words) took 6.3s, 167889 effective words/s\n",
      "INFO - 22:41:56: EPOCH 12 - PROGRESS: at 15.95% examples, 166550 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:57: EPOCH 12 - PROGRESS: at 32.38% examples, 168132 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:41:58: EPOCH 12 - PROGRESS: at 47.21% examples, 164581 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:41:59: EPOCH 12 - PROGRESS: at 63.56% examples, 166673 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:00: EPOCH 12 - PROGRESS: at 79.98% examples, 167950 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:01: EPOCH 12 - PROGRESS: at 96.62% examples, 168188 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:01: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:01: EPOCH - 12 : training on 2056851 raw words (1063730 effective words) took 6.3s, 169670 effective words/s\n",
      "INFO - 22:42:03: EPOCH 13 - PROGRESS: at 16.46% examples, 167024 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:04: EPOCH 13 - PROGRESS: at 32.86% examples, 169274 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:05: EPOCH 13 - PROGRESS: at 49.13% examples, 170316 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:06: EPOCH 13 - PROGRESS: at 66.00% examples, 171652 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:07: EPOCH 13 - PROGRESS: at 82.92% examples, 172585 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:08: EPOCH 13 - PROGRESS: at 99.57% examples, 172714 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 22:42:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:08: EPOCH - 13 : training on 2056851 raw words (1062839 effective words) took 6.1s, 172887 effective words/s\n",
      "INFO - 22:42:09: EPOCH 14 - PROGRESS: at 15.95% examples, 167810 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:10: EPOCH 14 - PROGRESS: at 32.86% examples, 171706 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:11: EPOCH 14 - PROGRESS: at 48.65% examples, 170317 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:12: EPOCH 14 - PROGRESS: at 64.05% examples, 168401 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:13: EPOCH 14 - PROGRESS: at 80.47% examples, 168000 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:14: EPOCH 14 - PROGRESS: at 97.58% examples, 169327 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:14: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:14: EPOCH - 14 : training on 2056851 raw words (1063112 effective words) took 6.3s, 170011 effective words/s\n",
      "INFO - 22:42:15: EPOCH 15 - PROGRESS: at 15.95% examples, 166933 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:16: EPOCH 15 - PROGRESS: at 31.86% examples, 167186 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:17: EPOCH 15 - PROGRESS: at 48.65% examples, 170564 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:18: EPOCH 15 - PROGRESS: at 63.56% examples, 167099 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:19: EPOCH 15 - PROGRESS: at 79.98% examples, 167846 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:42:20: EPOCH 15 - PROGRESS: at 96.16% examples, 166800 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:20: EPOCH - 15 : training on 2056851 raw words (1062814 effective words) took 6.3s, 167616 effective words/s\n",
      "INFO - 22:42:21: EPOCH 16 - PROGRESS: at 15.98% examples, 163959 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:22: EPOCH 16 - PROGRESS: at 32.86% examples, 170373 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:23: EPOCH 16 - PROGRESS: at 48.65% examples, 169744 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:24: EPOCH 16 - PROGRESS: at 65.04% examples, 170715 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:25: EPOCH 16 - PROGRESS: at 80.97% examples, 170483 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:26: EPOCH 16 - PROGRESS: at 97.58% examples, 170556 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:26: EPOCH - 16 : training on 2056851 raw words (1063293 effective words) took 6.2s, 171804 effective words/s\n",
      "INFO - 22:42:27: EPOCH 17 - PROGRESS: at 14.05% examples, 146279 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:28: EPOCH 17 - PROGRESS: at 29.95% examples, 156200 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:29: EPOCH 17 - PROGRESS: at 46.27% examples, 161997 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:42:30: EPOCH 17 - PROGRESS: at 61.62% examples, 162178 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:32: EPOCH 17 - PROGRESS: at 78.05% examples, 163577 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:33: EPOCH 17 - PROGRESS: at 95.68% examples, 166196 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:33: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:33: EPOCH - 17 : training on 2056851 raw words (1062952 effective words) took 6.4s, 167129 effective words/s\n",
      "INFO - 22:42:34: EPOCH 18 - PROGRESS: at 15.48% examples, 163972 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:35: EPOCH 18 - PROGRESS: at 31.89% examples, 163192 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:36: EPOCH 18 - PROGRESS: at 48.65% examples, 168408 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:37: EPOCH 18 - PROGRESS: at 65.04% examples, 169092 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:42:38: EPOCH 18 - PROGRESS: at 81.45% examples, 169743 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:39: EPOCH 18 - PROGRESS: at 98.07% examples, 170289 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:39: EPOCH - 18 : training on 2056851 raw words (1063445 effective words) took 6.2s, 171032 effective words/s\n",
      "INFO - 22:42:40: EPOCH 19 - PROGRESS: at 16.46% examples, 171229 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:41: EPOCH 19 - PROGRESS: at 32.86% examples, 172110 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:42: EPOCH 19 - PROGRESS: at 49.13% examples, 170627 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:43: EPOCH 19 - PROGRESS: at 64.05% examples, 167198 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:44: EPOCH 19 - PROGRESS: at 79.98% examples, 167460 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:45: EPOCH 19 - PROGRESS: at 96.64% examples, 168698 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:45: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:45: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:45: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:45: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:45: EPOCH - 19 : training on 2056851 raw words (1062696 effective words) took 6.3s, 168853 effective words/s\n",
      "INFO - 22:42:46: EPOCH 20 - PROGRESS: at 15.48% examples, 162600 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:47: EPOCH 20 - PROGRESS: at 32.38% examples, 163886 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:48: EPOCH 20 - PROGRESS: at 49.60% examples, 169191 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:49: EPOCH 20 - PROGRESS: at 66.48% examples, 170504 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:42:51: EPOCH 20 - PROGRESS: at 82.45% examples, 169555 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:52: EPOCH 20 - PROGRESS: at 98.56% examples, 169371 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:52: EPOCH - 20 : training on 2056851 raw words (1062641 effective words) took 6.2s, 170069 effective words/s\n",
      "INFO - 22:42:53: EPOCH 21 - PROGRESS: at 16.46% examples, 173266 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:54: EPOCH 21 - PROGRESS: at 32.86% examples, 173062 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:55: EPOCH 21 - PROGRESS: at 49.13% examples, 172260 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:56: EPOCH 21 - PROGRESS: at 65.55% examples, 172053 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:42:57: EPOCH 21 - PROGRESS: at 81.97% examples, 172154 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:42:58: EPOCH 21 - PROGRESS: at 99.08% examples, 171901 words/s, in_qsize 2, out_qsize 3\n",
      "INFO - 22:42:58: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:42:58: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:42:58: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:42:58: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:42:58: EPOCH - 21 : training on 2056851 raw words (1062373 effective words) took 6.1s, 173049 effective words/s\n",
      "INFO - 22:42:59: EPOCH 22 - PROGRESS: at 13.09% examples, 133546 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:00: EPOCH 22 - PROGRESS: at 26.58% examples, 137310 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:01: EPOCH 22 - PROGRESS: at 41.48% examples, 143591 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:43:02: EPOCH 22 - PROGRESS: at 56.76% examples, 148331 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:03: EPOCH 22 - PROGRESS: at 72.81% examples, 152421 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:04: EPOCH 22 - PROGRESS: at 90.31% examples, 156370 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:04: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:04: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:04: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:04: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:04: EPOCH - 22 : training on 2056851 raw words (1062702 effective words) took 6.7s, 158421 effective words/s\n",
      "INFO - 22:43:06: EPOCH 23 - PROGRESS: at 16.46% examples, 167605 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:07: EPOCH 23 - PROGRESS: at 32.86% examples, 168203 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:08: EPOCH 23 - PROGRESS: at 49.13% examples, 170025 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 22:43:09: EPOCH 23 - PROGRESS: at 65.55% examples, 170977 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:10: EPOCH 23 - PROGRESS: at 81.47% examples, 170395 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:11: EPOCH 23 - PROGRESS: at 98.07% examples, 170715 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:11: EPOCH - 23 : training on 2056851 raw words (1063698 effective words) took 6.2s, 171021 effective words/s\n",
      "INFO - 22:43:12: EPOCH 24 - PROGRESS: at 15.95% examples, 165790 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:13: EPOCH 24 - PROGRESS: at 32.38% examples, 167603 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:14: EPOCH 24 - PROGRESS: at 49.13% examples, 169484 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:15: EPOCH 24 - PROGRESS: at 65.04% examples, 168010 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:16: EPOCH 24 - PROGRESS: at 81.47% examples, 167941 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:17: EPOCH 24 - PROGRESS: at 98.07% examples, 168817 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:17: EPOCH - 24 : training on 2056851 raw words (1063587 effective words) took 6.3s, 169340 effective words/s\n",
      "INFO - 22:43:18: EPOCH 25 - PROGRESS: at 15.95% examples, 163444 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:43:19: EPOCH 25 - PROGRESS: at 32.38% examples, 166359 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:20: EPOCH 25 - PROGRESS: at 49.13% examples, 167920 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 22:43:21: EPOCH 25 - PROGRESS: at 66.03% examples, 169061 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:22: EPOCH 25 - PROGRESS: at 82.44% examples, 170024 words/s, in_qsize 0, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:43:23: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:23: EPOCH 25 - PROGRESS: at 100.00% examples, 172361 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:23: EPOCH - 25 : training on 2056851 raw words (1063297 effective words) took 6.2s, 172292 effective words/s\n",
      "INFO - 22:43:24: EPOCH 26 - PROGRESS: at 15.01% examples, 155937 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:25: EPOCH 26 - PROGRESS: at 30.42% examples, 158582 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:26: EPOCH 26 - PROGRESS: at 46.71% examples, 161054 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:27: EPOCH 26 - PROGRESS: at 62.13% examples, 160654 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:28: EPOCH 26 - PROGRESS: at 77.15% examples, 158754 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:29: EPOCH 26 - PROGRESS: at 93.28% examples, 159744 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:30: EPOCH - 26 : training on 2056851 raw words (1063475 effective words) took 6.6s, 160797 effective words/s\n",
      "INFO - 22:43:31: EPOCH 27 - PROGRESS: at 15.01% examples, 155505 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:32: EPOCH 27 - PROGRESS: at 30.92% examples, 162088 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:33: EPOCH 27 - PROGRESS: at 45.81% examples, 160877 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:34: EPOCH 27 - PROGRESS: at 60.17% examples, 159063 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:35: EPOCH 27 - PROGRESS: at 75.22% examples, 158318 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:36: EPOCH 27 - PROGRESS: at 90.80% examples, 158344 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:36: EPOCH - 27 : training on 2056851 raw words (1063603 effective words) took 6.6s, 160591 effective words/s\n",
      "INFO - 22:43:37: EPOCH 28 - PROGRESS: at 14.05% examples, 148269 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:38: EPOCH 28 - PROGRESS: at 27.54% examples, 145810 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:39: EPOCH 28 - PROGRESS: at 43.42% examples, 152744 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 22:43:41: EPOCH 28 - PROGRESS: at 60.65% examples, 158541 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 22:43:42: EPOCH 28 - PROGRESS: at 78.05% examples, 163150 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:43: EPOCH 28 - PROGRESS: at 94.72% examples, 164717 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:43: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:43: EPOCH - 28 : training on 2056851 raw words (1062781 effective words) took 6.4s, 165182 effective words/s\n",
      "INFO - 22:43:44: EPOCH 29 - PROGRESS: at 15.95% examples, 166277 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:45: EPOCH 29 - PROGRESS: at 31.89% examples, 166488 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:46: EPOCH 29 - PROGRESS: at 47.70% examples, 166088 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:47: EPOCH 29 - PROGRESS: at 62.60% examples, 164291 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 22:43:48: EPOCH 29 - PROGRESS: at 78.05% examples, 164296 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:49: EPOCH 29 - PROGRESS: at 93.29% examples, 162692 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 22:43:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:49: EPOCH - 29 : training on 2056851 raw words (1062775 effective words) took 6.5s, 163505 effective words/s\n",
      "INFO - 22:43:50: EPOCH 30 - PROGRESS: at 15.48% examples, 160589 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:51: EPOCH 30 - PROGRESS: at 32.38% examples, 169059 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:52: EPOCH 30 - PROGRESS: at 48.65% examples, 169396 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:53: EPOCH 30 - PROGRESS: at 65.04% examples, 170267 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:55: EPOCH 30 - PROGRESS: at 81.45% examples, 169931 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:56: EPOCH 30 - PROGRESS: at 98.07% examples, 170402 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 22:43:56: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 22:43:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 22:43:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 22:43:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 22:43:56: EPOCH - 30 : training on 2056851 raw words (1063781 effective words) took 6.3s, 170165 effective words/s\n",
      "INFO - 22:43:56: training on a 61705530 raw words (31888033 effective words) took 190.7s, 167229 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 3.18 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 30, report_delay = 1)\n",
    "print('Time to build vocab: {} mins'.format(round((time()-t)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:47:32: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reconstruct': <gensim.models.keyedvectors.Vocab at 0x1837f020bb0>,\n",
       " 'subject': <gensim.models.keyedvectors.Vocab at 0x18335426d00>,\n",
       " 'specif': <gensim.models.keyedvectors.Vocab at 0x1837f028e50>,\n",
       " 'effect': <gensim.models.keyedvectors.Vocab at 0x1837f028f40>,\n",
       " 'map': <gensim.models.keyedvectors.Vocab at 0x18336fac8e0>,\n",
       " 'predict': <gensim.models.keyedvectors.Vocab at 0x1837f1593d0>,\n",
       " 'model': <gensim.models.keyedvectors.Vocab at 0x1837f159eb0>,\n",
       " 'allow': <gensim.models.keyedvectors.Vocab at 0x1837f159d60>,\n",
       " 'infer': <gensim.models.keyedvectors.Vocab at 0x1837f1595b0>,\n",
       " 'analyz': <gensim.models.keyedvectors.Vocab at 0x1837f159a90>,\n",
       " 'diseas': <gensim.models.keyedvectors.Vocab at 0x1837f1597c0>,\n",
       " 'relat': <gensim.models.keyedvectors.Vocab at 0x1837f159f40>,\n",
       " 'alter': <gensim.models.keyedvectors.Vocab at 0x1837f159ac0>,\n",
       " 'neuroimag': <gensim.models.keyedvectors.Vocab at 0x1837f1591f0>,\n",
       " 'data': <gensim.models.keyedvectors.Vocab at 0x1837f159fa0>,\n",
       " 'given': <gensim.models.keyedvectors.Vocab at 0x1837f159220>,\n",
       " 'made': <gensim.models.keyedvectors.Vocab at 0x1837f1599d0>,\n",
       " 'two': <gensim.models.keyedvectors.Vocab at 0x1837f1591c0>,\n",
       " 'level': <gensim.models.keyedvectors.Vocab at 0x1837f159310>,\n",
       " 'global': <gensim.models.keyedvectors.Vocab at 0x1837f159550>,\n",
       " 'e': <gensim.models.keyedvectors.Vocab at 0x1837f159610>,\n",
       " 'condit': <gensim.models.keyedvectors.Vocab at 0x1837f1594c0>,\n",
       " 'presenc': <gensim.models.keyedvectors.Vocab at 0x1837f159e50>,\n",
       " 'local': <gensim.models.keyedvectors.Vocab at 0x1837f159460>,\n",
       " 'detect': <gensim.models.keyedvectors.Vocab at 0x1837f1595e0>,\n",
       " 'individu': <gensim.models.keyedvectors.Vocab at 0x1837f159760>,\n",
       " 'measur': <gensim.models.keyedvectors.Vocab at 0x1837f159ca0>,\n",
       " 'extract': <gensim.models.keyedvectors.Vocab at 0x1837f159b50>,\n",
       " 'wide_use': <gensim.models.keyedvectors.Vocab at 0x1837f1596d0>,\n",
       " 'use': <gensim.models.keyedvectors.Vocab at 0x1837f159fd0>,\n",
       " 'form': <gensim.models.keyedvectors.Vocab at 0x1837f159490>,\n",
       " 'rare': <gensim.models.keyedvectors.Vocab at 0x1837f159250>,\n",
       " 'exist': <gensim.models.keyedvectors.Vocab at 0x1837f159430>,\n",
       " 'often': <gensim.models.keyedvectors.Vocab at 0x1837f159e80>,\n",
       " 'yield': <gensim.models.keyedvectors.Vocab at 0x1837f159370>,\n",
       " 'noisi': <gensim.models.keyedvectors.Vocab at 0x1837f159c10>,\n",
       " 'compos': <gensim.models.keyedvectors.Vocab at 0x1837f159bb0>,\n",
       " 'dispers': <gensim.models.keyedvectors.Vocab at 0x1837f159b80>,\n",
       " 'isol': <gensim.models.keyedvectors.Vocab at 0x1837f159c70>,\n",
       " 'island': <gensim.models.keyedvectors.Vocab at 0x1837f1594f0>,\n",
       " 'articl': <gensim.models.keyedvectors.Vocab at 0x1837f159dc0>,\n",
       " 'propos': <gensim.models.keyedvectors.Vocab at 0x1837f1599a0>,\n",
       " 'method': <gensim.models.keyedvectors.Vocab at 0x1837f1593a0>,\n",
       " 'name': <gensim.models.keyedvectors.Vocab at 0x1837f159d90>,\n",
       " 'rsm': <gensim.models.keyedvectors.Vocab at 0x1837f159b20>,\n",
       " 'improv': <gensim.models.keyedvectors.Vocab at 0x1837f159d30>,\n",
       " 'approach': <gensim.models.keyedvectors.Vocab at 0x18336f10df0>,\n",
       " 'particular': <gensim.models.keyedvectors.Vocab at 0x1837fa2d040>,\n",
       " 'binari': <gensim.models.keyedvectors.Vocab at 0x1837fa2d0a0>,\n",
       " 'classifi': <gensim.models.keyedvectors.Vocab at 0x1837fa2d100>,\n",
       " 'aim': <gensim.models.keyedvectors.Vocab at 0x1837fa2d160>,\n",
       " 'reduc': <gensim.models.keyedvectors.Vocab at 0x1837fa2d1c0>,\n",
       " 'nois': <gensim.models.keyedvectors.Vocab at 0x1837fa2d220>,\n",
       " 'due': <gensim.models.keyedvectors.Vocab at 0x1837fa2d280>,\n",
       " 'sampl': <gensim.models.keyedvectors.Vocab at 0x1837fa2d2e0>,\n",
       " 'error': <gensim.models.keyedvectors.Vocab at 0x1837fa2d340>,\n",
       " 'associ': <gensim.models.keyedvectors.Vocab at 0x1837fa2d3a0>,\n",
       " 'finit': <gensim.models.keyedvectors.Vocab at 0x1837fa2d400>,\n",
       " 'exampl': <gensim.models.keyedvectors.Vocab at 0x1837fa2d460>,\n",
       " 'train': <gensim.models.keyedvectors.Vocab at 0x1837fa2d4c0>,\n",
       " 'wrapper': <gensim.models.keyedvectors.Vocab at 0x1837fa2d520>,\n",
       " 'type': <gensim.models.keyedvectors.Vocab at 0x1837fa2d580>,\n",
       " 'algorithm': <gensim.models.keyedvectors.Vocab at 0x1837fa2d5e0>,\n",
       " 'differ': <gensim.models.keyedvectors.Vocab at 0x1837fa2d640>,\n",
       " 'diagnost': <gensim.models.keyedvectors.Vocab at 0x1837fa2d6a0>,\n",
       " 'manner': <gensim.models.keyedvectors.Vocab at 0x1837fa2d700>,\n",
       " 'without': <gensim.models.keyedvectors.Vocab at 0x1837fa2d760>,\n",
       " 'inform': <gensim.models.keyedvectors.Vocab at 0x1837fa2d7c0>,\n",
       " 'pose': <gensim.models.keyedvectors.Vocab at 0x1837fa2d820>,\n",
       " 'maximum': <gensim.models.keyedvectors.Vocab at 0x1837fa2d880>,\n",
       " 'posteriori': <gensim.models.keyedvectors.Vocab at 0x1837fa2d8e0>,\n",
       " 'problem': <gensim.models.keyedvectors.Vocab at 0x1837fa2d940>,\n",
       " 'prior': <gensim.models.keyedvectors.Vocab at 0x1837fa2d9a0>,\n",
       " 'whose': <gensim.models.keyedvectors.Vocab at 0x1837fa2da00>,\n",
       " 'paramet': <gensim.models.keyedvectors.Vocab at 0x1837fa2da60>,\n",
       " 'estim': <gensim.models.keyedvectors.Vocab at 0x1837fa2dac0>,\n",
       " 'fashion': <gensim.models.keyedvectors.Vocab at 0x1837fa2db20>,\n",
       " 'experiment': <gensim.models.keyedvectors.Vocab at 0x1837fa2db80>,\n",
       " 'evalu': <gensim.models.keyedvectors.Vocab at 0x1837fa2dbe0>,\n",
       " 'perform': <gensim.models.keyedvectors.Vocab at 0x1837fa2dc40>,\n",
       " 'synthet': <gensim.models.keyedvectors.Vocab at 0x1837fa2dca0>,\n",
       " 'generat': <gensim.models.keyedvectors.Vocab at 0x1837fa2dd00>,\n",
       " 'alzheim': <gensim.models.keyedvectors.Vocab at 0x1837fa2dd60>,\n",
       " 'initi': <gensim.models.keyedvectors.Vocab at 0x1837fa2ddc0>,\n",
       " 'adni': <gensim.models.keyedvectors.Vocab at 0x1837fa2de20>,\n",
       " 'databas': <gensim.models.keyedvectors.Vocab at 0x1837fa2de80>,\n",
       " 'result': <gensim.models.keyedvectors.Vocab at 0x1837fa2dee0>,\n",
       " 'synthet_data': <gensim.models.keyedvectors.Vocab at 0x1837fa2df40>,\n",
       " 'demonstr': <gensim.models.keyedvectors.Vocab at 0x1837fa2dfa0>,\n",
       " 'higher': <gensim.models.keyedvectors.Vocab at 0x1837fa2e040>,\n",
       " 'accuraci': <gensim.models.keyedvectors.Vocab at 0x1837fa2e0a0>,\n",
       " 'compar': <gensim.models.keyedvectors.Vocab at 0x1837fa2e100>,\n",
       " 'direct': <gensim.models.keyedvectors.Vocab at 0x1837fa2e160>,\n",
       " 'bootstrap': <gensim.models.keyedvectors.Vocab at 0x1837fa2e1c0>,\n",
       " 'averag': <gensim.models.keyedvectors.Vocab at 0x1837fa2e220>,\n",
       " 'analys': <gensim.models.keyedvectors.Vocab at 0x1837fa2e280>,\n",
       " 'dataset': <gensim.models.keyedvectors.Vocab at 0x1837fa2e2e0>,\n",
       " 'show': <gensim.models.keyedvectors.Vocab at 0x1837fa2e340>,\n",
       " 'also': <gensim.models.keyedvectors.Vocab at 0x1837fa2e3a0>,\n",
       " 'correl': <gensim.models.keyedvectors.Vocab at 0x1837fa2e400>,\n",
       " 'cortic': <gensim.models.keyedvectors.Vocab at 0x1837fa2e460>,\n",
       " 'thick': <gensim.models.keyedvectors.Vocab at 0x1837fa2e4c0>,\n",
       " 'non': <gensim.models.keyedvectors.Vocab at 0x1837fa2e520>,\n",
       " 'imag': <gensim.models.keyedvectors.Vocab at 0x1837fa2e580>,\n",
       " 'marker': <gensim.models.keyedvectors.Vocab at 0x1837fa2e5e0>,\n",
       " 'ad': <gensim.models.keyedvectors.Vocab at 0x1837fa2e640>,\n",
       " 'mini': <gensim.models.keyedvectors.Vocab at 0x1837fa2e6a0>,\n",
       " 'mental': <gensim.models.keyedvectors.Vocab at 0x1837fa2e700>,\n",
       " 'state': <gensim.models.keyedvectors.Vocab at 0x1837fa2e760>,\n",
       " 'examin': <gensim.models.keyedvectors.Vocab at 0x1837fa2e7c0>,\n",
       " 'score': <gensim.models.keyedvectors.Vocab at 0x1837fa2e820>,\n",
       " 'fluid': <gensim.models.keyedvectors.Vocab at 0x1837fa2e880>,\n",
       " 'amyloid': <gensim.models.keyedvectors.Vocab at 0x1837fa2e8e0>,\n",
       " 'beta': <gensim.models.keyedvectors.Vocab at 0x1837fa2e940>,\n",
       " 'reliabl': <gensim.models.keyedvectors.Vocab at 0x1837fa2e9a0>,\n",
       " 'studi': <gensim.models.keyedvectors.Vocab at 0x1837fa2ea00>,\n",
       " 'longitudin': <gensim.models.keyedvectors.Vocab at 0x1837fa2ea60>,\n",
       " 'rotat': <gensim.models.keyedvectors.Vocab at 0x1837fa2eac0>,\n",
       " 'invari': <gensim.models.keyedvectors.Vocab at 0x1837fa2eb20>,\n",
       " 'neural_network': <gensim.models.keyedvectors.Vocab at 0x1837fa2eb80>,\n",
       " 'translat': <gensim.models.keyedvectors.Vocab at 0x1837fa2ebe0>,\n",
       " 'great': <gensim.models.keyedvectors.Vocab at 0x1837fa2ec40>,\n",
       " 'valu': <gensim.models.keyedvectors.Vocab at 0x1837fa2eca0>,\n",
       " 'recognit': <gensim.models.keyedvectors.Vocab at 0x1837fa2ed00>,\n",
       " 'task': <gensim.models.keyedvectors.Vocab at 0x1837fa2ed60>,\n",
       " 'paper': <gensim.models.keyedvectors.Vocab at 0x1837fa2edc0>,\n",
       " 'bring': <gensim.models.keyedvectors.Vocab at 0x1837fa2ee20>,\n",
       " 'new': <gensim.models.keyedvectors.Vocab at 0x1837fa2ee80>,\n",
       " 'architectur': <gensim.models.keyedvectors.Vocab at 0x1837fa2eee0>,\n",
       " 'convolut_neural': <gensim.models.keyedvectors.Vocab at 0x1837fa2ef40>,\n",
       " 'network_cnn': <gensim.models.keyedvectors.Vocab at 0x1837fa2efa0>,\n",
       " 'cyclic': <gensim.models.keyedvectors.Vocab at 0x1837fa2f040>,\n",
       " 'convolut': <gensim.models.keyedvectors.Vocab at 0x1837fa2f0a0>,\n",
       " 'layer': <gensim.models.keyedvectors.Vocab at 0x1837fa2f100>,\n",
       " 'achiev': <gensim.models.keyedvectors.Vocab at 0x1837fa2f160>,\n",
       " 'symbol': <gensim.models.keyedvectors.Vocab at 0x1837fa2f1c0>,\n",
       " 'get': <gensim.models.keyedvectors.Vocab at 0x1837fa2f220>,\n",
       " 'posit': <gensim.models.keyedvectors.Vocab at 0x1837fa2f280>,\n",
       " 'orient': <gensim.models.keyedvectors.Vocab at 0x1837fa2f2e0>,\n",
       " 'network': <gensim.models.keyedvectors.Vocab at 0x1837fa2f340>,\n",
       " 'purpos': <gensim.models.keyedvectors.Vocab at 0x1837fa2f3a0>,\n",
       " 'multipl': <gensim.models.keyedvectors.Vocab at 0x1837fa2f400>,\n",
       " 'overlap': <gensim.models.keyedvectors.Vocab at 0x1837fa2f460>,\n",
       " 'target': <gensim.models.keyedvectors.Vocab at 0x1837fa2f4c0>,\n",
       " 'last': <gensim.models.keyedvectors.Vocab at 0x1837fa2f520>,\n",
       " 'least': <gensim.models.keyedvectors.Vocab at 0x1837fa2f580>,\n",
       " 'one': <gensim.models.keyedvectors.Vocab at 0x1837fa2f5e0>,\n",
       " 'shot_learn': <gensim.models.keyedvectors.Vocab at 0x1837fa2f640>,\n",
       " 'case': <gensim.models.keyedvectors.Vocab at 0x1837fa2f6a0>,\n",
       " 'spheric': <gensim.models.keyedvectors.Vocab at 0x1837fa2f700>,\n",
       " 'polyharmon': <gensim.models.keyedvectors.Vocab at 0x1837fa2f760>,\n",
       " 'poisson': <gensim.models.keyedvectors.Vocab at 0x1837fa2f7c0>,\n",
       " 'kernel': <gensim.models.keyedvectors.Vocab at 0x1837fa2f820>,\n",
       " 'function': <gensim.models.keyedvectors.Vocab at 0x1837fa2f880>,\n",
       " 'introduc': <gensim.models.keyedvectors.Vocab at 0x1837fa2f8e0>,\n",
       " 'develop': <gensim.models.keyedvectors.Vocab at 0x1837fa2f940>,\n",
       " 'notion': <gensim.models.keyedvectors.Vocab at 0x1837fa2f9a0>,\n",
       " 'natur': <gensim.models.keyedvectors.Vocab at 0x1837fa2fa00>,\n",
       " 'generalis': <gensim.models.keyedvectors.Vocab at 0x1837fa2fa60>,\n",
       " 'harmon': <gensim.models.keyedvectors.Vocab at 0x1837fa2fac0>,\n",
       " 'theori': <gensim.models.keyedvectors.Vocab at 0x1837fa2fb20>,\n",
       " 'zonal': <gensim.models.keyedvectors.Vocab at 0x1837fa2fb80>,\n",
       " 'allow_us': <gensim.models.keyedvectors.Vocab at 0x1837fa2fbe0>,\n",
       " 'analog': <gensim.models.keyedvectors.Vocab at 0x1837fa2fc40>,\n",
       " 'construct': <gensim.models.keyedvectors.Vocab at 0x1837fa2fca0>,\n",
       " 'union': <gensim.models.keyedvectors.Vocab at 0x1837fa2fd00>,\n",
       " 'ball': <gensim.models.keyedvectors.Vocab at 0x1837fa2fd60>,\n",
       " 'find': <gensim.models.keyedvectors.Vocab at 0x1837fa2fdc0>,\n",
       " 'represent': <gensim.models.keyedvectors.Vocab at 0x1837fa2fe20>,\n",
       " 'term': <gensim.models.keyedvectors.Vocab at 0x1837fa2fe80>,\n",
       " 'gegenbau': <gensim.models.keyedvectors.Vocab at 0x1837fa2fee0>,\n",
       " 'polynomi': <gensim.models.keyedvectors.Vocab at 0x1837fa2ff40>,\n",
       " 'connect': <gensim.models.keyedvectors.Vocab at 0x1837fa2ffa0>,\n",
       " 'classic': <gensim.models.keyedvectors.Vocab at 0x1837fa30040>,\n",
       " 'cauchi': <gensim.models.keyedvectors.Vocab at 0x1837fa300a0>,\n",
       " 'holomorph': <gensim.models.keyedvectors.Vocab at 0x1837fa30100>,\n",
       " 'lie': <gensim.models.keyedvectors.Vocab at 0x1837fa30160>,\n",
       " 'finit_element': <gensim.models.keyedvectors.Vocab at 0x1837fa301c0>,\n",
       " 'approxim': <gensim.models.keyedvectors.Vocab at 0x1837fa30220>,\n",
       " 'stochast': <gensim.models.keyedvectors.Vocab at 0x1837fa30280>,\n",
       " 'maxwel': <gensim.models.keyedvectors.Vocab at 0x1837fa302e0>,\n",
       " 'landau': <gensim.models.keyedvectors.Vocab at 0x1837fa30340>,\n",
       " 'lifshitz': <gensim.models.keyedvectors.Vocab at 0x1837fa303a0>,\n",
       " 'gilbert': <gensim.models.keyedvectors.Vocab at 0x1837fa30400>,\n",
       " 'system': <gensim.models.keyedvectors.Vocab at 0x1837fa30460>,\n",
       " 'equat': <gensim.models.keyedvectors.Vocab at 0x1837fa304c0>,\n",
       " 'coupl': <gensim.models.keyedvectors.Vocab at 0x1837fa30520>,\n",
       " 'call': <gensim.models.keyedvectors.Vocab at 0x1837fa30580>,\n",
       " 'describ': <gensim.models.keyedvectors.Vocab at 0x1837fa305e0>,\n",
       " 'creation': <gensim.models.keyedvectors.Vocab at 0x1837fa30640>,\n",
       " 'domain_wall': <gensim.models.keyedvectors.Vocab at 0x1837fa306a0>,\n",
       " 'vortic': <gensim.models.keyedvectors.Vocab at 0x1837fa30700>,\n",
       " 'fundament': <gensim.models.keyedvectors.Vocab at 0x1837fa30760>,\n",
       " 'object': <gensim.models.keyedvectors.Vocab at 0x1837fa307c0>,\n",
       " 'novel': <gensim.models.keyedvectors.Vocab at 0x1837fa30820>,\n",
       " 'nanostructur': <gensim.models.keyedvectors.Vocab at 0x1837fa30880>,\n",
       " 'magnet': <gensim.models.keyedvectors.Vocab at 0x18367859f10>,\n",
       " 'memori': <gensim.models.keyedvectors.Vocab at 0x1837f014580>,\n",
       " 'first': <gensim.models.keyedvectors.Vocab at 0x18336e86910>,\n",
       " 'reformul': <gensim.models.keyedvectors.Vocab at 0x1837fa308b0>,\n",
       " 'time': <gensim.models.keyedvectors.Vocab at 0x1837fa30910>,\n",
       " 'differenti': <gensim.models.keyedvectors.Vocab at 0x1837fa30970>,\n",
       " 'solut': <gensim.models.keyedvectors.Vocab at 0x1837fa309d0>,\n",
       " 'converg': <gensim.models.keyedvectors.Vocab at 0x1837fa30a30>,\n",
       " 'theta': <gensim.models.keyedvectors.Vocab at 0x1837fa30a90>,\n",
       " 'linear': <gensim.models.keyedvectors.Vocab at 0x1837fa30af0>,\n",
       " 'scheme': <gensim.models.keyedvectors.Vocab at 0x1837fa30b50>,\n",
       " 'consequ': <gensim.models.keyedvectors.Vocab at 0x1837fa30bb0>,\n",
       " 'prove': <gensim.models.keyedvectors.Vocab at 0x1837fa30c10>,\n",
       " 'minor': <gensim.models.keyedvectors.Vocab at 0x1837fa30c70>,\n",
       " 'space': <gensim.models.keyedvectors.Vocab at 0x1837fa30cd0>,\n",
       " 'step': <gensim.models.keyedvectors.Vocab at 0x1837fa30d30>,\n",
       " 'depend': <gensim.models.keyedvectors.Vocab at 0x1837fa30d90>,\n",
       " 'henc': <gensim.models.keyedvectors.Vocab at 0x1837fa30df0>,\n",
       " 'weak': <gensim.models.keyedvectors.Vocab at 0x1837fa30e50>,\n",
       " 'martingal': <gensim.models.keyedvectors.Vocab at 0x1837fa30eb0>,\n",
       " 'numer': <gensim.models.keyedvectors.Vocab at 0x1837fa30f10>,\n",
       " 'present': <gensim.models.keyedvectors.Vocab at 0x1837fa30f70>,\n",
       " 'applic': <gensim.models.keyedvectors.Vocab at 0x1837fa30fd0>,\n",
       " 'discret': <gensim.models.keyedvectors.Vocab at 0x1837fa31070>,\n",
       " 'wavelet': <gensim.models.keyedvectors.Vocab at 0x1837fa310d0>,\n",
       " 'transform': <gensim.models.keyedvectors.Vocab at 0x1837fa31130>,\n",
       " 'tensor': <gensim.models.keyedvectors.Vocab at 0x1837fa31190>,\n",
       " 'decomposit': <gensim.models.keyedvectors.Vocab at 0x1837fa311f0>,\n",
       " 'featur_extract': <gensim.models.keyedvectors.Vocab at 0x1837fa31250>,\n",
       " 'ftir': <gensim.models.keyedvectors.Vocab at 0x1837fa312b0>,\n",
       " 'medicin': <gensim.models.keyedvectors.Vocab at 0x1837fa31310>,\n",
       " 'plant': <gensim.models.keyedvectors.Vocab at 0x1837fa31370>,\n",
       " 'fourier_transform': <gensim.models.keyedvectors.Vocab at 0x1837fa313d0>,\n",
       " 'infra': <gensim.models.keyedvectors.Vocab at 0x1837fa31430>,\n",
       " 'red': <gensim.models.keyedvectors.Vocab at 0x1837fa31490>,\n",
       " 'spectra': <gensim.models.keyedvectors.Vocab at 0x1837fa314f0>,\n",
       " 'speci': <gensim.models.keyedvectors.Vocab at 0x1837fa31550>,\n",
       " 'explor': <gensim.models.keyedvectors.Vocab at 0x1837fa315b0>,\n",
       " 'influenc': <gensim.models.keyedvectors.Vocab at 0x1837fa31610>,\n",
       " 'preprocess': <gensim.models.keyedvectors.Vocab at 0x1837fa31670>,\n",
       " 'effici': <gensim.models.keyedvectors.Vocab at 0x1837fa316d0>,\n",
       " 'machin_learn': <gensim.models.keyedvectors.Vocab at 0x1837fa31730>,\n",
       " 'dwt': <gensim.models.keyedvectors.Vocab at 0x1837fa31790>,\n",
       " 'techniqu': <gensim.models.keyedvectors.Vocab at 0x1837fa317f0>,\n",
       " 'various': <gensim.models.keyedvectors.Vocab at 0x1837fa31850>,\n",
       " 'combin': <gensim.models.keyedvectors.Vocab at 0x1837fa318b0>,\n",
       " 'signal': <gensim.models.keyedvectors.Vocab at 0x1837fa31910>,\n",
       " 'process': <gensim.models.keyedvectors.Vocab at 0x1837fa31970>,\n",
       " 'behavior': <gensim.models.keyedvectors.Vocab at 0x1837fa319d0>,\n",
       " 'appli': <gensim.models.keyedvectors.Vocab at 0x1837fa31a30>,\n",
       " 'classif': <gensim.models.keyedvectors.Vocab at 0x1837fa31a90>,\n",
       " 'cluster': <gensim.models.keyedvectors.Vocab at 0x1837fa31af0>,\n",
       " 'best': <gensim.models.keyedvectors.Vocab at 0x1837fa31b50>,\n",
       " 'found': <gensim.models.keyedvectors.Vocab at 0x1837fa31bb0>,\n",
       " 'grid': <gensim.models.keyedvectors.Vocab at 0x1837fa31c10>,\n",
       " 'search': <gensim.models.keyedvectors.Vocab at 0x1837fa31c70>,\n",
       " 'similar': <gensim.models.keyedvectors.Vocab at 0x1837fa31cd0>,\n",
       " 'signific_improv': <gensim.models.keyedvectors.Vocab at 0x1837fa31d30>,\n",
       " 'qualiti': <gensim.models.keyedvectors.Vocab at 0x1837fa31d90>,\n",
       " 'well': <gensim.models.keyedvectors.Vocab at 0x1837fa31df0>,\n",
       " 'classif_accuraci': <gensim.models.keyedvectors.Vocab at 0x1837fa31e50>,\n",
       " 'tune': <gensim.models.keyedvectors.Vocab at 0x1837fa31eb0>,\n",
       " 'logist_regress': <gensim.models.keyedvectors.Vocab at 0x1837fa31f10>,\n",
       " 'comparison': <gensim.models.keyedvectors.Vocab at 0x1837fa31f70>,\n",
       " 'origin': <gensim.models.keyedvectors.Vocab at 0x1837fa31fd0>,\n",
       " 'unlik': <gensim.models.keyedvectors.Vocab at 0x1837fa32070>,\n",
       " 'rank': <gensim.models.keyedvectors.Vocab at 0x1837fa320d0>,\n",
       " 'make': <gensim.models.keyedvectors.Vocab at 0x1837fa32130>,\n",
       " 'versatil': <gensim.models.keyedvectors.Vocab at 0x1837fa32190>,\n",
       " 'easier': <gensim.models.keyedvectors.Vocab at 0x1837fa321f0>,\n",
       " 'tool': <gensim.models.keyedvectors.Vocab at 0x1837fa32250>,\n",
       " 'maxim': <gensim.models.keyedvectors.Vocab at 0x1837fa322b0>,\n",
       " 'frequenc': <gensim.models.keyedvectors.Vocab at 0x1837fa32310>,\n",
       " 'complement': <gensim.models.keyedvectors.Vocab at 0x1837fa32370>,\n",
       " 'obstacl': <gensim.models.keyedvectors.Vocab at 0x1837fa323d0>,\n",
       " 'let': <gensim.models.keyedvectors.Vocab at 0x1837fa32430>,\n",
       " 'omega': <gensim.models.keyedvectors.Vocab at 0x1837fa32490>,\n",
       " 'subset_mathbb': <gensim.models.keyedvectors.Vocab at 0x1837fa324f0>,\n",
       " 'r_n': <gensim.models.keyedvectors.Vocab at 0x1837fa32550>,\n",
       " 'bound': <gensim.models.keyedvectors.Vocab at 0x1837fa325b0>,\n",
       " 'domain': <gensim.models.keyedvectors.Vocab at 0x1837fa32610>,\n",
       " 'satisfi': <gensim.models.keyedvectors.Vocab at 0x1837fa32670>,\n",
       " 'asymmetri': <gensim.models.keyedvectors.Vocab at 0x1837fa326d0>,\n",
       " 'arbitrari': <gensim.models.keyedvectors.Vocab at 0x1837fa32730>,\n",
       " 'refer': <gensim.models.keyedvectors.Vocab at 0x1837fa32790>,\n",
       " 'interest': <gensim.models.keyedvectors.Vocab at 0x1837fa327f0>,\n",
       " 'behaviour': <gensim.models.keyedvectors.Vocab at 0x1837fa32850>,\n",
       " 'dirichlet': <gensim.models.keyedvectors.Vocab at 0x1837fa328b0>,\n",
       " 'eigenvalu': <gensim.models.keyedvectors.Vocab at 0x1837fa32910>,\n",
       " 'lambda': <gensim.models.keyedvectors.Vocab at 0x1837fa32970>,\n",
       " 'setminus': <gensim.models.keyedvectors.Vocab at 0x1837fa329d0>,\n",
       " 'x': <gensim.models.keyedvectors.Vocab at 0x1837fa32a30>,\n",
       " 'upper_bound': <gensim.models.keyedvectors.Vocab at 0x1837fa32a90>,\n",
       " 'distanc': <gensim.models.keyedvectors.Vocab at 0x1837fa32af0>,\n",
       " 'set': <gensim.models.keyedvectors.Vocab at 0x1837fa32b50>,\n",
       " 'point': <gensim.models.keyedvectors.Vocab at 0x1837fa32bb0>,\n",
       " 'ground_state': <gensim.models.keyedvectors.Vocab at 0x1837fa32c10>,\n",
       " 'phi': <gensim.models.keyedvectors.Vocab at 0x1837fa32c70>,\n",
       " 'short': <gensim.models.keyedvectors.Vocab at 0x1837fa32cd0>,\n",
       " 'corollari': <gensim.models.keyedvectors.Vocab at 0x1837fa32d30>,\n",
       " 'begin': <gensim.models.keyedvectors.Vocab at 0x1837fa32d90>,\n",
       " 'mu': <gensim.models.keyedvectors.Vocab at 0x1837fa32df0>,\n",
       " 'max': <gensim.models.keyedvectors.Vocab at 0x1837fa32e50>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x1837fa32eb0>,\n",
       " 'larg': <gensim.models.keyedvectors.Vocab at 0x1837fa32f10>,\n",
       " 'enough': <gensim.models.keyedvectors.Vocab at 0x1837fa32f70>,\n",
       " 'close': <gensim.models.keyedvectors.Vocab at 0x1837fa32fd0>,\n",
       " 'second': <gensim.models.keyedvectors.Vocab at 0x1837fa33070>,\n",
       " 'discuss': <gensim.models.keyedvectors.Vocab at 0x1837fa330d0>,\n",
       " 'distribut': <gensim.models.keyedvectors.Vocab at 0x1837fa33130>,\n",
       " 'possibl': <gensim.models.keyedvectors.Vocab at 0x1837fa33190>,\n",
       " 'inscrib': <gensim.models.keyedvectors.Vocab at 0x1837fa331f0>,\n",
       " 'wavelength': <gensim.models.keyedvectors.Vocab at 0x1837fa33250>,\n",
       " 'final': <gensim.models.keyedvectors.Vocab at 0x1837fa332b0>,\n",
       " 'specifi': <gensim.models.keyedvectors.Vocab at 0x1837fa33310>,\n",
       " 'observ': <gensim.models.keyedvectors.Vocab at 0x1837fa33370>,\n",
       " 'convex': <gensim.models.keyedvectors.Vocab at 0x1837fa333d0>,\n",
       " 'suffici': <gensim.models.keyedvectors.Vocab at 0x1837fa33430>,\n",
       " 'respect': <gensim.models.keyedvectors.Vocab at 0x1837fa33490>,\n",
       " 'contain': <gensim.models.keyedvectors.Vocab at 0x1837fa334f0>,\n",
       " 'period': <gensim.models.keyedvectors.Vocab at 0x1837fa33550>,\n",
       " 'shape': <gensim.models.keyedvectors.Vocab at 0x1837fa335b0>,\n",
       " 'hyperbol': <gensim.models.keyedvectors.Vocab at 0x1837fa33610>,\n",
       " 'asteroid': <gensim.models.keyedvectors.Vocab at 0x1837fa33670>,\n",
       " 'oumuamua': <gensim.models.keyedvectors.Vocab at 0x1837fa336d0>,\n",
       " 'u': <gensim.models.keyedvectors.Vocab at 0x1837fa33730>,\n",
       " 'lightcurv': <gensim.models.keyedvectors.Vocab at 0x1837fa33790>,\n",
       " 'newli': <gensim.models.keyedvectors.Vocab at 0x1837fa337f0>,\n",
       " 'discov': <gensim.models.keyedvectors.Vocab at 0x1837fa33850>,\n",
       " 'planet': <gensim.models.keyedvectors.Vocab at 0x1837fa338b0>,\n",
       " 'octob': <gensim.models.keyedvectors.Vocab at 0x1837fa33910>,\n",
       " 'observatori': <gensim.models.keyedvectors.Vocab at 0x1837fa33970>,\n",
       " 'discoveri': <gensim.models.keyedvectors.Vocab at 0x1837fa339d0>,\n",
       " 'channel': <gensim.models.keyedvectors.Vocab at 0x1837fa33a30>,\n",
       " 'telescop': <gensim.models.keyedvectors.Vocab at 0x1837fa33a90>,\n",
       " 'deriv': <gensim.models.keyedvectors.Vocab at 0x1837fa33af0>,\n",
       " 'partial': <gensim.models.keyedvectors.Vocab at 0x1837fa33b50>,\n",
       " 'peak': <gensim.models.keyedvectors.Vocab at 0x1837fa33bb0>,\n",
       " 'trough': <gensim.models.keyedvectors.Vocab at 0x1837fa33c10>,\n",
       " 'amplitud': <gensim.models.keyedvectors.Vocab at 0x1837fa33c70>,\n",
       " 'mag': <gensim.models.keyedvectors.Vocab at 0x1837fa33cd0>,\n",
       " 'segment': <gensim.models.keyedvectors.Vocab at 0x1837fa33d30>,\n",
       " 'rule': <gensim.models.keyedvectors.Vocab at 0x1837fa33d90>,\n",
       " 'less': <gensim.models.keyedvectors.Vocab at 0x1837fa33df0>,\n",
       " 'hr': <gensim.models.keyedvectors.Vocab at 0x1837fa33e50>,\n",
       " 'suggest': <gensim.models.keyedvectors.Vocab at 0x1837fa33eb0>,\n",
       " 'assumpt': <gensim.models.keyedvectors.Vocab at 0x1837fa33f10>,\n",
       " 'variabl': <gensim.models.keyedvectors.Vocab at 0x1837fa33f70>,\n",
       " 'chang': <gensim.models.keyedvectors.Vocab at 0x1837fa33fd0>,\n",
       " 'cross_section': <gensim.models.keyedvectors.Vocab at 0x1837fa34070>,\n",
       " 'axial': <gensim.models.keyedvectors.Vocab at 0x1837fa340d0>,\n",
       " 'ratio': <gensim.models.keyedvectors.Vocab at 0x1837fa34130>,\n",
       " 'saw': <gensim.models.keyedvectors.Vocab at 0x1837fa34190>,\n",
       " 'evid': <gensim.models.keyedvectors.Vocab at 0x1837fa341f0>,\n",
       " 'coma': <gensim.models.keyedvectors.Vocab at 0x1837fa34250>,\n",
       " 'tail': <gensim.models.keyedvectors.Vocab at 0x1837fa342b0>,\n",
       " 'either': <gensim.models.keyedvectors.Vocab at 0x1837fa34310>,\n",
       " 'stack': <gensim.models.keyedvectors.Vocab at 0x1837fa34370>,\n",
       " 'equival': <gensim.models.keyedvectors.Vocab at 0x1837fa343d0>,\n",
       " 'exposur': <gensim.models.keyedvectors.Vocab at 0x1837fa34430>,\n",
       " 'advers': <gensim.models.keyedvectors.Vocab at 0x1837fa34490>,\n",
       " 'polym': <gensim.models.keyedvectors.Vocab at 0x1837fa344f0>,\n",
       " 'coat': <gensim.models.keyedvectors.Vocab at 0x1837fa34550>,\n",
       " 'heat': <gensim.models.keyedvectors.Vocab at 0x1837fa345b0>,\n",
       " 'transport': <gensim.models.keyedvectors.Vocab at 0x1837fa34610>,\n",
       " 'solid': <gensim.models.keyedvectors.Vocab at 0x1837fa34670>,\n",
       " 'liquid': <gensim.models.keyedvectors.Vocab at 0x1837fa346d0>,\n",
       " 'interfac': <gensim.models.keyedvectors.Vocab at 0x1837fa34730>,\n",
       " 'abil': <gensim.models.keyedvectors.Vocab at 0x1837fa34790>,\n",
       " 'metal': <gensim.models.keyedvectors.Vocab at 0x1837fa347f0>,\n",
       " 'nanoparticl': <gensim.models.keyedvectors.Vocab at 0x1837fa34850>,\n",
       " 'suppli': <gensim.models.keyedvectors.Vocab at 0x1837fa348b0>,\n",
       " 'environ': <gensim.models.keyedvectors.Vocab at 0x1837fa34910>,\n",
       " 'extern': <gensim.models.keyedvectors.Vocab at 0x1837fa34970>,\n",
       " 'optic': <gensim.models.keyedvectors.Vocab at 0x1837fa349d0>,\n",
       " 'field': <gensim.models.keyedvectors.Vocab at 0x1837fa34a30>,\n",
       " 'attract': <gensim.models.keyedvectors.Vocab at 0x1837fa34a90>,\n",
       " 'grow': <gensim.models.keyedvectors.Vocab at 0x1837fa34af0>,\n",
       " 'biomed': <gensim.models.keyedvectors.Vocab at 0x1837fa34b50>,\n",
       " 'control': <gensim.models.keyedvectors.Vocab at 0x1837fa34bb0>,\n",
       " 'thermal': <gensim.models.keyedvectors.Vocab at 0x1837fa34c10>,\n",
       " 'properti': <gensim.models.keyedvectors.Vocab at 0x1837fa34c70>,\n",
       " 'appear': <gensim.models.keyedvectors.Vocab at 0x1837fa34cd0>,\n",
       " 'relev': <gensim.models.keyedvectors.Vocab at 0x1837fa34d30>,\n",
       " 'work': <gensim.models.keyedvectors.Vocab at 0x1837fa34d90>,\n",
       " 'address': <gensim.models.keyedvectors.Vocab at 0x1837fa34df0>,\n",
       " 'water': <gensim.models.keyedvectors.Vocab at 0x1837fa34e50>,\n",
       " 'gold': <gensim.models.keyedvectors.Vocab at 0x1837fa34eb0>,\n",
       " 'surfac': <gensim.models.keyedvectors.Vocab at 0x1837fa34f10>,\n",
       " 'molecular_dynam': <gensim.models.keyedvectors.Vocab at 0x1837fa34f70>,\n",
       " 'simul': <gensim.models.keyedvectors.Vocab at 0x1837fa34fd0>,\n",
       " 'increas': <gensim.models.keyedvectors.Vocab at 0x1837fa35070>,\n",
       " 'densiti': <gensim.models.keyedvectors.Vocab at 0x1837fa350d0>,\n",
       " 'displac': <gensim.models.keyedvectors.Vocab at 0x1837fa35130>,\n",
       " 'resist': <gensim.models.keyedvectors.Vocab at 0x1837fa35190>,\n",
       " 'flow': <gensim.models.keyedvectors.Vocab at 0x1837fa351f0>,\n",
       " 'affect': <gensim.models.keyedvectors.Vocab at 0x1837fa35250>,\n",
       " 'amount': <gensim.models.keyedvectors.Vocab at 0x1837fa352b0>,\n",
       " 'energi': <gensim.models.keyedvectors.Vocab at 0x1837fa35310>,\n",
       " 'releas': <gensim.models.keyedvectors.Vocab at 0x1837fa35370>,\n",
       " 'unexpect': <gensim.models.keyedvectors.Vocab at 0x1837fa353d0>,\n",
       " 'trade': <gensim.models.keyedvectors.Vocab at 0x1837fa35430>,\n",
       " 'establish': <gensim.models.keyedvectors.Vocab at 0x1837fa35490>,\n",
       " 'sph': <gensim.models.keyedvectors.Vocab at 0x1837fa354f0>,\n",
       " 'calcul': <gensim.models.keyedvectors.Vocab at 0x1837fa35550>,\n",
       " 'mar': <gensim.models.keyedvectors.Vocab at 0x1837fa355b0>,\n",
       " 'scale': <gensim.models.keyedvectors.Vocab at 0x1837fa35610>,\n",
       " 'collis': <gensim.models.keyedvectors.Vocab at 0x1837fa35670>,\n",
       " 'role': <gensim.models.keyedvectors.Vocab at 0x1837fa356d0>,\n",
       " 'materi': <gensim.models.keyedvectors.Vocab at 0x1837fa35730>,\n",
       " 'rheolog': <gensim.models.keyedvectors.Vocab at 0x1837fa35790>,\n",
       " 'larg_scale': <gensim.models.keyedvectors.Vocab at 0x1837fa357f0>,\n",
       " 'approx': <gensim.models.keyedvectors.Vocab at 0x1837fa35850>,\n",
       " 'km': <gensim.models.keyedvectors.Vocab at 0x1837fa358b0>,\n",
       " 'impact': <gensim.models.keyedvectors.Vocab at 0x1837fa35910>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x1837fa35970>,\n",
       " 'smooth': <gensim.models.keyedvectors.Vocab at 0x1837fa359d0>,\n",
       " 'particl': <gensim.models.keyedvectors.Vocab at 0x1837fa35a30>,\n",
       " 'hydrodynam': <gensim.models.keyedvectors.Vocab at 0x1837fa35a90>,\n",
       " 'code': <gensim.models.keyedvectors.Vocab at 0x1837fa35af0>,\n",
       " 'strength': <gensim.models.keyedvectors.Vocab at 0x1837fa35b50>,\n",
       " 'post': <gensim.models.keyedvectors.Vocab at 0x1837fa35bb0>,\n",
       " 'temperatur': <gensim.models.keyedvectors.Vocab at 0x1837fa35c10>,\n",
       " 'investig': <gensim.models.keyedvectors.Vocab at 0x1837fa35c70>,\n",
       " 'eject': <gensim.models.keyedvectors.Vocab at 0x1837fa35cd0>,\n",
       " 'escap': <gensim.models.keyedvectors.Vocab at 0x1837fa35d30>,\n",
       " 'disc': <gensim.models.keyedvectors.Vocab at 0x1837fa35d90>,\n",
       " 'mass': <gensim.models.keyedvectors.Vocab at 0x1837fa35df0>,\n",
       " 'potenti': <gensim.models.keyedvectors.Vocab at 0x1837fa35e50>,\n",
       " 'context': <gensim.models.keyedvectors.Vocab at 0x1837fa35eb0>,\n",
       " 'discontinu': <gensim.models.keyedvectors.Vocab at 0x1837fa35f10>,\n",
       " 'rigid': <gensim.models.keyedvectors.Vocab at 0x1837fa35f70>,\n",
       " 'bodi': <gensim.models.keyedvectors.Vocab at 0x1837fa35fd0>,\n",
       " 'regim': <gensim.models.keyedvectors.Vocab at 0x1837fa37070>,\n",
       " 'consid': <gensim.models.keyedvectors.Vocab at 0x1837fa370d0>,\n",
       " 'veloc': <gensim.models.keyedvectors.Vocab at 0x1837fa37130>,\n",
       " 'substanti': <gensim.models.keyedvectors.Vocab at 0x1837fa37190>,\n",
       " 'impactor': <gensim.models.keyedvectors.Vocab at 0x1837fa371f0>,\n",
       " 'subtl': <gensim.models.keyedvectors.Vocab at 0x1837fa37250>,\n",
       " 'high': <gensim.models.keyedvectors.Vocab at 0x1837fa372b0>,\n",
       " 'mathcal': <gensim.models.keyedvectors.Vocab at 0x1837fa37310>,\n",
       " 'r': <gensim.models.keyedvectors.Vocab at 0x1837fa37370>,\n",
       " 'fail': <gensim.models.keyedvectors.Vocab at 0x1837fa373d0>,\n",
       " 'outbreak': <gensim.models.keyedvectors.Vocab at 0x1837fa37430>,\n",
       " 'boost': <gensim.models.keyedvectors.Vocab at 0x1837fa37490>,\n",
       " 'immun': <gensim.models.keyedvectors.Vocab at 0x1837fa374f0>,\n",
       " 'time_vari': <gensim.models.keyedvectors.Vocab at 0x1837fa37550>,\n",
       " 'suscept': <gensim.models.keyedvectors.Vocab at 0x1837fa375b0>,\n",
       " 'host': <gensim.models.keyedvectors.Vocab at 0x1837fa37610>,\n",
       " 'wane': <gensim.models.keyedvectors.Vocab at 0x1837fa37670>,\n",
       " 'known': <gensim.models.keyedvectors.Vocab at 0x1837fa376d0>,\n",
       " 'induc': <gensim.models.keyedvectors.Vocab at 0x1837fa37730>,\n",
       " 'rich': <gensim.models.keyedvectors.Vocab at 0x1837fa37790>,\n",
       " 'long_term': <gensim.models.keyedvectors.Vocab at 0x1837fa377f0>,\n",
       " 'transmiss': <gensim.models.keyedvectors.Vocab at 0x1837fa37850>,\n",
       " 'dynam': <gensim.models.keyedvectors.Vocab at 0x1837fa378b0>,\n",
       " 'meanwhil': <gensim.models.keyedvectors.Vocab at 0x1837fa37910>,\n",
       " 'heterogen': <gensim.models.keyedvectors.Vocab at 0x1837fa37970>,\n",
       " 'shot': <gensim.models.keyedvectors.Vocab at 0x1837fa379d0>,\n",
       " 'epidem': <gensim.models.keyedvectors.Vocab at 0x1837fa37a30>,\n",
       " 'even_though': <gensim.models.keyedvectors.Vocab at 0x1837fa37a90>,\n",
       " 'larg_amount': <gensim.models.keyedvectors.Vocab at 0x1837fa37af0>,\n",
       " 'avail': <gensim.models.keyedvectors.Vocab at 0x1837fa37b50>,\n",
       " 'epidemiolog': <gensim.models.keyedvectors.Vocab at 0x1837fa37bb0>,\n",
       " 'short_term': <gensim.models.keyedvectors.Vocab at 0x1837fa37c10>,\n",
       " 'parsimoni': <gensim.models.keyedvectors.Vocab at 0x1837fa37c70>,\n",
       " 'mathemat': <gensim.models.keyedvectors.Vocab at 0x1837fa37cd0>,\n",
       " 'take_account': <gensim.models.keyedvectors.Vocab at 0x1837fa37d30>,\n",
       " 'obtain': <gensim.models.keyedvectors.Vocab at 0x1837fa37d90>,\n",
       " 'explicit': <gensim.models.keyedvectors.Vocab at 0x1837fa37df0>,\n",
       " 'delay': <gensim.models.keyedvectors.Vocab at 0x1837fa37e50>,\n",
       " 'take': <gensim.models.keyedvectors.Vocab at 0x1837fa37eb0>,\n",
       " 'negat': <gensim.models.keyedvectors.Vocab at 0x1837fa37f10>,\n",
       " 'slope': <gensim.models.keyedvectors.Vocab at 0x1837fa37f70>,\n",
       " 'curv': <gensim.models.keyedvectors.Vocab at 0x1837fa37fd0>,\n",
       " 'phase': <gensim.models.keyedvectors.Vocab at 0x1837fa38070>,\n",
       " 'addit': <gensim.models.keyedvectors.Vocab at 0x1837fa380d0>,\n",
       " 'common': <gensim.models.keyedvectors.Vocab at 0x1837fa38130>,\n",
       " 'standard': <gensim.models.keyedvectors.Vocab at 0x1837fa38190>,\n",
       " 'sir': <gensim.models.keyedvectors.Vocab at 0x1837fa381f0>,\n",
       " 'leq': <gensim.models.keyedvectors.Vocab at 0x1837fa38250>,\n",
       " 'normal': <gensim.models.keyedvectors.Vocab at 0x1837fa382b0>,\n",
       " 'employ': <gensim.models.keyedvectors.Vocab at 0x1837fa38310>,\n",
       " 'sensit': <gensim.models.keyedvectors.Vocab at 0x1837fa38370>,\n",
       " 'analysi': <gensim.models.keyedvectors.Vocab at 0x1837fa383d0>,\n",
       " 'order': <gensim.models.keyedvectors.Vocab at 0x1837fa38430>,\n",
       " 'hydraul': <gensim.models.keyedvectors.Vocab at 0x1837fa38490>,\n",
       " 'fractur': <gensim.models.keyedvectors.Vocab at 0x1837fa384f0>,\n",
       " 'horizont': <gensim.models.keyedvectors.Vocab at 0x1837fa38550>,\n",
       " 'systemat': <gensim.models.keyedvectors.Vocab at 0x1837fa385b0>,\n",
       " 'sobol': <gensim.models.keyedvectors.Vocab at 0x1837fa38610>,\n",
       " 'util': <gensim.models.keyedvectors.Vocab at 0x1837fa38670>,\n",
       " 'quantiti': <gensim.models.keyedvectors.Vocab at 0x1837fa386d0>,\n",
       " 'pore': <gensim.models.keyedvectors.Vocab at 0x1837fa38730>,\n",
       " 'pressur': <gensim.models.keyedvectors.Vocab at 0x1837fa38790>,\n",
       " 'deplet': <gensim.models.keyedvectors.Vocab at 0x1837fa387f0>,\n",
       " 'stress': <gensim.models.keyedvectors.Vocab at 0x1837fa38850>,\n",
       " 'around': <gensim.models.keyedvectors.Vocab at 0x1837fa388b0>,\n",
       " 'base': <gensim.models.keyedvectors.Vocab at 0x1837fa38910>,\n",
       " 'degre': <gensim.models.keyedvectors.Vocab at 0x1837fa38970>,\n",
       " 'import': <gensim.models.keyedvectors.Vocab at 0x1837fa389d0>,\n",
       " 'includ': <gensim.models.keyedvectors.Vocab at 0x1837fa38a30>,\n",
       " 'rock': <gensim.models.keyedvectors.Vocab at 0x1837fa38a90>,\n",
       " 'stimul': <gensim.models.keyedvectors.Vocab at 0x1837fa38af0>,\n",
       " 'design': <gensim.models.keyedvectors.Vocab at 0x1837fa38b50>,\n",
       " 'fulli': <gensim.models.keyedvectors.Vocab at 0x1837fa38bb0>,\n",
       " 'poroelast': <gensim.models.keyedvectors.Vocab at 0x1837fa38c10>,\n",
       " 'account': <gensim.models.keyedvectors.Vocab at 0x1837fa38c70>,\n",
       " 'product': <gensim.models.keyedvectors.Vocab at 0x1837fa38cd0>,\n",
       " 'eas': <gensim.models.keyedvectors.Vocab at 0x1837fa38d30>,\n",
       " 'comput_cost': <gensim.models.keyedvectors.Vocab at 0x1837fa38d90>,\n",
       " 'provid': <gensim.models.keyedvectors.Vocab at 0x1837fa38df0>,\n",
       " 'rom': <gensim.models.keyedvectors.Vocab at 0x1837fa38e50>,\n",
       " 'replac': <gensim.models.keyedvectors.Vocab at 0x1837fa38eb0>,\n",
       " 'complex': <gensim.models.keyedvectors.Vocab at 0x1837fa38f10>,\n",
       " 'rather': <gensim.models.keyedvectors.Vocab at 0x1837fa38f70>,\n",
       " 'simpl': <gensim.models.keyedvectors.Vocab at 0x1837fa38fd0>,\n",
       " 'analyt': <gensim.models.keyedvectors.Vocab at 0x1837fa3a070>,\n",
       " 'locat': <gensim.models.keyedvectors.Vocab at 0x1837fa3a0d0>,\n",
       " 'main': <gensim.models.keyedvectors.Vocab at 0x1837fa3a130>,\n",
       " 'research': <gensim.models.keyedvectors.Vocab at 0x1837fa3a190>,\n",
       " 'mobil': <gensim.models.keyedvectors.Vocab at 0x1837fa3a1f0>,\n",
       " 'half': <gensim.models.keyedvectors.Vocab at 0x1837fa3a250>,\n",
       " 'length': <gensim.models.keyedvectors.Vocab at 0x1837fa3a2b0>,\n",
       " 'contributor': <gensim.models.keyedvectors.Vocab at 0x1837fa3a310>,\n",
       " 'percentag': <gensim.models.keyedvectors.Vocab at 0x1837fa3a370>,\n",
       " 'contribut': <gensim.models.keyedvectors.Vocab at 0x1837fa3a3d0>,\n",
       " 'pre': <gensim.models.keyedvectors.Vocab at 0x1837fa3a430>,\n",
       " 'ii': <gensim.models.keyedvectors.Vocab at 0x1837fa3a490>,\n",
       " 'progress': <gensim.models.keyedvectors.Vocab at 0x1837fa3a4f0>,\n",
       " 'decreas': <gensim.models.keyedvectors.Vocab at 0x1837fa3a550>,\n",
       " 'iii': <gensim.models.keyedvectors.Vocab at 0x1837fa3a5b0>,\n",
       " 'domin': <gensim.models.keyedvectors.Vocab at 0x1837fa3a610>,\n",
       " 'iv': <gensim.models.keyedvectors.Vocab at 0x1837fa3a670>,\n",
       " 'zone': <gensim.models.keyedvectors.Vocab at 0x1837fa3a6d0>,\n",
       " 'tip': <gensim.models.keyedvectors.Vocab at 0x1837fa3a730>,\n",
       " 'insid': <gensim.models.keyedvectors.Vocab at 0x1837fa3a790>,\n",
       " 'area': <gensim.models.keyedvectors.Vocab at 0x1837fa3a7f0>,\n",
       " 'factor': <gensim.models.keyedvectors.Vocab at 0x1837fa3a850>,\n",
       " 'minimum': <gensim.models.keyedvectors.Vocab at 0x1837fa3a8b0>,\n",
       " 'guidelin': <gensim.models.keyedvectors.Vocab at 0x1837fa3a910>,\n",
       " 'legaci': <gensim.models.keyedvectors.Vocab at 0x1837fa3a970>,\n",
       " 'secondari': <gensim.models.keyedvectors.Vocab at 0x1837fa3a9d0>,\n",
       " 'oper': <gensim.models.keyedvectors.Vocab at 0x1837fa3aa30>,\n",
       " 'drill': <gensim.models.keyedvectors.Vocab at 0x1837fa3aa90>,\n",
       " 'separ': <gensim.models.keyedvectors.Vocab at 0x1837fa3aaf0>,\n",
       " 'social': <gensim.models.keyedvectors.Vocab at 0x1837fa3ab50>,\n",
       " 'dilemma': <gensim.models.keyedvectors.Vocab at 0x1837fa3abb0>,\n",
       " 'topolog': <gensim.models.keyedvectors.Vocab at 0x1837fa3ac10>,\n",
       " 'frustrat': <gensim.models.keyedvectors.Vocab at 0x1837fa3ac70>,\n",
       " 'three': <gensim.models.keyedvectors.Vocab at 0x1837fa3acd0>,\n",
       " 'crowd': <gensim.models.keyedvectors.Vocab at 0x1837fa3ad30>,\n",
       " 'old': <gensim.models.keyedvectors.Vocab at 0x1837fa3ad90>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x1837fa3adf0>,\n",
       " 'interact': <gensim.models.keyedvectors.Vocab at 0x1837fa3ae50>,\n",
       " 'configur': <gensim.models.keyedvectors.Vocab at 0x1837fa3aeb0>,\n",
       " 'statist': <gensim.models.keyedvectors.Vocab at 0x1837fa3af10>,\n",
       " 'physic': <gensim.models.keyedvectors.Vocab at 0x1837fa3af70>,\n",
       " 'accord': <gensim.models.keyedvectors.Vocab at 0x1837fa3afd0>,\n",
       " 'within': <gensim.models.keyedvectors.Vocab at 0x1837fa3d070>,\n",
       " 'triangl': <gensim.models.keyedvectors.Vocab at 0x1837fa3d0d0>,\n",
       " 'deserv': <gensim.models.keyedvectors.Vocab at 0x1837fa3d130>,\n",
       " 'special': <gensim.models.keyedvectors.Vocab at 0x1837fa3d190>,\n",
       " 'attent': <gensim.models.keyedvectors.Vocab at 0x1837fa3d1f0>,\n",
       " 'motiv': <gensim.models.keyedvectors.Vocab at 0x1837fa3d250>,\n",
       " 'evolutionari': <gensim.models.keyedvectors.Vocab at 0x1837fa3d2b0>,\n",
       " 'game': <gensim.models.keyedvectors.Vocab at 0x1837fa3d310>,\n",
       " 'triangular': <gensim.models.keyedvectors.Vocab at 0x1837fa3d370>,\n",
       " 'lattic': <gensim.models.keyedvectors.Vocab at 0x1837fa3d3d0>,\n",
       " 'prevent': <gensim.models.keyedvectors.Vocab at 0x1837fa3d430>,\n",
       " 'anti': <gensim.models.keyedvectors.Vocab at 0x1837fa3d490>,\n",
       " 'coordin': <gensim.models.keyedvectors.Vocab at 0x1837fa3d4f0>,\n",
       " 'compet': <gensim.models.keyedvectors.Vocab at 0x1837fa3d550>,\n",
       " 'strategi': <gensim.models.keyedvectors.Vocab at 0x1837fa3d5b0>,\n",
       " 'would': <gensim.models.keyedvectors.Vocab at 0x1837fa3d610>,\n",
       " 'need': <gensim.models.keyedvectors.Vocab at 0x1837fa3d670>,\n",
       " 'optim': <gensim.models.keyedvectors.Vocab at 0x1837fa3d6d0>,\n",
       " 'outcom': <gensim.models.keyedvectors.Vocab at 0x1837fa3d730>,\n",
       " 'updat': <gensim.models.keyedvectors.Vocab at 0x1837fa3d790>,\n",
       " 'protocol': <gensim.models.keyedvectors.Vocab at 0x1837fa3d7f0>,\n",
       " 'spatial': <gensim.models.keyedvectors.Vocab at 0x1837fa3d850>,\n",
       " 'pattern': <gensim.models.keyedvectors.Vocab at 0x1837fa3d8b0>,\n",
       " 'payoff': <gensim.models.keyedvectors.Vocab at 0x1837fa3d910>,\n",
       " 'reminisc': <gensim.models.keyedvectors.Vocab at 0x1837fa3d970>,\n",
       " 'honeycomb': <gensim.models.keyedvectors.Vocab at 0x1837fa3d9d0>,\n",
       " 'organ': <gensim.models.keyedvectors.Vocab at 0x1837fa3da30>,\n",
       " 'help': <gensim.models.keyedvectors.Vocab at 0x1837fa3da90>,\n",
       " 'minim': <gensim.models.keyedvectors.Vocab at 0x1837fa3daf0>,\n",
       " 'emerg': <gensim.models.keyedvectors.Vocab at 0x1837fa3db50>,\n",
       " 'microscop': <gensim.models.keyedvectors.Vocab at 0x1837fa3dbb0>,\n",
       " 'mean': <gensim.models.keyedvectors.Vocab at 0x1837fa3dc10>,\n",
       " 'mean_field': <gensim.models.keyedvectors.Vocab at 0x1837fa3dc70>,\n",
       " 'mont_carlo': <gensim.models.keyedvectors.Vocab at 0x1837fa3dcd0>,\n",
       " 'squar': <gensim.models.keyedvectors.Vocab at 0x1837fa3dd30>,\n",
       " 'cours': <gensim.models.keyedvectors.Vocab at 0x1837fa3dd90>,\n",
       " 'absent': <gensim.models.keyedvectors.Vocab at 0x1837fa3ddf0>,\n",
       " 'howev': <gensim.models.keyedvectors.Vocab at 0x1837fa3de50>,\n",
       " 'delet': <gensim.models.keyedvectors.Vocab at 0x1837fa3deb0>,\n",
       " 'diagon': <gensim.models.keyedvectors.Vocab at 0x1837fa3df10>,\n",
       " 'link': <gensim.models.keyedvectors.Vocab at 0x1837fa3df70>,\n",
       " 'gradual': <gensim.models.keyedvectors.Vocab at 0x1837fa3dfd0>,\n",
       " 'bridg': <gensim.models.keyedvectors.Vocab at 0x1837fa3e070>,\n",
       " 'gap': <gensim.models.keyedvectors.Vocab at 0x1837fa3e0d0>,\n",
       " 'cooper': <gensim.models.keyedvectors.Vocab at 0x1837fa3e130>,\n",
       " 'indic': <gensim.models.keyedvectors.Vocab at 0x1837fa3e190>,\n",
       " 'thus': <gensim.models.keyedvectors.Vocab at 0x1837fa3e1f0>,\n",
       " 'determin': <gensim.models.keyedvectors.Vocab at 0x1837fa3e250>,\n",
       " 'exciton': <gensim.models.keyedvectors.Vocab at 0x1837fa3e2b0>,\n",
       " 'polaron': <gensim.models.keyedvectors.Vocab at 0x1837fa3e310>,\n",
       " 'quantum': <gensim.models.keyedvectors.Vocab at 0x1837fa3e370>,\n",
       " 'self': <gensim.models.keyedvectors.Vocab at 0x1837fa3e3d0>,\n",
       " 'format': <gensim.models.keyedvectors.Vocab at 0x1837fa3e430>,\n",
       " 'cd': <gensim.models.keyedvectors.Vocab at 0x1837fa3e490>,\n",
       " 'mn': <gensim.models.keyedvectors.Vocab at 0x1837fa3e4f0>,\n",
       " 'se': <gensim.models.keyedvectors.Vocab at 0x1837fa3e550>,\n",
       " 'mg': <gensim.models.keyedvectors.Vocab at 0x1837fa3e5b0>,\n",
       " 'dilut': <gensim.models.keyedvectors.Vocab at 0x1837fa3e610>,\n",
       " 'semiconductor': <gensim.models.keyedvectors.Vocab at 0x1837fa3e670>,\n",
       " 'resolv': <gensim.models.keyedvectors.Vocab at 0x1837fa3e6d0>,\n",
       " 'photoluminesc': <gensim.models.keyedvectors.Vocab at 0x1837fa3e730>,\n",
       " 'pl': <gensim.models.keyedvectors.Vocab at 0x1837fa3e790>,\n",
       " 'magnet_field': <gensim.models.keyedvectors.Vocab at 0x1837fa3e7f0>,\n",
       " 'deduc': <gensim.models.keyedvectors.Vocab at 0x1837fa3e850>,\n",
       " 'mev': <gensim.models.keyedvectors.Vocab at 0x1837fa3e8b0>,\n",
       " 'agreement': <gensim.models.keyedvectors.Vocab at 0x1837fa3e910>,\n",
       " 'integr': <gensim.models.keyedvectors.Vocab at 0x1837fa3e970>,\n",
       " 'select': <gensim.models.keyedvectors.Vocab at 0x1837fa3e9d0>,\n",
       " 'excit': <gensim.models.keyedvectors.Vocab at 0x1837fa3ea30>,\n",
       " 'circular': <gensim.models.keyedvectors.Vocab at 0x1837fa3ea90>,\n",
       " 'polar': <gensim.models.keyedvectors.Vocab at 0x1837fa3eaf0>,\n",
       " 'ps': <gensim.models.keyedvectors.Vocab at 0x1837fa3eb50>,\n",
       " 'signific': <gensim.models.keyedvectors.Vocab at 0x1837fa3ebb0>,\n",
       " 'longer': <gensim.models.keyedvectors.Vocab at 0x1837fa3ec10>,\n",
       " 'correspond': <gensim.models.keyedvectors.Vocab at 0x1837fa3ec70>,\n",
       " 'report': <gensim.models.keyedvectors.Vocab at 0x1837fa3ecd0>,\n",
       " 'earlier': <gensim.models.keyedvectors.Vocab at 0x1837fa3ed30>,\n",
       " 'strong': <gensim.models.keyedvectors.Vocab at 0x1837fa3ed90>,\n",
       " 'accompani': <gensim.models.keyedvectors.Vocab at 0x1837fa3edf0>,\n",
       " 'squeez': <gensim.models.keyedvectors.Vocab at 0x1837fa3ee50>,\n",
       " 'heavi': <gensim.models.keyedvectors.Vocab at 0x1837fa3eeb0>,\n",
       " 'hole': <gensim.models.keyedvectors.Vocab at 0x1837fa3ef10>,\n",
       " 'envelop': <gensim.models.keyedvectors.Vocab at 0x1837fa3ef70>,\n",
       " 'wavefunct': <gensim.models.keyedvectors.Vocab at 0x1837fa3efd0>,\n",
       " 'conclus': <gensim.models.keyedvectors.Vocab at 0x1837fa40070>,\n",
       " 'support': <gensim.models.keyedvectors.Vocab at 0x1837fa400d0>,\n",
       " 'lifetim': <gensim.models.keyedvectors.Vocab at 0x1837fa40130>,\n",
       " 'varieti': <gensim.models.keyedvectors.Vocab at 0x1837fa40190>,\n",
       " 'automata': <gensim.models.keyedvectors.Vocab at 0x1837fa401f0>,\n",
       " 'eilenberg': <gensim.models.keyedvectors.Vocab at 0x1837fa40250>,\n",
       " 'concept': <gensim.models.keyedvectors.Vocab at 0x1837fa402b0>,\n",
       " 'syntact': <gensim.models.keyedvectors.Vocab at 0x1837fa40310>,\n",
       " 'monoid': <gensim.models.keyedvectors.Vocab at 0x1837fa40370>,\n",
       " 'regular': <gensim.models.keyedvectors.Vocab at 0x1837fa403d0>,\n",
       " 'languag': <gensim.models.keyedvectors.Vocab at 0x1837fa40430>,\n",
       " 'pseudovarieti': <gensim.models.keyedvectors.Vocab at 0x1837fa40490>,\n",
       " 'modif': <gensim.models.keyedvectors.Vocab at 0x1837fa404f0>,\n",
       " 'general': <gensim.models.keyedvectors.Vocab at 0x1837fa40550>,\n",
       " 'class': <gensim.models.keyedvectors.Vocab at 0x1837fa405b0>,\n",
       " 'hand': <gensim.models.keyedvectors.Vocab at 0x1837fa40610>,\n",
       " 'algebra': <gensim.models.keyedvectors.Vocab at 0x1837fa40670>,\n",
       " 'structur': <gensim.models.keyedvectors.Vocab at 0x1837fa406d0>,\n",
       " 'preimag': <gensim.models.keyedvectors.Vocab at 0x1837fa40730>,\n",
       " 'homomorph': <gensim.models.keyedvectors.Vocab at 0x1837fa40790>,\n",
       " 'equip': <gensim.models.keyedvectors.Vocab at 0x1837fa407f0>,\n",
       " 'compat': <gensim.models.keyedvectors.Vocab at 0x1837fa40850>,\n",
       " 'distinguish': <gensim.models.keyedvectors.Vocab at 0x1837fa408b0>,\n",
       " 'counterpart': <gensim.models.keyedvectors.Vocab at 0x1837fa40910>,\n",
       " 'previous': <gensim.models.keyedvectors.Vocab at 0x1837fa40970>,\n",
       " 'c': <gensim.models.keyedvectors.Vocab at 0x1837fa409d0>,\n",
       " 'spontan': <gensim.models.keyedvectors.Vocab at 0x1837fa40a30>,\n",
       " 'abrikosov': <gensim.models.keyedvectors.Vocab at 0x1837fa40a90>,\n",
       " 'vortex': <gensim.models.keyedvectors.Vocab at 0x1837fa40af0>,\n",
       " 'ferromagnet': <gensim.models.keyedvectors.Vocab at 0x1837fa40b50>,\n",
       " 'superconductor': <gensim.models.keyedvectors.Vocab at 0x1837fa40bb0>,\n",
       " 'euf': <gensim.models.keyedvectors.Vocab at 0x1837fa40c10>,\n",
       " 'p': <gensim.models.keyedvectors.Vocab at 0x1837fa40c70>,\n",
       " 'x_x': <gensim.models.keyedvectors.Vocab at 0x1837fa40cd0>,\n",
       " 'low_temperatur': <gensim.models.keyedvectors.Vocab at 0x1837fa40d30>,\n",
       " 'forc': <gensim.models.keyedvectors.Vocab at 0x1837fa40d90>,\n",
       " 'microscopi': <gensim.models.keyedvectors.Vocab at 0x1837fa40df0>,\n",
       " 'mfm': <gensim.models.keyedvectors.Vocab at 0x1837fa40e50>,\n",
       " 'svp': <gensim.models.keyedvectors.Vocab at 0x1837fa40eb0>,\n",
       " 'singl_crystal': <gensim.models.keyedvectors.Vocab at 0x1837fa40f10>,\n",
       " 'superconduct': <gensim.models.keyedvectors.Vocab at 0x1837fa40f70>,\n",
       " 'rm': <gensim.models.keyedvectors.Vocab at 0x1837fa40fd0>,\n",
       " 'sc': <gensim.models.keyedvectors.Vocab at 0x1837fa43070>,\n",
       " 'k': <gensim.models.keyedvectors.Vocab at 0x1837fa430d0>,\n",
       " 'fm': <gensim.models.keyedvectors.Vocab at 0x1837fa43130>,\n",
       " 'sim': <gensim.models.keyedvectors.Vocab at 0x1837fa43190>,\n",
       " 'transit': <gensim.models.keyedvectors.Vocab at 0x1837fa431f0>,\n",
       " 'antivortex': <gensim.models.keyedvectors.Vocab at 0x1837fa43250>,\n",
       " 'v': <gensim.models.keyedvectors.Vocab at 0x1837fa432b0>,\n",
       " 'av': <gensim.models.keyedvectors.Vocab at 0x1837fa43310>,\n",
       " 'pair': <gensim.models.keyedvectors.Vocab at 0x1837fa43370>,\n",
       " 'vicin': <gensim.models.keyedvectors.Vocab at 0x1837fa433d0>,\n",
       " 'upon': <gensim.models.keyedvectors.Vocab at 0x1837fa43430>,\n",
       " 'cool': <gensim.models.keyedvectors.Vocab at 0x1837fa43490>,\n",
       " 'cycl': <gensim.models.keyedvectors.Vocab at 0x1837fa434f0>,\n",
       " 'near': <gensim.models.keyedvectors.Vocab at 0x1837fa43550>,\n",
       " 'first_order': <gensim.models.keyedvectors.Vocab at 0x1837fa435b0>,\n",
       " 'meissner': <gensim.models.keyedvectors.Vocab at 0x1837fa43610>,\n",
       " 'long': <gensim.models.keyedvectors.Vocab at 0x1837fa43670>,\n",
       " 'scenario': <gensim.models.keyedvectors.Vocab at 0x1837fa436d0>,\n",
       " 'character': <gensim.models.keyedvectors.Vocab at 0x1837fa43730>,\n",
       " 'larger': <gensim.models.keyedvectors.Vocab at 0x1837fa43790>,\n",
       " 'peculiar': <gensim.models.keyedvectors.Vocab at 0x1837fa437f0>,\n",
       " 'branch': <gensim.models.keyedvectors.Vocab at 0x1837fa43850>,\n",
       " 'stripe': <gensim.models.keyedvectors.Vocab at 0x1837fa438b0>,\n",
       " 'typic': <gensim.models.keyedvectors.Vocab at 0x1837fa43910>,\n",
       " 'uniaxi': <gensim.models.keyedvectors.Vocab at 0x1837fa43970>,\n",
       " 'perpendicular': <gensim.models.keyedvectors.Vocab at 0x1837fa439d0>,\n",
       " 'anisotropi': <gensim.models.keyedvectors.Vocab at 0x1837fa43a30>,\n",
       " 'pma': <gensim.models.keyedvectors.Vocab at 0x1837fa43a90>,\n",
       " 'ware': <gensim.models.keyedvectors.Vocab at 0x1837fa43af0>,\n",
       " 'sm': <gensim.models.keyedvectors.Vocab at 0x1837fa43b50>,\n",
       " 'langl': <gensim.models.keyedvectors.Vocab at 0x1837fa43bb0>,\n",
       " 'rangl': <gensim.models.keyedvectors.Vocab at 0x1837fa43c10>,\n",
       " 'symmetri': <gensim.models.keyedvectors.Vocab at 0x1837fa43c70>,\n",
       " 'recent': <gensim.models.keyedvectors.Vocab at 0x1837fa43cd0>,\n",
       " 'expon': <gensim.models.keyedvectors.Vocab at 0x1837fa43d30>,\n",
       " 'matrix': <gensim.models.keyedvectors.Vocab at 0x1837fa43d90>,\n",
       " 'symmetr': <gensim.models.keyedvectors.Vocab at 0x1837fa43df0>,\n",
       " 'better_understand': <gensim.models.keyedvectors.Vocab at 0x1837fa43e50>,\n",
       " 'group': <gensim.models.keyedvectors.Vocab at 0x1837fa43eb0>,\n",
       " 'pdbi': <gensim.models.keyedvectors.Vocab at 0x1837fa43f10>,\n",
       " 'arcsecond': <gensim.models.keyedvectors.Vocab at 0x1837fa43f70>,\n",
       " 'survey': <gensim.models.keyedvectors.Vocab at 0x1837fa43fd0>,\n",
       " 'paw': <gensim.models.keyedvectors.Vocab at 0x1837fa44070>,\n",
       " 'spiral': <gensim.models.keyedvectors.Vocab at 0x1837fa440d0>,\n",
       " 'arm': <gensim.models.keyedvectors.Vocab at 0x1837fa44130>,\n",
       " 'cloud': <gensim.models.keyedvectors.Vocab at 0x1837fa44190>,\n",
       " 'star_format': <gensim.models.keyedvectors.Vocab at 0x1837fa441f0>,\n",
       " 'lead': <gensim.models.keyedvectors.Vocab at 0x1837fa44250>,\n",
       " 'bright': <gensim.models.keyedvectors.Vocab at 0x1837fa442b0>,\n",
       " 'star_form': <gensim.models.keyedvectors.Vocab at 0x1837fa44310>,\n",
       " 'site': <gensim.models.keyedvectors.Vocab at 0x1837fa44370>,\n",
       " 'along': <gensim.models.keyedvectors.Vocab at 0x1837fa443d0>,\n",
       " 'promin': <gensim.models.keyedvectors.Vocab at 0x1837fa44430>,\n",
       " 'remain': <gensim.models.keyedvectors.Vocab at 0x1837fa44490>,\n",
       " 'elus': <gensim.models.keyedvectors.Vocab at 0x1837fa444f0>,\n",
       " 'multi': <gensim.models.keyedvectors.Vocab at 0x1837fa44550>,\n",
       " 'nearbi': <gensim.models.keyedvectors.Vocab at 0x1837fa445b0>,\n",
       " 'grand': <gensim.models.keyedvectors.Vocab at 0x1837fa44610>,\n",
       " 'galaxi': <gensim.models.keyedvectors.Vocab at 0x1837fa44670>,\n",
       " 'belong': <gensim.models.keyedvectors.Vocab at 0x1837fa446d0>,\n",
       " 'wave': <gensim.models.keyedvectors.Vocab at 0x1837fa44730>,\n",
       " 'exhibit': <gensim.models.keyedvectors.Vocab at 0x1837fa44790>,\n",
       " 'nine': <gensim.models.keyedvectors.Vocab at 0x1837fa447f0>,\n",
       " 'gas': <gensim.models.keyedvectors.Vocab at 0x1837fa44850>,\n",
       " 'spur': <gensim.models.keyedvectors.Vocab at 0x1837fa448b0>,\n",
       " 'ioniz': <gensim.models.keyedvectors.Vocab at 0x1837fa44910>,\n",
       " 'atom': <gensim.models.keyedvectors.Vocab at 0x1837fa44970>,\n",
       " 'molecular': <gensim.models.keyedvectors.Vocab at 0x1837fa449d0>,\n",
       " 'dusti': <gensim.models.keyedvectors.Vocab at 0x1837fa44a30>,\n",
       " 'interstellar': <gensim.models.keyedvectors.Vocab at 0x1837fa44a90>,\n",
       " 'medium': <gensim.models.keyedvectors.Vocab at 0x1837fa44af0>,\n",
       " 'ism': <gensim.models.keyedvectors.Vocab at 0x1837fa44b50>,\n",
       " 'tracer': <gensim.models.keyedvectors.Vocab at 0x1837fa44bb0>,\n",
       " 'hii': <gensim.models.keyedvectors.Vocab at 0x1837fa44c10>,\n",
       " 'region': <gensim.models.keyedvectors.Vocab at 0x1837fa44c70>,\n",
       " 'young': <gensim.models.keyedvectors.Vocab at 0x1837fa44cd0>,\n",
       " 'myr': <gensim.models.keyedvectors.Vocab at 0x1837fa44d30>,\n",
       " 'stellar': <gensim.models.keyedvectors.Vocab at 0x1837fa44d90>,\n",
       " 'variat': <gensim.models.keyedvectors.Vocab at 0x1837fa44df0>,\n",
       " 'giant': <gensim.models.keyedvectors.Vocab at 0x1837fa44e50>,\n",
       " 'gmc': <gensim.models.keyedvectors.Vocab at 0x1837fa44eb0>,\n",
       " 'extinct': <gensim.models.keyedvectors.Vocab at 0x1837fa44f10>,\n",
       " 'aris': <gensim.models.keyedvectors.Vocab at 0x1837fa44f70>,\n",
       " 'ongo': <gensim.models.keyedvectors.Vocab at 0x1837fa44fd0>,\n",
       " 'despit': <gensim.models.keyedvectors.Vocab at 0x1837fa46070>,\n",
       " 'trend': <gensim.models.keyedvectors.Vocab at 0x1837fa460d0>,\n",
       " 'age': <gensim.models.keyedvectors.Vocab at 0x1837fa46130>,\n",
       " 'feedback': <gensim.models.keyedvectors.Vocab at 0x1837fa46190>,\n",
       " 'tentat': <gensim.models.keyedvectors.Vocab at 0x1837fa461f0>,\n",
       " 'trigger': <gensim.models.keyedvectors.Vocab at 0x1837fa46250>,\n",
       " 'entiti': <gensim.models.keyedvectors.Vocab at 0x1837fa462b0>,\n",
       " 'blend': <gensim.models.keyedvectors.Vocab at 0x1837fa46310>,\n",
       " 'lower': <gensim.models.keyedvectors.Vocab at 0x1837fa46370>,\n",
       " 'resolut': <gensim.models.keyedvectors.Vocab at 0x1837fa463d0>,\n",
       " 'conclud': <gensim.models.keyedvectors.Vocab at 0x1837fa46430>,\n",
       " 'coher': <gensim.models.keyedvectors.Vocab at 0x1837fa46490>,\n",
       " 'onset': <gensim.models.keyedvectors.Vocab at 0x1837fa464f0>,\n",
       " 'mechan': <gensim.models.keyedvectors.Vocab at 0x1837fa46550>,\n",
       " 'sole': <gensim.models.keyedvectors.Vocab at 0x1837fa465b0>,\n",
       " 'occur': <gensim.models.keyedvectors.Vocab at 0x1837fa46610>,\n",
       " 'proceed': <gensim.models.keyedvectors.Vocab at 0x1837fa46670>,\n",
       " 'sever': <gensim.models.keyedvectors.Vocab at 0x1837fa466d0>,\n",
       " 'million': <gensim.models.keyedvectors.Vocab at 0x1837fa46730>,\n",
       " 'year': <gensim.models.keyedvectors.Vocab at 0x1837fa46790>,\n",
       " 'impli': <gensim.models.keyedvectors.Vocab at 0x1837fa467f0>,\n",
       " 'act': <gensim.models.keyedvectors.Vocab at 0x1837fa46850>,\n",
       " 'sustain': <gensim.models.keyedvectors.Vocab at 0x1837fa468b0>,\n",
       " 'unstabl': <gensim.models.keyedvectors.Vocab at 0x1837fa46910>,\n",
       " 'adam': <gensim.models.keyedvectors.Vocab at 0x1837fa46970>,\n",
       " 'spectral': <gensim.models.keyedvectors.Vocab at 0x1837fa469d0>,\n",
       " 'sequenc': <gensim.models.keyedvectors.Vocab at 0x1837fa46a30>,\n",
       " 'variant': <gensim.models.keyedvectors.Vocab at 0x1837fa46a90>,\n",
       " 'free': <gensim.models.keyedvectors.Vocab at 0x1837fa46af0>,\n",
       " 'simplici': <gensim.models.keyedvectors.Vocab at 0x1837fa46b50>,\n",
       " 'h': <gensim.models.keyedvectors.Vocab at 0x1837fa46bb0>,\n",
       " 'mathbb_f': <gensim.models.keyedvectors.Vocab at 0x1837fa46c10>,\n",
       " 'mathbb_q': <gensim.models.keyedvectors.Vocab at 0x1837fa46c70>,\n",
       " 'filtrat': <gensim.models.keyedvectors.Vocab at 0x1837fa46cd0>,\n",
       " 'appropri': <gensim.models.keyedvectors.Vocab at 0x1837fa46d30>,\n",
       " 'cohomolog': <gensim.models.keyedvectors.Vocab at 0x1837fa46d90>,\n",
       " 'covari': <gensim.models.keyedvectors.Vocab at 0x1837fa46df0>,\n",
       " 'priorit': <gensim.models.keyedvectors.Vocab at 0x1837fa46e50>,\n",
       " 'via': <gensim.models.keyedvectors.Vocab at 0x1837fa46eb0>,\n",
       " 'match': <gensim.models.keyedvectors.Vocab at 0x1837fa46f10>,\n",
       " 'causal': <gensim.models.keyedvectors.Vocab at 0x1837fa46f70>,\n",
       " 'five': <gensim.models.keyedvectors.Vocab at 0x1837fa46fd0>,\n",
       " 'empir': <gensim.models.keyedvectors.Vocab at 0x1837fa49070>,\n",
       " 'seek': <gensim.models.keyedvectors.Vocab at 0x1837fa490d0>,\n",
       " 'assum': <gensim.models.keyedvectors.Vocab at 0x1837fa49130>,\n",
       " 'treatment': <gensim.models.keyedvectors.Vocab at 0x1837fa49190>,\n",
       " 'identif': <gensim.models.keyedvectors.Vocab at 0x1837fa491f0>,\n",
       " 'analyst': <gensim.models.keyedvectors.Vocab at 0x1837fa49250>,\n",
       " 'must': <gensim.models.keyedvectors.Vocab at 0x1837fa492b0>,\n",
       " 'adjust': <gensim.models.keyedvectors.Vocab at 0x1837fa49310>,\n",
       " 'confound': <gensim.models.keyedvectors.Vocab at 0x1837fa49370>,\n",
       " 'basic': <gensim.models.keyedvectors.Vocab at 0x1837fa493d0>,\n",
       " 'regress': <gensim.models.keyedvectors.Vocab at 0x1837fa49430>,\n",
       " 'robust': <gensim.models.keyedvectors.Vocab at 0x1837fa49490>,\n",
       " 'weight': <gensim.models.keyedvectors.Vocab at 0x1837fa494f0>,\n",
       " 'becom': <gensim.models.keyedvectors.Vocab at 0x1837fa49550>,\n",
       " 'late': <gensim.models.keyedvectors.Vocab at 0x1837fa495b0>,\n",
       " 'even': <gensim.models.keyedvectors.Vocab at 0x1837fa49610>,\n",
       " 'flexibl': <gensim.models.keyedvectors.Vocab at 0x1837fa49670>,\n",
       " 'black_box': <gensim.models.keyedvectors.Vocab at 0x1837fa496d0>,\n",
       " 'littl': <gensim.models.keyedvectors.Vocab at 0x1837fa49730>,\n",
       " 'input': <gensim.models.keyedvectors.Vocab at 0x1837fa49790>,\n",
       " 'competit': <gensim.models.keyedvectors.Vocab at 0x1837fa497f0>,\n",
       " 'substant': <gensim.models.keyedvectors.Vocab at 0x1837fa49850>,\n",
       " 'contrast': <gensim.models.keyedvectors.Vocab at 0x1837fa498b0>,\n",
       " 'black': <gensim.models.keyedvectors.Vocab at 0x1837fa49910>,\n",
       " 'replic': <gensim.models.keyedvectors.Vocab at 0x1837fa49970>,\n",
       " 'custom': <gensim.models.keyedvectors.Vocab at 0x1837fa499d0>,\n",
       " 'respons': <gensim.models.keyedvectors.Vocab at 0x1837fa49a30>,\n",
       " 'expertis': <gensim.models.keyedvectors.Vocab at 0x1837fa49a90>,\n",
       " 'across': <gensim.models.keyedvectors.Vocab at 0x1837fa49af0>,\n",
       " 'advic': <gensim.models.keyedvectors.Vocab at 0x1837fa49b50>,\n",
       " 'acoust': <gensim.models.keyedvectors.Vocab at 0x1837fa49bb0>,\n",
       " 'imped': <gensim.models.keyedvectors.Vocab at 0x1837fa49c10>,\n",
       " 'invers': <gensim.models.keyedvectors.Vocab at 0x1837fa49c70>,\n",
       " 'helmholtz': <gensim.models.keyedvectors.Vocab at 0x1837fa49cd0>,\n",
       " 'assign': <gensim.models.keyedvectors.Vocab at 0x1837fa49d30>,\n",
       " 'homogen': <gensim.models.keyedvectors.Vocab at 0x1837fa49d90>,\n",
       " 'boundari_condit': <gensim.models.keyedvectors.Vocab at 0x1837fa49df0>,\n",
       " 'navier_stoke': <gensim.models.keyedvectors.Vocab at 0x1837fa49e50>,\n",
       " 'solver': <gensim.models.keyedvectors.Vocab at 0x1837fa49eb0>,\n",
       " 'output': <gensim.models.keyedvectors.Vocab at 0x1837fa49f10>,\n",
       " 'eigenfunct': <gensim.models.keyedvectors.Vocab at 0x1837fa49f70>,\n",
       " 'ih': <gensim.models.keyedvectors.Vocab at 0x1837fa49fd0>,\n",
       " 'revers': <gensim.models.keyedvectors.Vocab at 0x1837fa4a070>,\n",
       " 'procedur': <gensim.models.keyedvectors.Vocab at 0x1837fa4a0d0>,\n",
       " 'return': <gensim.models.keyedvectors.Vocab at 0x1837fa4a130>,\n",
       " 'unknown': <gensim.models.keyedvectors.Vocab at 0x1837fa4a190>,\n",
       " 'boundari': <gensim.models.keyedvectors.Vocab at 0x1837fa4a1f0>,\n",
       " 'ib': <gensim.models.keyedvectors.Vocab at 0x1837fa4a250>,\n",
       " 'real': <gensim.models.keyedvectors.Vocab at 0x1837fa4a2b0>,\n",
       " 'second_order': <gensim.models.keyedvectors.Vocab at 0x1837fa4a310>,\n",
       " 'unstructur': <gensim.models.keyedvectors.Vocab at 0x1837fa4a370>,\n",
       " 'stagger': <gensim.models.keyedvectors.Vocab at 0x1837fa4a3d0>,\n",
       " 'arrang': <gensim.models.keyedvectors.Vocab at 0x1837fa4a430>,\n",
       " 'momentum': <gensim.models.keyedvectors.Vocab at 0x1837fa4a490>,\n",
       " 'extend': <gensim.models.keyedvectors.Vocab at 0x1837fa4a4f0>,\n",
       " 'center': <gensim.models.keyedvectors.Vocab at 0x1837fa4a550>,\n",
       " 'face': <gensim.models.keyedvectors.Vocab at 0x1837fa4a5b0>,\n",
       " 'compon': <gensim.models.keyedvectors.Vocab at 0x1837fa4a610>,\n",
       " 'co': <gensim.models.keyedvectors.Vocab at 0x1837fa4a670>,\n",
       " 'treat': <gensim.models.keyedvectors.Vocab at 0x1837fa4a6d0>,\n",
       " 'closur': <gensim.models.keyedvectors.Vocab at 0x1837fa4a730>,\n",
       " 'gradient': <gensim.models.keyedvectors.Vocab at 0x1837fa4a790>,\n",
       " 'waveform': <gensim.models.keyedvectors.Vocab at 0x1837fa4a7f0>,\n",
       " 'carri': <gensim.models.keyedvectors.Vocab at 0x1837fa4a850>,\n",
       " 'independ': <gensim.models.keyedvectors.Vocab at 0x1837fa4a8b0>,\n",
       " 'complet': <gensim.models.keyedvectors.Vocab at 0x1837fa4a910>,\n",
       " 'broadband': <gensim.models.keyedvectors.Vocab at 0x1837fa4a970>,\n",
       " 'desir': <gensim.models.keyedvectors.Vocab at 0x1837fa4a9d0>,\n",
       " 'rang': <gensim.models.keyedvectors.Vocab at 0x1837fa4aa30>,\n",
       " 'valid': <gensim.models.keyedvectors.Vocab at 0x1837fa4aa90>,\n",
       " 'inviscid': <gensim.models.keyedvectors.Vocab at 0x1837fa4aaf0>,\n",
       " 'viscous': <gensim.models.keyedvectors.Vocab at 0x1837fa4ab50>,\n",
       " 'rectangular': <gensim.models.keyedvectors.Vocab at 0x1837fa4abb0>,\n",
       " 'duct': <gensim.models.keyedvectors.Vocab at 0x1837fa4ac10>,\n",
       " 'geometr': <gensim.models.keyedvectors.Vocab at 0x1837fa4ac70>,\n",
       " 'toy': <gensim.models.keyedvectors.Vocab at 0x1837fa4acd0>,\n",
       " 'caviti': <gensim.models.keyedvectors.Vocab at 0x1837fa4ad30>,\n",
       " 'verifi': <gensim.models.keyedvectors.Vocab at 0x1837fa4ad90>,\n",
       " 'companion': <gensim.models.keyedvectors.Vocab at 0x1837fa4adf0>,\n",
       " 'full': <gensim.models.keyedvectors.Vocab at 0x1837fa4ae50>,\n",
       " 'compress': <gensim.models.keyedvectors.Vocab at 0x1837fa4aeb0>,\n",
       " 'geometri': <gensim.models.keyedvectors.Vocab at 0x1837fa4af10>,\n",
       " 'one_dimension': <gensim.models.keyedvectors.Vocab at 0x1837fa4af70>,\n",
       " 'test': <gensim.models.keyedvectors.Vocab at 0x1837fa4afd0>,\n",
       " 'tube': <gensim.models.keyedvectors.Vocab at 0x1837fa4c070>,\n",
       " 'methodolog': <gensim.models.keyedvectors.Vocab at 0x1837fa4c0d0>,\n",
       " 'shown': <gensim.models.keyedvectors.Vocab at 0x1837fa4c130>,\n",
       " 'captur': <gensim.models.keyedvectors.Vocab at 0x1837fa4c190>,\n",
       " 'quantit': <gensim.models.keyedvectors.Vocab at 0x1837fa4c1f0>,\n",
       " 'growth_rate': <gensim.models.keyedvectors.Vocab at 0x1837fa4c250>,\n",
       " 'deciph': <gensim.models.keyedvectors.Vocab at 0x1837fa4c2b0>,\n",
       " 'amplif': <gensim.models.keyedvectors.Vocab at 0x1837fa4c310>,\n",
       " 'reduct': <gensim.models.keyedvectors.Vocab at 0x1837fa4c370>,\n",
       " 'open': <gensim.models.keyedvectors.Vocab at 0x1837fa4c3d0>,\n",
       " 'chemic': <gensim.models.keyedvectors.Vocab at 0x1837fa4c430>,\n",
       " 'reaction': <gensim.models.keyedvectors.Vocab at 0x1837fa4c490>,\n",
       " 'random': <gensim.models.keyedvectors.Vocab at 0x1837fa4c4f0>,\n",
       " 'fluctuat': <gensim.models.keyedvectors.Vocab at 0x1837fa4c550>,\n",
       " 'biolog': <gensim.models.keyedvectors.Vocab at 0x1837fa4c5b0>,\n",
       " 'longstand': <gensim.models.keyedvectors.Vocab at 0x1837fa4c610>,\n",
       " 'issu': <gensim.models.keyedvectors.Vocab at 0x1837fa4c670>,\n",
       " 'understand': <gensim.models.keyedvectors.Vocab at 0x1837fa4c6d0>,\n",
       " 'shed_light': <gensim.models.keyedvectors.Vocab at 0x1837fa4c730>,\n",
       " 'impos': <gensim.models.keyedvectors.Vocab at 0x1837fa4c790>,\n",
       " 'intrins': <gensim.models.keyedvectors.Vocab at 0x1837fa4c7f0>,\n",
       " 'ration': <gensim.models.keyedvectors.Vocab at 0x1837fa4c850>,\n",
       " 'differenti_equat': <gensim.models.keyedvectors.Vocab at 0x1837fa4c8b0>,\n",
       " 'formal': <gensim.models.keyedvectors.Vocab at 0x1837fa4c910>,\n",
       " 'contact': <gensim.models.keyedvectors.Vocab at 0x1837fa4c970>,\n",
       " 'action': <gensim.models.keyedvectors.Vocab at 0x1837fa4c9d0>,\n",
       " 'kinet': <gensim.models.keyedvectors.Vocab at 0x1837fa4ca30>,\n",
       " 'repres': <gensim.models.keyedvectors.Vocab at 0x1837fa4ca90>,\n",
       " 'biomolecular': <gensim.models.keyedvectors.Vocab at 0x1837fa4caf0>,\n",
       " 'breakag': <gensim.models.keyedvectors.Vocab at 0x1837fa4cb50>,\n",
       " 'cascad': <gensim.models.keyedvectors.Vocab at 0x1837fa4cbb0>,\n",
       " 'metabol': <gensim.models.keyedvectors.Vocab at 0x1837fa4cc10>,\n",
       " 'zero': <gensim.models.keyedvectors.Vocab at 0x1837fa4cc70>,\n",
       " 'defici': <gensim.models.keyedvectors.Vocab at 0x1837fa4ccd0>,\n",
       " 'admit': <gensim.models.keyedvectors.Vocab at 0x1837fa4cd30>,\n",
       " 'detail': <gensim.models.keyedvectors.Vocab at 0x1837fa4cd90>,\n",
       " 'balanc': <gensim.models.keyedvectors.Vocab at 0x1837fa4cdf0>,\n",
       " 'steadi_state': <gensim.models.keyedvectors.Vocab at 0x1837fa4ce50>,\n",
       " 'uncorrel': <gensim.models.keyedvectors.Vocab at 0x1837fa4ceb0>,\n",
       " 'number': <gensim.models.keyedvectors.Vocab at 0x1837fa4cf10>,\n",
       " 'molecul': <gensim.models.keyedvectors.Vocab at 0x1837fa4cf70>,\n",
       " 'follow': <gensim.models.keyedvectors.Vocab at 0x1837fa4cfd0>,\n",
       " 'fano': <gensim.models.keyedvectors.Vocab at 0x1837fa4f070>,\n",
       " 'equal': <gensim.models.keyedvectors.Vocab at 0x1837fa4f0d0>,\n",
       " 'unbalanc': <gensim.models.keyedvectors.Vocab at 0x1837fa4f130>,\n",
       " 'non_equilibrium': <gensim.models.keyedvectors.Vocab at 0x1837fa4f190>,\n",
       " 'non_zero': <gensim.models.keyedvectors.Vocab at 0x1837fa4f1f0>,\n",
       " 'flux': <gensim.models.keyedvectors.Vocab at 0x1837fa4f250>,\n",
       " 'defin': <gensim.models.keyedvectors.Vocab at 0x1837fa4f2b0>,\n",
       " 'multipli': <gensim.models.keyedvectors.Vocab at 0x1837fa4f310>,\n",
       " 'adequ': <gensim.models.keyedvectors.Vocab at 0x1837fa4f370>,\n",
       " 'stoichiometr': <gensim.models.keyedvectors.Vocab at 0x1837fa4f3d0>,\n",
       " 'coeffici': <gensim.models.keyedvectors.Vocab at 0x1837fa4f430>,\n",
       " 'lowest': <gensim.models.keyedvectors.Vocab at 0x1837fa4f490>,\n",
       " 'highest': <gensim.models.keyedvectors.Vocab at 0x1837fa4f4f0>,\n",
       " 'amplifi': <gensim.models.keyedvectors.Vocab at 0x1837fa4f550>,\n",
       " 'goe': <gensim.models.keyedvectors.Vocab at 0x1837fa4f5b0>,\n",
       " 'opposit': <gensim.models.keyedvectors.Vocab at 0x1837fa4f610>,\n",
       " 'possess': <gensim.models.keyedvectors.Vocab at 0x1837fa4f670>,\n",
       " 'vanish': <gensim.models.keyedvectors.Vocab at 0x1837fa4f6d0>,\n",
       " 'conjectur': <gensim.models.keyedvectors.Vocab at 0x1837fa4f730>,\n",
       " 'hold': <gensim.models.keyedvectors.Vocab at 0x1837fa4f790>,\n",
       " 'mani_bodi': <gensim.models.keyedvectors.Vocab at 0x1837fa4f7f0>,\n",
       " 'stabil': <gensim.models.keyedvectors.Vocab at 0x1837fa4f850>,\n",
       " 'instabl': <gensim.models.keyedvectors.Vocab at 0x1837fa4f8b0>,\n",
       " 'disord': <gensim.models.keyedvectors.Vocab at 0x1837fa4f910>,\n",
       " 'griffith': <gensim.models.keyedvectors.Vocab at 0x1837fa4f970>,\n",
       " 'spoil': <gensim.models.keyedvectors.Vocab at 0x1837fa4f9d0>,\n",
       " 'perturb': <gensim.models.keyedvectors.Vocab at 0x1837fa4fa30>,\n",
       " 'motion': <gensim.models.keyedvectors.Vocab at 0x1837fa4fa90>,\n",
       " 'spin_chain': <gensim.models.keyedvectors.Vocab at 0x1837fa4faf0>,\n",
       " 'dimens': <gensim.models.keyedvectors.Vocab at 0x1837fa4fb50>,\n",
       " 'reason': <gensim.models.keyedvectors.Vocab at 0x1837fa4fbb0>,\n",
       " 'idea': <gensim.models.keyedvectors.Vocab at 0x1837fa4fc10>,\n",
       " 'situat': <gensim.models.keyedvectors.Vocab at 0x1837fa4fc70>,\n",
       " 'ensur': <gensim.models.keyedvectors.Vocab at 0x1837fa4fcd0>,\n",
       " 'involv': <gensim.models.keyedvectors.Vocab at 0x1837fa4fd30>,\n",
       " 'much_smaller': <gensim.models.keyedvectors.Vocab at 0x1837fa4fd90>,\n",
       " 'argu': <gensim.models.keyedvectors.Vocab at 0x1837fa4fdf0>,\n",
       " 'ergod': <gensim.models.keyedvectors.Vocab at 0x1837fa4fe50>,\n",
       " 'restor': <gensim.models.keyedvectors.Vocab at 0x1837fa4feb0>,\n",
       " 'although': <gensim.models.keyedvectors.Vocab at 0x1837fa4ff10>,\n",
       " 'equilibr': <gensim.models.keyedvectors.Vocab at 0x1837fa4ff70>,\n",
       " 'extrem': <gensim.models.keyedvectors.Vocab at 0x1837fa4ffd0>,\n",
       " 'slow': <gensim.models.keyedvectors.Vocab at 0x1837fa50070>,\n",
       " 'glass': <gensim.models.keyedvectors.Vocab at 0x1837fa500d0>,\n",
       " 'fault': <gensim.models.keyedvectors.Vocab at 0x1837fa50130>,\n",
       " 'user': <gensim.models.keyedvectors.Vocab at 0x1837fa50190>,\n",
       " 'guid': <gensim.models.keyedvectors.Vocab at 0x1837fa501f0>,\n",
       " 'collect': <gensim.models.keyedvectors.Vocab at 0x1837fa50250>,\n",
       " 'matlab': <gensim.models.keyedvectors.Vocab at 0x1837fa502b0>,\n",
       " 'implement': <gensim.models.keyedvectors.Vocab at 0x1837fa50310>,\n",
       " 'comput': <gensim.models.keyedvectors.Vocab at 0x1837fa50370>,\n",
       " 'chapter': <gensim.models.keyedvectors.Vocab at 0x1837fa503d0>,\n",
       " 'book': <gensim.models.keyedvectors.Vocab at 0x1837fa50430>,\n",
       " 'solv': <gensim.models.keyedvectors.Vocab at 0x1837fa50490>,\n",
       " 'diagnosi': <gensim.models.keyedvectors.Vocab at 0x1837fa504f0>,\n",
       " 'synthesi': <gensim.models.keyedvectors.Vocab at 0x1837fa50550>,\n",
       " 'springer': <gensim.models.keyedvectors.Vocab at 0x1837fa505b0>,\n",
       " 'document': <gensim.models.keyedvectors.Vocab at 0x1837fa50610>,\n",
       " 'version': <gensim.models.keyedvectors.Vocab at 0x1837fa50670>,\n",
       " 'background': <gensim.models.keyedvectors.Vocab at 0x1837fa506d0>,\n",
       " 'exact': <gensim.models.keyedvectors.Vocab at 0x1837fa50730>,\n",
       " 'filter': <gensim.models.keyedvectors.Vocab at 0x1837fa50790>,\n",
       " 'give': <gensim.models.keyedvectors.Vocab at 0x1837fa507f0>,\n",
       " 'depth': <gensim.models.keyedvectors.Vocab at 0x1837fa50850>,\n",
       " 'command': <gensim.models.keyedvectors.Vocab at 0x1837fa508b0>,\n",
       " 'syntax': <gensim.models.keyedvectors.Vocab at 0x1837fa50910>,\n",
       " 'illustr': <gensim.models.keyedvectors.Vocab at 0x1837fa50970>,\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "word_vectors.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "    \n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "    \n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(wv,text_list):\n",
    "    return np.vstack([word_averaging(wv,post) for post in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15729, 300)\n",
      "(5243, 300)\n"
     ]
    }
   ],
   "source": [
    "# Split the train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df['Computer Science'], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "\n",
    "train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "\n",
    "xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "\n",
    "print(xtrain_wa.shape)\n",
    "print(xtest_wa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.014618</td>\n",
       "      <td>0.047860</td>\n",
       "      <td>-0.050113</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>-0.022383</td>\n",
       "      <td>-0.056061</td>\n",
       "      <td>0.060153</td>\n",
       "      <td>0.009776</td>\n",
       "      <td>-0.049013</td>\n",
       "      <td>-0.013292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057321</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>-0.100632</td>\n",
       "      <td>0.191991</td>\n",
       "      <td>0.088021</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.043819</td>\n",
       "      <td>-0.008516</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.093756</td>\n",
       "      <td>0.049626</td>\n",
       "      <td>0.102375</td>\n",
       "      <td>-0.073823</td>\n",
       "      <td>0.055747</td>\n",
       "      <td>-0.068086</td>\n",
       "      <td>0.120122</td>\n",
       "      <td>-0.029481</td>\n",
       "      <td>-0.052809</td>\n",
       "      <td>0.026032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018846</td>\n",
       "      <td>-0.027491</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>-0.079491</td>\n",
       "      <td>0.073759</td>\n",
       "      <td>-0.014351</td>\n",
       "      <td>-0.050653</td>\n",
       "      <td>0.058809</td>\n",
       "      <td>-0.063871</td>\n",
       "      <td>0.061981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.083751</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>0.015794</td>\n",
       "      <td>0.018384</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>-0.058922</td>\n",
       "      <td>0.085349</td>\n",
       "      <td>-0.016647</td>\n",
       "      <td>-0.007047</td>\n",
       "      <td>-0.008458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030261</td>\n",
       "      <td>-0.035627</td>\n",
       "      <td>0.103987</td>\n",
       "      <td>-0.044708</td>\n",
       "      <td>-0.001222</td>\n",
       "      <td>0.064943</td>\n",
       "      <td>0.015229</td>\n",
       "      <td>0.100035</td>\n",
       "      <td>-0.011215</td>\n",
       "      <td>-0.054788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.071317</td>\n",
       "      <td>0.023466</td>\n",
       "      <td>-0.077676</td>\n",
       "      <td>-0.103610</td>\n",
       "      <td>-0.030570</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>0.088635</td>\n",
       "      <td>0.059473</td>\n",
       "      <td>0.046658</td>\n",
       "      <td>0.006653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006172</td>\n",
       "      <td>-0.074985</td>\n",
       "      <td>0.043026</td>\n",
       "      <td>-0.033050</td>\n",
       "      <td>0.102773</td>\n",
       "      <td>0.011975</td>\n",
       "      <td>0.007889</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>0.018075</td>\n",
       "      <td>0.006074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.061280</td>\n",
       "      <td>-0.052777</td>\n",
       "      <td>-0.133619</td>\n",
       "      <td>-0.077958</td>\n",
       "      <td>-0.015337</td>\n",
       "      <td>-0.033525</td>\n",
       "      <td>0.060325</td>\n",
       "      <td>-0.001626</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015439</td>\n",
       "      <td>-0.139653</td>\n",
       "      <td>0.073742</td>\n",
       "      <td>-0.075676</td>\n",
       "      <td>0.053698</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>-0.096018</td>\n",
       "      <td>0.066364</td>\n",
       "      <td>0.024741</td>\n",
       "      <td>-0.100761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15724</th>\n",
       "      <td>-0.097838</td>\n",
       "      <td>0.059032</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>-0.004988</td>\n",
       "      <td>0.050463</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.051192</td>\n",
       "      <td>0.044422</td>\n",
       "      <td>-0.035786</td>\n",
       "      <td>0.027019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038188</td>\n",
       "      <td>-0.035572</td>\n",
       "      <td>0.108180</td>\n",
       "      <td>-0.086528</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.057002</td>\n",
       "      <td>-0.063360</td>\n",
       "      <td>0.083206</td>\n",
       "      <td>-0.057793</td>\n",
       "      <td>0.007262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.019478</td>\n",
       "      <td>-0.038963</td>\n",
       "      <td>-0.098868</td>\n",
       "      <td>-0.026077</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.029416</td>\n",
       "      <td>0.083121</td>\n",
       "      <td>0.009542</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070968</td>\n",
       "      <td>0.009757</td>\n",
       "      <td>0.085073</td>\n",
       "      <td>-0.098240</td>\n",
       "      <td>0.069621</td>\n",
       "      <td>0.106050</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>-0.022522</td>\n",
       "      <td>0.100848</td>\n",
       "      <td>0.051783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15726</th>\n",
       "      <td>0.051299</td>\n",
       "      <td>0.065459</td>\n",
       "      <td>-0.030969</td>\n",
       "      <td>0.024133</td>\n",
       "      <td>-0.029633</td>\n",
       "      <td>0.037998</td>\n",
       "      <td>0.008332</td>\n",
       "      <td>0.053965</td>\n",
       "      <td>0.058602</td>\n",
       "      <td>-0.034442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097818</td>\n",
       "      <td>-0.038238</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>-0.022193</td>\n",
       "      <td>0.063535</td>\n",
       "      <td>-0.019130</td>\n",
       "      <td>-0.133386</td>\n",
       "      <td>-0.070269</td>\n",
       "      <td>-0.039086</td>\n",
       "      <td>0.033931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15727</th>\n",
       "      <td>-0.075527</td>\n",
       "      <td>-0.062555</td>\n",
       "      <td>0.025544</td>\n",
       "      <td>0.056513</td>\n",
       "      <td>0.064236</td>\n",
       "      <td>-0.022811</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>-0.016022</td>\n",
       "      <td>-0.040015</td>\n",
       "      <td>0.008870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041931</td>\n",
       "      <td>-0.153296</td>\n",
       "      <td>0.106969</td>\n",
       "      <td>-0.066198</td>\n",
       "      <td>0.046234</td>\n",
       "      <td>-0.033109</td>\n",
       "      <td>-0.028210</td>\n",
       "      <td>0.049250</td>\n",
       "      <td>-0.059483</td>\n",
       "      <td>-0.021686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15728</th>\n",
       "      <td>-0.112160</td>\n",
       "      <td>-0.008228</td>\n",
       "      <td>-0.051851</td>\n",
       "      <td>-0.004379</td>\n",
       "      <td>-0.027771</td>\n",
       "      <td>-0.070957</td>\n",
       "      <td>0.101904</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>-0.058641</td>\n",
       "      <td>-0.031210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042213</td>\n",
       "      <td>-0.099781</td>\n",
       "      <td>0.100270</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>0.124435</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>-0.076789</td>\n",
       "      <td>0.065980</td>\n",
       "      <td>-0.058803</td>\n",
       "      <td>-0.021101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15729 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.014618  0.047860 -0.050113  0.008336 -0.022383 -0.056061  0.060153   \n",
       "1     -0.093756  0.049626  0.102375 -0.073823  0.055747 -0.068086  0.120122   \n",
       "2     -0.083751  0.011087  0.015794  0.018384  0.000674 -0.058922  0.085349   \n",
       "3     -0.071317  0.023466 -0.077676 -0.103610 -0.030570  0.008958  0.088635   \n",
       "4     -0.061280 -0.052777 -0.133619 -0.077958 -0.015337 -0.033525  0.060325   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "15724 -0.097838  0.059032 -0.002600 -0.004988  0.050463  0.001966  0.051192   \n",
       "15725  0.009576  0.019478 -0.038963 -0.098868 -0.026077  0.008163  0.029416   \n",
       "15726  0.051299  0.065459 -0.030969  0.024133 -0.029633  0.037998  0.008332   \n",
       "15727 -0.075527 -0.062555  0.025544  0.056513  0.064236 -0.022811  0.008730   \n",
       "15728 -0.112160 -0.008228 -0.051851 -0.004379 -0.027771 -0.070957  0.101904   \n",
       "\n",
       "            7         8         9    ...       290       291       292  \\\n",
       "0      0.009776 -0.049013 -0.013292  ...  0.057321  0.008039  0.082902   \n",
       "1     -0.029481 -0.052809  0.026032  ... -0.018846 -0.027491  0.005716   \n",
       "2     -0.016647 -0.007047 -0.008458  ... -0.030261 -0.035627  0.103987   \n",
       "3      0.059473  0.046658  0.006653  ... -0.006172 -0.074985  0.043026   \n",
       "4     -0.001626  0.004828  0.000015  ... -0.015439 -0.139653  0.073742   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "15724  0.044422 -0.035786  0.027019  ... -0.038188 -0.035572  0.108180   \n",
       "15725  0.083121  0.009542  0.024447  ...  0.070968  0.009757  0.085073   \n",
       "15726  0.053965  0.058602 -0.034442  ...  0.097818 -0.038238  0.026172   \n",
       "15727 -0.016022 -0.040015  0.008870  ... -0.041931 -0.153296  0.106969   \n",
       "15728  0.018663 -0.058641 -0.031210  ... -0.042213 -0.099781  0.100270   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "0     -0.100632  0.191991  0.088021  0.002853  0.043819 -0.008516  0.000537  \n",
       "1     -0.079491  0.073759 -0.014351 -0.050653  0.058809 -0.063871  0.061981  \n",
       "2     -0.044708 -0.001222  0.064943  0.015229  0.100035 -0.011215 -0.054788  \n",
       "3     -0.033050  0.102773  0.011975  0.007889  0.062184  0.018075  0.006074  \n",
       "4     -0.075676  0.053698  0.021478 -0.096018  0.066364  0.024741 -0.100761  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "15724 -0.086528  0.004711  0.057002 -0.063360  0.083206 -0.057793  0.007262  \n",
       "15725 -0.098240  0.069621  0.106050  0.007733 -0.022522  0.100848  0.051783  \n",
       "15726 -0.022193  0.063535 -0.019130 -0.133386 -0.070269 -0.039086  0.033931  \n",
       "15727 -0.066198  0.046234 -0.033109 -0.028210  0.049250 -0.059483 -0.021686  \n",
       "15728  0.010561  0.124435  0.003198 -0.076789  0.065980 -0.058803 -0.021101  \n",
       "\n",
       "[15729 rows x 300 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(xtrain_wa.shape[1])\n",
    "xtrain_wa_df = pd.DataFrame(xtrain_wa)\n",
    "display(xtrain_wa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86300063 0.86141133 0.86904007 0.86363637 0.86263913]\n",
      "Baseline: 86.39455080%  (0.26484564%)\n"
     ]
    }
   ],
   "source": [
    "# using SVM\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "n_cols = xtrain_wa_df.shape[1]\n",
    "n_rows = xtrain_wa_df.shape[0]\n",
    "\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150,activation = 'relu',input_dim = n_cols))\n",
    "    model.add(Dense(100,activation = 'relu'))\n",
    "    model.add(Dense(50,activation = 'relu'))\n",
    "    model.add(Dense(10,activation = 'relu'))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model\n",
    "estimator = KerasClassifier(build_fn = create_baseline, epochs = 10, batch_size = 5, verbose = 0)\n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "results = cross_val_score(estimator, xtrain_wa_df, ytrain, cv = kfold)\n",
    "print(results)\n",
    "print(\"Baseline: %.8f%%  (%.8f%%)\" % (results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting on first dataset\n",
    "estimator = KerasClassifier(build_fn = create_baseline, epochs = 10, batch_size = 5, verbose = 0)\n",
    "estimator.fit(xtrain_wa,ytrain)\n",
    "y_pred = estimator.predict(xtest_wa)\n",
    "display(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.8603852756055693\n",
      "f1_score: 0.8603852756055692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88      3127\n",
      "           1       0.79      0.89      0.84      2116\n",
      "\n",
      "    accuracy                           0.86      5243\n",
      "   macro avg       0.85      0.86      0.86      5243\n",
      "weighted avg       0.87      0.86      0.86      5243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluating f1-score\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "print(\"accuracy_score:\", accuracy_score(ytest,y_pred))\n",
    "print(\"f1_score:\", f1_score(ytest,y_pred,average = 'micro'))\n",
    "print(classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8989, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing on the final test set\n",
    "x_test_2 = df_test_2['title_abstract_combined_processed']\n",
    "test_2_tokenized = x_test_2.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "\n",
    "xtest_2_wa = word_averaging_list(word_vectors,test_2_tokenized)\n",
    "\n",
    "print(xtest_2_wa.shape)\n",
    "y_pred_2 = estimator.predict(xtest_2_wa)\n",
    "display(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Computer Science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8984</th>\n",
       "      <td>29957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8985</th>\n",
       "      <td>29958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8986</th>\n",
       "      <td>29959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8987</th>\n",
       "      <td>29960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8988</th>\n",
       "      <td>29961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8989 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Computer Science\n",
       "0     20973                 0\n",
       "1     20974                 0\n",
       "2     20975                 1\n",
       "3     20976                 0\n",
       "4     20977                 1\n",
       "...     ...               ...\n",
       "8984  29957                 1\n",
       "8985  29958                 1\n",
       "8986  29959                 1\n",
       "8987  29960                 0\n",
       "8988  29961                 1\n",
       "\n",
       "[8989 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adding the results in the final dataframe\n",
    "df_test_fs = pd.read_csv('test.csv')\n",
    "df_test_fs = df_test_fs.drop(labels = ['TITLE','ABSTRACT'],axis = 1)\n",
    "y_pred_list = np.array(y_pred_2).tolist()\n",
    "df_pred = pd.DataFrame(y_pred_list)\n",
    "df_test_fs['Computer Science'] = df_pred\n",
    "display(df_test_fs)\n",
    "#df_test_fs.to_csv('submission_u_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92053401 0.90718371 0.9287985  0.93356645 0.93195546]\n",
      "Baseline: 92.44076252%  (0.97139264%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90495867 0.89860141 0.89796567 0.89129055 0.90206677]\n",
      "Baseline: 89.89766121%  (0.45950499%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88111889 0.88016528 0.86999363 0.88334394 0.8845787 ]\n",
      "Baseline: 87.98400879%  (0.51657588%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9780674  0.97520661 0.974253   0.9726637  0.97519875]\n",
      "Baseline: 97.50778913%  (0.17593800%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98951048 0.99237126 0.99078196 0.99141765 0.99077904]\n",
      "Baseline: 99.09720778%  (0.09344442%)\n"
     ]
    }
   ],
   "source": [
    "# Looping through other labels\n",
    "for i in range(len(tagnames)-1):\n",
    "    xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df[tagnames[i+1]], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "    \n",
    "    train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    \n",
    "    xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "    xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "    # evaluate model\n",
    "    estimator = KerasClassifier(build_fn = create_baseline, epochs = 10, batch_size = 5, verbose = 0)\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "    results = cross_val_score(estimator, xtrain_wa_df, ytrain, cv = kfold)\n",
    "    print(results)\n",
    "    print(\"Baseline: %.8f%%  (%.8f%%)\" % (results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9233263398817471\n",
      "f1_score: 0.9233263398817471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      3716\n",
      "           1       0.86      0.88      0.87      1527\n",
      "\n",
      "    accuracy                           0.92      5243\n",
      "   macro avg       0.90      0.91      0.91      5243\n",
      "weighted avg       0.92      0.92      0.92      5243\n",
      "\n",
      "(8989, 300)\n",
      "accuracy_score: 0.9029181766164409\n",
      "f1_score: 0.9029181766164409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      3815\n",
      "           1       0.82      0.82      0.82      1428\n",
      "\n",
      "    accuracy                           0.90      5243\n",
      "   macro avg       0.88      0.88      0.88      5243\n",
      "weighted avg       0.90      0.90      0.90      5243\n",
      "\n",
      "(8989, 300)\n",
      "accuracy_score: 0.8779324814037764\n",
      "f1_score: 0.8779324814037764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      3937\n",
      "           1       0.74      0.79      0.76      1306\n",
      "\n",
      "    accuracy                           0.88      5243\n",
      "   macro avg       0.83      0.85      0.84      5243\n",
      "weighted avg       0.88      0.88      0.88      5243\n",
      "\n",
      "(8989, 300)\n",
      "accuracy_score: 0.9702460423421705\n",
      "f1_score: 0.9702460423421705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      5076\n",
      "           1       0.56      0.31      0.40       167\n",
      "\n",
      "    accuracy                           0.97      5243\n",
      "   macro avg       0.77      0.65      0.69      5243\n",
      "weighted avg       0.96      0.97      0.97      5243\n",
      "\n",
      "(8989, 300)\n",
      "accuracy_score: 0.9944688155636087\n",
      "f1_score: 0.9944688155636087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5184\n",
      "           1       0.83      0.64      0.72        59\n",
      "\n",
      "    accuracy                           0.99      5243\n",
      "   macro avg       0.91      0.82      0.86      5243\n",
      "weighted avg       0.99      0.99      0.99      5243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looping through other labels\n",
    "for i in range(len(tagnames)-1):\n",
    "    # Split the train and test dataset\n",
    "    xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df[tagnames[i+1]], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "    \n",
    "    train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    \n",
    "    xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "    xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "    \n",
    "    # predicting on first dataset\n",
    "    estimator = KerasClassifier(build_fn = create_baseline, epochs = 10, batch_size = 5, verbose = 0)\n",
    "    estimator.fit(xtrain_wa,ytrain)\n",
    "    y_pred = estimator.predict(xtest_wa)\n",
    "    print(\"accuracy_score:\", accuracy_score(ytest,y_pred))\n",
    "    print(\"f1_score:\", f1_score(ytest,y_pred,average = 'micro'))\n",
    "    print(classification_report(ytest,y_pred))\n",
    "    \n",
    "    x_test_2 = df_test_2['title_abstract_combined_processed']\n",
    "    test_2_tokenized = x_test_2.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    xtest_2_wa = word_averaging_list(word_vectors,test_2_tokenized)\n",
    "    print(xtest_2_wa.shape)\n",
    "    y_pred_2 = estimator.predict(xtest_2_wa)\n",
    "    \n",
    "    y_pred_list = np.array(y_pred_2).tolist()\n",
    "    df_pred = pd.DataFrame(y_pred_list)\n",
    "    df_test_fs[tagnames[i+1]] = df_pred\n",
    "display(df_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the final results in proper format\n",
    "df_test_fs.to_csv('submission_u_svm_w2v.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
