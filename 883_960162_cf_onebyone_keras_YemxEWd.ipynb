{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>We present novel understandings of the Gamma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
       "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>Milky Way open clusters are very diverse in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>Proving that a cryptographic protocol is cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              TITLE  \\\n",
       "0  20973  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  20974  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  20975         Case For Static AMSDU Aggregation in WLANs   \n",
       "3  20976  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  20977  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                                            ABSTRACT  \n",
       "0    We present novel understandings of the Gamma...  \n",
       "1    Meteorites contain minerals from Solar Syste...  \n",
       "2    Frame aggregation is a mechanism by which mu...  \n",
       "3    Milky Way open clusters are very diverse in ...  \n",
       "4    Proving that a cryptographic protocol is cor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import pandas and numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#read the train csv file and explore\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test_2 = pd.read_csv('test.csv')\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shape of the dataframe is:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20972, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'column names are as follows:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'TITLE', 'ABSTRACT', 'Computer Science', 'Physics', 'Mathematics',\n",
       "       'Statistics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#perform EDA on the dataframe\n",
    "display(\"shape of the dataframe is:\",df.shape)\n",
    "display(\"column names are as follows:\",df.columns)\n",
    "index_final = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20972 entries, 0 to 20971\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   ID                    20972 non-null  int64 \n",
      " 1   TITLE                 20972 non-null  object\n",
      " 2   ABSTRACT              20972 non-null  object\n",
      " 3   Computer Science      20972 non-null  int64 \n",
      " 4   Physics               20972 non-null  int64 \n",
      " 5   Mathematics           20972 non-null  int64 \n",
      " 6   Statistics            20972 non-null  int64 \n",
      " 7   Quantitative Biology  20972 non-null  int64 \n",
      " 8   Quantitative Finance  20972 non-null  int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "      <td>20972.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10486.500000</td>\n",
       "      <td>0.409784</td>\n",
       "      <td>0.286716</td>\n",
       "      <td>0.267881</td>\n",
       "      <td>0.248236</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>0.011873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6054.239259</td>\n",
       "      <td>0.491806</td>\n",
       "      <td>0.452238</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.164947</td>\n",
       "      <td>0.108317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5243.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10486.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15729.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20972.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  Computer Science       Physics   Mathematics  \\\n",
       "count  20972.000000      20972.000000  20972.000000  20972.000000   \n",
       "mean   10486.500000          0.409784      0.286716      0.267881   \n",
       "std     6054.239259          0.491806      0.452238      0.442866   \n",
       "min        1.000000          0.000000      0.000000      0.000000   \n",
       "25%     5243.750000          0.000000      0.000000      0.000000   \n",
       "50%    10486.500000          0.000000      0.000000      0.000000   \n",
       "75%    15729.250000          1.000000      1.000000      1.000000   \n",
       "max    20972.000000          1.000000      1.000000      1.000000   \n",
       "\n",
       "         Statistics  Quantitative Biology  Quantitative Finance  \n",
       "count  20972.000000          20972.000000          20972.000000  \n",
       "mean       0.248236              0.027990              0.011873  \n",
       "std        0.432000              0.164947              0.108317  \n",
       "min        0.000000              0.000000              0.000000  \n",
       "25%        0.000000              0.000000              0.000000  \n",
       "50%        0.000000              0.000000              0.000000  \n",
       "75%        0.000000              0.000000              0.000000  \n",
       "max        1.000000              1.000000              1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer Science',\n",
       " 'Physics',\n",
       " 'Mathematics',\n",
       " 'Statistics',\n",
       " 'Quantitative Biology',\n",
       " 'Quantitative Finance']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the number of research paper under each tag\n",
    "#tag names\n",
    "tagnames = df.drop(['ID','TITLE','ABSTRACT'],axis=1).columns.tolist()\n",
    "display(tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Computer Science': 8594,\n",
       " 'Physics': 6013,\n",
       " 'Mathematics': 5618,\n",
       " 'Statistics': 5206,\n",
       " 'Quantitative Biology': 587,\n",
       " 'Quantitative Finance': 249}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagno = {}\n",
    "for name in tagnames:\n",
    "    tagno.update({name:df[name].sum()})\n",
    "display(tagno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib and plot number of research paper under each tag\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(tagno)),list(tagno.values()),align ='center')\n",
    "plt.xticks(range(len(tagno)),list(tagno.keys()),rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum tags that belong to a research paper are:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e479f7a0d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the maximum number of tags present for a research paper\n",
    "df['combinations'] = df['Computer Science'] + df['Physics'] + df['Mathematics'] + df['Statistics'] + df['Quantitative Biology'] + df['Quantitative Finance']\n",
    "display(\"maximum tags that belong to a research paper are:\", df['combinations'].max())\n",
    "df['combinations'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Computer Science', 'Physics'],\n",
       " ['Computer Science', 'Mathematics'],\n",
       " ['Computer Science', 'Statistics'],\n",
       " ['Computer Science', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Quantitative Finance'],\n",
       " ['Physics', 'Mathematics'],\n",
       " ['Physics', 'Statistics'],\n",
       " ['Physics', 'Quantitative Biology'],\n",
       " ['Physics', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Statistics'],\n",
       " ['Mathematics', 'Quantitative Biology'],\n",
       " ['Mathematics', 'Quantitative Finance'],\n",
       " ['Statistics', 'Quantitative Biology'],\n",
       " ['Statistics', 'Quantitative Finance'],\n",
       " ['Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Physics', 'Mathematics'],\n",
       " ['Computer Science', 'Physics', 'Statistics'],\n",
       " ['Computer Science', 'Physics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Physics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Mathematics', 'Statistics'],\n",
       " ['Computer Science', 'Mathematics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Mathematics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Statistics', 'Quantitative Biology'],\n",
       " ['Computer Science', 'Statistics', 'Quantitative Finance'],\n",
       " ['Computer Science', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Physics', 'Mathematics', 'Statistics'],\n",
       " ['Physics', 'Mathematics', 'Quantitative Biology'],\n",
       " ['Physics', 'Mathematics', 'Quantitative Finance'],\n",
       " ['Physics', 'Statistics', 'Quantitative Biology'],\n",
       " ['Physics', 'Statistics', 'Quantitative Finance'],\n",
       " ['Physics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Statistics', 'Quantitative Biology'],\n",
       " ['Mathematics', 'Statistics', 'Quantitative Finance'],\n",
       " ['Mathematics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Statistics', 'Quantitative Biology', 'Quantitative Finance'],\n",
       " ['Computer Science'],\n",
       " ['Physics'],\n",
       " ['Mathematics'],\n",
       " ['Statistics'],\n",
       " ['Quantitative Biology'],\n",
       " ['Quantitative Finance']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'total tag combinations that can be possibly present:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find all unique combinations of 3 tags\n",
    "from itertools import combinations\n",
    "comb2,comb3 = list(combinations(tagnames,2)),list(combinations(tagnames,3))\n",
    "totcomb = [list(ele) for ele in comb2+comb3]\n",
    "totcomb = totcomb + [[el] for el in tagnames]\n",
    "display(totcomb)\n",
    "display(\"total tag combinations that can be possibly present:\", len(totcomb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Computer Science',): 4910,\n",
       "         ('Mathematics',): 3610,\n",
       "         ('Computer Science', 'Statistics'): 2285,\n",
       "         ('Physics',): 5120,\n",
       "         ('Quantitative Biology',): 443,\n",
       "         ('Statistics',): 1636,\n",
       "         ('Physics', 'Mathematics'): 293,\n",
       "         ('Mathematics', 'Statistics'): 825,\n",
       "         ('Computer Science', 'Mathematics'): 682,\n",
       "         ('Quantitative Finance',): 209,\n",
       "         ('Computer Science', 'Physics'): 437,\n",
       "         ('Computer Science', 'Mathematics', 'Statistics'): 179,\n",
       "         ('Physics', 'Statistics'): 99,\n",
       "         ('Computer Science', 'Physics', 'Statistics'): 36,\n",
       "         ('Computer Science', 'Quantitative Biology'): 30,\n",
       "         ('Statistics', 'Quantitative Biology'): 105,\n",
       "         ('Statistics', 'Quantitative Finance'): 24,\n",
       "         ('Physics', 'Mathematics', 'Statistics'): 9,\n",
       "         ('Computer Science', 'Quantitative Finance'): 9,\n",
       "         ('Quantitative Biology', 'Quantitative Finance'): 4,\n",
       "         ('Computer Science', 'Statistics', 'Quantitative Biology'): 5,\n",
       "         ('Computer Science', 'Physics', 'Mathematics'): 19,\n",
       "         ('Computer Science', 'Statistics', 'Quantitative Finance'): 2,\n",
       "         ('Mathematics', 'Statistics', 'Quantitative Finance'): 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'number of unique combinations'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the counts of all unique combinations of tags\n",
    "from collections import Counter\n",
    "co = []\n",
    "for index, row in df.iterrows():\n",
    "    l = []\n",
    "    for name in tagnames:\n",
    "        if row[name] == 1:\n",
    "            l.append(name)\n",
    "    co.append(l)\n",
    "Ot = Counter([tuple(i) for i in co])\n",
    "display(Ot,\"number of unique combinations\",len(Ot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reconstructing',\n",
       " 'Subject-Specific',\n",
       " 'Effect',\n",
       " 'Maps',\n",
       " 'Rotation',\n",
       " 'Invariance',\n",
       " 'Neural',\n",
       " 'Network',\n",
       " 'Spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'A',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'Maxwell--Landau--Lifshitz--Gilbert',\n",
       " 'system',\n",
       " 'Comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'Discrete',\n",
       " 'Wavelet',\n",
       " 'Transforms',\n",
       " 'and',\n",
       " 'Wavelet',\n",
       " 'Tensor',\n",
       " 'Train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'FTIR',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'On',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'On',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " '1I/`Oumuamua',\n",
       " '(2017)',\n",
       " 'U1',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'Adverse',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'polymer',\n",
       " 'coating',\n",
       " 'on',\n",
       " 'heat',\n",
       " 'transport',\n",
       " 'at',\n",
       " 'solid-liquid',\n",
       " 'interface',\n",
       " 'SPH',\n",
       " 'calculations',\n",
       " 'of',\n",
       " 'Mars-scale',\n",
       " 'collisions:',\n",
       " 'the',\n",
       " 'role',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Equation',\n",
       " 'of',\n",
       " 'State,',\n",
       " 'material',\n",
       " 'rheologies,',\n",
       " 'and',\n",
       " 'numerical',\n",
       " 'effects',\n",
       " '$\\\\mathcal{R}_{0}$',\n",
       " 'fails',\n",
       " 'to',\n",
       " 'predict',\n",
       " 'the',\n",
       " 'outbreak',\n",
       " 'potential',\n",
       " 'in',\n",
       " 'the',\n",
       " 'presence',\n",
       " 'of',\n",
       " 'natural-boosting',\n",
       " 'immunity',\n",
       " 'A',\n",
       " 'global',\n",
       " 'sensitivity',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'reduced',\n",
       " 'order',\n",
       " 'models',\n",
       " 'for',\n",
       " 'hydraulically-fractured',\n",
       " 'horizontal',\n",
       " 'wells',\n",
       " 'Role-separating',\n",
       " 'ordering',\n",
       " 'in',\n",
       " 'social',\n",
       " 'dilemmas',\n",
       " 'controlled',\n",
       " 'by',\n",
       " 'topological',\n",
       " 'frustration',\n",
       " 'Dynamics',\n",
       " 'of',\n",
       " 'exciton',\n",
       " 'magnetic',\n",
       " 'polarons',\n",
       " 'in',\n",
       " 'CdMnSe/CdMgSe',\n",
       " 'quantum',\n",
       " 'wells:',\n",
       " 'the',\n",
       " 'effect',\n",
       " 'of',\n",
       " 'self-localization',\n",
       " 'On',\n",
       " 'Varieties',\n",
       " 'of',\n",
       " 'Ordered',\n",
       " 'Automata',\n",
       " 'Direct',\n",
       " 'Evidence',\n",
       " 'of',\n",
       " 'Spontaneous',\n",
       " 'Abrikosov',\n",
       " 'Vortex',\n",
       " 'State',\n",
       " 'in',\n",
       " 'Ferromagnetic',\n",
       " 'Superconductor',\n",
       " 'EuFe$_2$(As$_{1-x}$P$_x$)$_2$',\n",
       " 'with',\n",
       " '$x=0.21$',\n",
       " 'A',\n",
       " 'rank',\n",
       " '18',\n",
       " 'Waring',\n",
       " 'decomposition',\n",
       " 'of',\n",
       " '$sM_{\\\\langle',\n",
       " '3\\\\rangle}$',\n",
       " 'with',\n",
       " '432',\n",
       " 'symmetries',\n",
       " 'The',\n",
       " 'PdBI',\n",
       " 'Arcsecond',\n",
       " 'Whirlpool',\n",
       " 'Survey',\n",
       " '(PAWS).',\n",
       " 'The',\n",
       " 'Role',\n",
       " 'of',\n",
       " 'Spiral',\n",
       " 'Arms',\n",
       " 'in',\n",
       " 'Cloud',\n",
       " 'and',\n",
       " 'Star',\n",
       " 'Formation',\n",
       " 'Higher',\n",
       " 'structure',\n",
       " 'in',\n",
       " 'the',\n",
       " 'unstable',\n",
       " 'Adams',\n",
       " 'spectral',\n",
       " 'sequence',\n",
       " 'Comparing',\n",
       " 'Covariate',\n",
       " 'Prioritization',\n",
       " 'via',\n",
       " 'Matching',\n",
       " 'to',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Methods',\n",
       " 'for',\n",
       " 'Causal',\n",
       " 'Inference',\n",
       " 'using',\n",
       " 'Five',\n",
       " 'Empirical',\n",
       " 'Applications',\n",
       " 'Acoustic',\n",
       " 'Impedance',\n",
       " 'Calculation',\n",
       " 'via',\n",
       " 'Numerical',\n",
       " 'Solution',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Inverse',\n",
       " 'Helmholtz',\n",
       " 'Problem',\n",
       " 'Deciphering',\n",
       " 'noise',\n",
       " 'amplification',\n",
       " 'and',\n",
       " 'reduction',\n",
       " 'in',\n",
       " 'open',\n",
       " 'chemical',\n",
       " 'reaction',\n",
       " 'networks',\n",
       " 'Many-Body',\n",
       " 'Localization:',\n",
       " 'Stability',\n",
       " 'and',\n",
       " 'Instability',\n",
       " 'Fault',\n",
       " 'Detection',\n",
       " 'and',\n",
       " 'Isolation',\n",
       " 'Tools',\n",
       " '(FDITOOLS)',\n",
       " \"User's\",\n",
       " 'Guide',\n",
       " 'Complexity',\n",
       " 'of',\n",
       " 'Deciding',\n",
       " 'Detectability',\n",
       " 'in',\n",
       " 'Discrete',\n",
       " 'Event',\n",
       " 'Systems',\n",
       " 'The',\n",
       " 'Knaster-Tarski',\n",
       " 'theorem',\n",
       " 'versus',\n",
       " 'monotone',\n",
       " 'nonexpansive',\n",
       " 'mappings',\n",
       " 'Efficient',\n",
       " 'methods',\n",
       " 'for',\n",
       " 'computing',\n",
       " 'integrals',\n",
       " 'in',\n",
       " 'electronic',\n",
       " 'structure',\n",
       " 'calculations',\n",
       " 'Diffraction-Aware',\n",
       " 'Sound',\n",
       " 'Localization',\n",
       " 'for',\n",
       " 'a',\n",
       " 'Non-Line-of-Sight',\n",
       " 'Source',\n",
       " \"Jacob's\",\n",
       " 'ladders,',\n",
       " 'crossbreeding',\n",
       " 'in',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " '$ζ$-factorization',\n",
       " 'formulas',\n",
       " 'and',\n",
       " 'selection',\n",
       " 'of',\n",
       " 'families',\n",
       " 'of',\n",
       " '$ζ$-kindred',\n",
       " 'real',\n",
       " 'continuous',\n",
       " 'functions',\n",
       " 'Minimax',\n",
       " 'Estimation',\n",
       " 'of',\n",
       " 'the',\n",
       " '$L_1$',\n",
       " 'Distance',\n",
       " 'Density',\n",
       " 'large',\n",
       " 'deviations',\n",
       " 'for',\n",
       " 'multidimensional',\n",
       " 'stochastic',\n",
       " 'hyperbolic',\n",
       " 'conservation',\n",
       " 'laws',\n",
       " 'mixup:',\n",
       " 'Beyond',\n",
       " 'Empirical',\n",
       " 'Risk',\n",
       " 'Minimization',\n",
       " 'Equality',\n",
       " 'of',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'definitions',\n",
       " 'of',\n",
       " 'Brakke',\n",
       " 'flow',\n",
       " 'Dynamic',\n",
       " 'Base',\n",
       " 'Station',\n",
       " 'Repositioning',\n",
       " 'to',\n",
       " 'Improve',\n",
       " 'Spectral',\n",
       " 'Efficiency',\n",
       " 'of',\n",
       " 'Drone',\n",
       " 'Small',\n",
       " 'Cells',\n",
       " 'An',\n",
       " 'Unsupervised',\n",
       " 'Homogenization',\n",
       " 'Pipeline',\n",
       " 'for',\n",
       " 'Clustering',\n",
       " 'Similar',\n",
       " 'Patients',\n",
       " 'using',\n",
       " 'Electronic',\n",
       " 'Health',\n",
       " 'Record',\n",
       " 'Data',\n",
       " 'Deep',\n",
       " 'Neural',\n",
       " 'Network',\n",
       " 'Optimized',\n",
       " 'to',\n",
       " 'Resistive',\n",
       " 'Memory',\n",
       " 'with',\n",
       " 'Nonlinear',\n",
       " 'Current-Voltage',\n",
       " 'Characteristics',\n",
       " 'Rate-Distortion',\n",
       " 'Region',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Gray-Wyner',\n",
       " 'Model',\n",
       " 'with',\n",
       " 'Side',\n",
       " 'Information',\n",
       " 'Fourier-based',\n",
       " 'numerical',\n",
       " 'approximation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Weertman',\n",
       " 'equation',\n",
       " 'for',\n",
       " 'moving',\n",
       " 'dislocations',\n",
       " 'Design',\n",
       " 'Decisions',\n",
       " 'for',\n",
       " 'Weave:',\n",
       " 'A',\n",
       " 'Real-Time',\n",
       " 'Web-based',\n",
       " 'Collaborative',\n",
       " 'Visualization',\n",
       " 'Framework',\n",
       " 'Suzaku',\n",
       " 'Analysis',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Supernova',\n",
       " 'Remnant',\n",
       " 'G306.3-0.9',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Gamma-ray',\n",
       " 'View',\n",
       " 'of',\n",
       " 'Its',\n",
       " 'Neighborhood',\n",
       " 'Japanese',\n",
       " 'Sentiment',\n",
       " 'Classification',\n",
       " 'using',\n",
       " 'a',\n",
       " 'Tree-Structured',\n",
       " 'Long',\n",
       " 'Short-Term',\n",
       " 'Memory',\n",
       " 'with',\n",
       " 'Attention',\n",
       " 'Covariances,',\n",
       " 'Robustness,',\n",
       " 'and',\n",
       " 'Variational',\n",
       " 'Bayes',\n",
       " 'Are',\n",
       " 'multi-factor',\n",
       " 'Gaussian',\n",
       " 'term',\n",
       " 'structure',\n",
       " 'models',\n",
       " 'still',\n",
       " 'useful?',\n",
       " 'An',\n",
       " 'empirical',\n",
       " 'analysis',\n",
       " 'on',\n",
       " 'Italian',\n",
       " 'BTPs',\n",
       " 'Probing',\n",
       " 'valley',\n",
       " 'filtering',\n",
       " 'effect',\n",
       " 'by',\n",
       " 'Andreev',\n",
       " 'reflection',\n",
       " 'in',\n",
       " 'zigzag',\n",
       " 'graphene',\n",
       " 'nanoribbon',\n",
       " 'Generalized',\n",
       " 'Approximate',\n",
       " 'Message-Passing',\n",
       " 'Decoder',\n",
       " 'for',\n",
       " 'Universal',\n",
       " 'Sparse',\n",
       " 'Superposition',\n",
       " 'Codes',\n",
       " 'LAAIR:',\n",
       " 'A',\n",
       " 'Layered',\n",
       " 'Architecture',\n",
       " 'for',\n",
       " 'Autonomous',\n",
       " 'Interactive',\n",
       " 'Robots',\n",
       " '3D',\n",
       " 'Human',\n",
       " 'Pose',\n",
       " 'Estimation',\n",
       " 'in',\n",
       " 'RGBD',\n",
       " 'Images',\n",
       " 'for',\n",
       " 'Robotic',\n",
       " 'Task',\n",
       " 'Learning',\n",
       " 'Simultaneous',\n",
       " 'non-vanishing',\n",
       " 'for',\n",
       " 'Dirichlet',\n",
       " 'L-functions',\n",
       " 'Wehrl',\n",
       " 'Entropy',\n",
       " 'Based',\n",
       " 'Quantification',\n",
       " 'of',\n",
       " 'Nonclassicality',\n",
       " 'for',\n",
       " 'Single',\n",
       " 'Mode',\n",
       " 'Quantum',\n",
       " 'Optical',\n",
       " 'States',\n",
       " 'Attention-based',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Person',\n",
       " 'Retrieval',\n",
       " 'Large',\n",
       " 'Scale',\n",
       " 'Automated',\n",
       " 'Forecasting',\n",
       " 'for',\n",
       " 'Monitoring',\n",
       " 'Network',\n",
       " 'Safety',\n",
       " 'and',\n",
       " 'Security',\n",
       " 'Contextual',\n",
       " 'Regression:',\n",
       " 'An',\n",
       " 'Accurate',\n",
       " 'and',\n",
       " 'Conveniently',\n",
       " 'Interpretable',\n",
       " 'Nonlinear',\n",
       " 'Model',\n",
       " 'for',\n",
       " 'Mining',\n",
       " 'Discovery',\n",
       " 'from',\n",
       " 'Scientific',\n",
       " 'Data',\n",
       " 'Multi-time',\n",
       " 'correlators',\n",
       " 'in',\n",
       " 'continuous',\n",
       " 'measurement',\n",
       " 'of',\n",
       " 'qubit',\n",
       " 'observables',\n",
       " 'Parallelism,',\n",
       " 'Concurrency',\n",
       " 'and',\n",
       " 'Distribution',\n",
       " 'in',\n",
       " 'Constraint',\n",
       " 'Handling',\n",
       " 'Rules:',\n",
       " 'A',\n",
       " 'Survey',\n",
       " 'Robustness',\n",
       " 'against',\n",
       " 'the',\n",
       " 'channel',\n",
       " 'effect',\n",
       " 'in',\n",
       " 'pathological',\n",
       " 'voice',\n",
       " 'detection',\n",
       " 'An',\n",
       " 'Effective',\n",
       " 'Framework',\n",
       " 'for',\n",
       " 'Constructing',\n",
       " 'Exponent',\n",
       " 'Lattice',\n",
       " 'Basis',\n",
       " 'of',\n",
       " 'Nonzero',\n",
       " 'Algebraic',\n",
       " 'Numbers',\n",
       " 'Competing',\n",
       " 'evolutionary',\n",
       " 'paths',\n",
       " 'in',\n",
       " 'growing',\n",
       " 'populations',\n",
       " 'with',\n",
       " 'applications',\n",
       " 'to',\n",
       " 'multidrug',\n",
       " 'resistance',\n",
       " 'Transient',\n",
       " 'flows',\n",
       " 'in',\n",
       " 'active',\n",
       " 'porous',\n",
       " 'media',\n",
       " 'An',\n",
       " 'information',\n",
       " 'model',\n",
       " 'for',\n",
       " 'modular',\n",
       " 'robots:',\n",
       " 'the',\n",
       " 'Hardware',\n",
       " 'Robot',\n",
       " 'Information',\n",
       " 'Model',\n",
       " '(HRIM)',\n",
       " 'Detecting',\n",
       " 'Adversarial',\n",
       " 'Samples',\n",
       " 'Using',\n",
       " 'Density',\n",
       " 'Ratio',\n",
       " 'Estimates',\n",
       " 'The',\n",
       " 'Query',\n",
       " 'Complexity',\n",
       " 'of',\n",
       " 'Cake',\n",
       " 'Cutting',\n",
       " 'Stacked',\n",
       " 'Convolutional',\n",
       " 'and',\n",
       " 'Recurrent',\n",
       " 'Neural',\n",
       " 'Networks',\n",
       " 'for',\n",
       " 'Music',\n",
       " 'Emotion',\n",
       " 'Recognition',\n",
       " 'Timed',\n",
       " 'Automata',\n",
       " 'with',\n",
       " 'Polynomial',\n",
       " 'Delay',\n",
       " 'and',\n",
       " 'their',\n",
       " 'Expressiveness',\n",
       " 'Superconducting',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'Cu',\n",
       " 'intercalated',\n",
       " 'Bi$_2$Se$_3$',\n",
       " 'studied',\n",
       " 'by',\n",
       " 'Muon',\n",
       " 'Spin',\n",
       " 'Spectroscopy',\n",
       " 'Time-domain',\n",
       " 'THz',\n",
       " 'spectroscopy',\n",
       " 'reveals',\n",
       " 'coupled',\n",
       " 'protein-hydration',\n",
       " 'dielectric',\n",
       " 'response',\n",
       " 'in',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'native',\n",
       " 'and',\n",
       " 'fibrils',\n",
       " 'of',\n",
       " 'human',\n",
       " 'lyso-zyme',\n",
       " 'Inversion',\n",
       " 'of',\n",
       " 'Qubit',\n",
       " 'Energy',\n",
       " 'Levels',\n",
       " 'in',\n",
       " 'Qubit-Oscillator',\n",
       " 'Circuits',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Deep-Strong-Coupling',\n",
       " 'Regime',\n",
       " 'Deep',\n",
       " 'Multiple',\n",
       " 'Instance',\n",
       " 'Feature',\n",
       " 'Learning',\n",
       " 'via',\n",
       " 'Variational',\n",
       " 'Autoencoder',\n",
       " 'Regularity',\n",
       " 'of',\n",
       " 'envelopes',\n",
       " 'in',\n",
       " 'Kähler',\n",
       " 'classes',\n",
       " '$S^1$-equivariant',\n",
       " 'Index',\n",
       " 'theorems',\n",
       " 'and',\n",
       " 'Morse',\n",
       " 'inequalities',\n",
       " 'on',\n",
       " 'complex',\n",
       " 'manifolds',\n",
       " 'with',\n",
       " 'boundary',\n",
       " 'Internal',\n",
       " 'Model',\n",
       " 'from',\n",
       " 'Observations',\n",
       " 'for',\n",
       " 'Reward',\n",
       " 'Shaping',\n",
       " 'Characterizations',\n",
       " 'of',\n",
       " 'quasitrivial',\n",
       " 'symmetric',\n",
       " 'nondecreasing',\n",
       " 'associative',\n",
       " 'operations',\n",
       " 'Multivariate',\n",
       " 'Dependency',\n",
       " 'Measure',\n",
       " 'based',\n",
       " 'on',\n",
       " 'Copula',\n",
       " 'and',\n",
       " 'Gaussian',\n",
       " 'Kernel',\n",
       " 'The',\n",
       " 'nature',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tensor',\n",
       " 'order',\n",
       " 'in',\n",
       " 'Cd2Re2O7',\n",
       " 'Efficient',\n",
       " 'and',\n",
       " 'consistent',\n",
       " 'inference',\n",
       " 'of',\n",
       " 'ancestral',\n",
       " 'sequences',\n",
       " 'in',\n",
       " 'an',\n",
       " 'evolutionary',\n",
       " 'model',\n",
       " 'with',\n",
       " 'insertions',\n",
       " 'and',\n",
       " 'deletions',\n",
       " 'under',\n",
       " 'dense',\n",
       " 'taxon',\n",
       " 'sampling',\n",
       " 'Flow',\n",
       " 'Characteristics',\n",
       " 'and',\n",
       " 'Cores',\n",
       " 'of',\n",
       " 'Complex',\n",
       " 'Network',\n",
       " 'and',\n",
       " 'Multiplex',\n",
       " 'Type',\n",
       " 'Systems',\n",
       " 'Pattern-forming',\n",
       " 'fronts',\n",
       " 'in',\n",
       " 'a',\n",
       " 'Swift-Hohenberg',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'directional',\n",
       " 'quenching',\n",
       " '-',\n",
       " 'parallel',\n",
       " 'and',\n",
       " 'oblique',\n",
       " 'stripes',\n",
       " 'Generalized',\n",
       " 'Minimum',\n",
       " 'Distance',\n",
       " 'Estimators',\n",
       " 'in',\n",
       " 'Linear',\n",
       " 'Regression',\n",
       " 'with',\n",
       " 'Dependent',\n",
       " 'Errors',\n",
       " 'Live',\n",
       " 'Service',\n",
       " 'Migration',\n",
       " 'in',\n",
       " 'Mobile',\n",
       " 'Edge',\n",
       " 'Clouds',\n",
       " 'Induced',\n",
       " 'density',\n",
       " 'correlations',\n",
       " 'in',\n",
       " 'a',\n",
       " 'sonic',\n",
       " 'black',\n",
       " 'hole',\n",
       " 'condensate',\n",
       " 'Genus',\n",
       " 'growth',\n",
       " 'in',\n",
       " '$\\\\mathbb{Z}_p$-towers',\n",
       " 'of',\n",
       " 'function',\n",
       " 'fields',\n",
       " 'Topological',\n",
       " 'Phases',\n",
       " 'emerging',\n",
       " 'from',\n",
       " 'Spin-Orbital',\n",
       " 'Physics',\n",
       " 'Accurate',\n",
       " 'and',\n",
       " 'Diverse',\n",
       " 'Sampling',\n",
       " 'of',\n",
       " 'Sequences',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " '\"Best',\n",
       " 'of',\n",
       " 'Many\"',\n",
       " 'Sample',\n",
       " 'Objective',\n",
       " 'Exploring',\n",
       " 'RNN-Transducer',\n",
       " 'for',\n",
       " 'Chinese',\n",
       " 'Speech',\n",
       " 'Recognition',\n",
       " 'A',\n",
       " 'Debt-Aware',\n",
       " 'Learning',\n",
       " 'Approach',\n",
       " 'for',\n",
       " 'Resource',\n",
       " 'Adaptations',\n",
       " 'in',\n",
       " 'Cloud',\n",
       " 'Elasticity',\n",
       " 'Management',\n",
       " 'Semi-simplicial',\n",
       " 'spaces',\n",
       " 'Constraints,',\n",
       " 'Lazy',\n",
       " 'Constraints,',\n",
       " 'or',\n",
       " 'Propagators',\n",
       " 'in',\n",
       " 'ASP',\n",
       " 'Solving:',\n",
       " 'An',\n",
       " 'Empirical',\n",
       " 'Analysis',\n",
       " 'A',\n",
       " 'Unified',\n",
       " 'Approach',\n",
       " 'to',\n",
       " 'Nonlinear',\n",
       " 'Transformation',\n",
       " 'Materials',\n",
       " 'Stationary',\n",
       " 'crack',\n",
       " 'propagation',\n",
       " 'in',\n",
       " 'a',\n",
       " 'two-dimensional',\n",
       " 'visco-elastic',\n",
       " 'network',\n",
       " 'model',\n",
       " 'A',\n",
       " 'note',\n",
       " 'on',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'group',\n",
       " 'of',\n",
       " 'Kodaira',\n",
       " 'fibrations',\n",
       " 'Photo-Chemically',\n",
       " 'Directed',\n",
       " 'Self-Assembly',\n",
       " 'of',\n",
       " 'Carbon',\n",
       " 'Nanotubes',\n",
       " 'on',\n",
       " 'Surfaces',\n",
       " 'Split-and-augmented',\n",
       " 'Gibbs',\n",
       " 'sampler',\n",
       " '-',\n",
       " 'Application',\n",
       " 'to',\n",
       " 'large-scale',\n",
       " 'inference',\n",
       " 'problems',\n",
       " 'Does',\n",
       " 'a',\n",
       " 'generalized',\n",
       " 'Chaplygin',\n",
       " 'gas',\n",
       " 'correctly',\n",
       " 'describe',\n",
       " 'the',\n",
       " 'cosmological',\n",
       " 'dark',\n",
       " 'sector?',\n",
       " 'The',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'subdiffusion',\n",
       " 'on',\n",
       " 'the',\n",
       " 'NTA',\n",
       " 'size',\n",
       " 'measurements',\n",
       " 'of',\n",
       " 'extracellular',\n",
       " 'vesicles',\n",
       " 'in',\n",
       " 'biological',\n",
       " 'samples',\n",
       " 'Empirical',\n",
       " 'regression',\n",
       " 'quantile',\n",
       " 'process',\n",
       " 'with',\n",
       " 'possible',\n",
       " 'application',\n",
       " 'to',\n",
       " 'risk',\n",
       " 'analysis',\n",
       " 'Primordial',\n",
       " 'perturbations',\n",
       " 'from',\n",
       " 'inflation',\n",
       " 'with',\n",
       " 'a',\n",
       " 'hyperbolic',\n",
       " 'field-space',\n",
       " 'Role',\n",
       " 'of',\n",
       " 'Vanadyl',\n",
       " 'Oxygen',\n",
       " 'in',\n",
       " 'Understanding',\n",
       " 'Metallic',\n",
       " 'Behavior',\n",
       " 'of',\n",
       " 'V2O5(001)',\n",
       " 'Nanorods',\n",
       " 'Graph',\n",
       " 'Convolution:',\n",
       " 'A',\n",
       " 'High-Order',\n",
       " 'and',\n",
       " 'Adaptive',\n",
       " 'Approach',\n",
       " 'Learning',\n",
       " 'Sparse',\n",
       " 'Representations',\n",
       " 'in',\n",
       " 'Reinforcement',\n",
       " 'Learning',\n",
       " 'with',\n",
       " 'Sparse',\n",
       " 'Coding',\n",
       " 'Almost',\n",
       " 'euclidean',\n",
       " 'Isoperimetric',\n",
       " 'Inequalities',\n",
       " 'in',\n",
       " 'spaces',\n",
       " 'satisfying',\n",
       " 'local',\n",
       " 'Ricci',\n",
       " 'curvature',\n",
       " 'lower',\n",
       " 'bounds',\n",
       " 'Exponential',\n",
       " 'Sums',\n",
       " 'and',\n",
       " 'Riesz',\n",
       " 'energies',\n",
       " 'One',\n",
       " 'dimensionalization',\n",
       " 'in',\n",
       " 'the',\n",
       " 'spin-1',\n",
       " 'Heisenberg',\n",
       " 'model',\n",
       " 'on',\n",
       " 'the',\n",
       " 'anisotropic',\n",
       " 'triangular',\n",
       " 'lattice',\n",
       " 'Memory',\n",
       " 'Aware',\n",
       " 'Synapses:',\n",
       " 'Learning',\n",
       " 'what',\n",
       " '(not)',\n",
       " 'to',\n",
       " 'forget',\n",
       " 'Uniform',\n",
       " 'Spectral',\n",
       " 'Convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Stochastic',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject-specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data.',\n",
       " 'Given',\n",
       " 'a',\n",
       " \"subject's\",\n",
       " 'data,',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels:',\n",
       " 'global,',\n",
       " 'i.e.',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject,',\n",
       " 'and',\n",
       " 'local,',\n",
       " 'i.e.',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " \"subject's\",\n",
       " 'data.',\n",
       " 'While',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used,',\n",
       " 'local',\n",
       " 'inference,',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject-specific',\n",
       " 'effect',\n",
       " 'maps,',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'article,',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method,',\n",
       " 'named',\n",
       " 'RSM,',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject-specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular,',\n",
       " 'binary',\n",
       " 'classifiers.',\n",
       " 'RSM',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers.',\n",
       " 'The',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper-type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner,',\n",
       " 'i.e.',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence.',\n",
       " 'Reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'Maximum-A-Posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier-specific',\n",
       " 'fashion.',\n",
       " 'Experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " \"Alzheimer's\",\n",
       " 'Disease',\n",
       " 'Neuroimaging',\n",
       " 'Initiative',\n",
       " '(ADNI)',\n",
       " 'database.',\n",
       " 'Results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'RSM',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging.',\n",
       " 'Analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ADNI',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'RSM',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject-specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non-imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " \"Alzheimer's\",\n",
       " 'Disease',\n",
       " '(AD),',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Mini',\n",
       " 'Mental',\n",
       " 'State',\n",
       " 'Examination',\n",
       " 'Score',\n",
       " 'and',\n",
       " 'Cerebrospinal',\n",
       " 'Fluid',\n",
       " 'amyloid-$\\\\beta$',\n",
       " 'levels.',\n",
       " 'Further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'ADNI',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'RSM',\n",
       " 'is',\n",
       " 'used.',\n",
       " 'Rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'paper,',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " '(CNN)',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " '2-D',\n",
       " 'symbol',\n",
       " 'recognition.',\n",
       " 'We',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " '2-D',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non-overlap',\n",
       " 'target.',\n",
       " 'Last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least,',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one-shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance.',\n",
       " 'We',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics,',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics.',\n",
       " 'In',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics,',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us,',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics,',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls.',\n",
       " 'We',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Gegenbauer',\n",
       " 'polynomials.',\n",
       " 'We',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'Poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball,',\n",
       " 'Poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls,',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Cauchy-Hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Lie',\n",
       " 'ball.',\n",
       " 'The',\n",
       " 'stochastic',\n",
       " 'Landau--Lifshitz--Gilbert',\n",
       " '(LLG)',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Maxwell',\n",
       " 'equations',\n",
       " '(the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'MLLG',\n",
       " 'system)',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " '(fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories).',\n",
       " 'We',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'LLG',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time-differentiable',\n",
       " 'solutions.',\n",
       " 'We',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " '$\\\\theta$-linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'consequence,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions,',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " '(depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " '$\\\\theta$).',\n",
       " 'Hence,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'MLLG',\n",
       " 'system.',\n",
       " 'Numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method.',\n",
       " 'Fourier-transform',\n",
       " 'infra-red',\n",
       " '(FTIR)',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " '7',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms.',\n",
       " 'Wavelet',\n",
       " 'Tensor',\n",
       " 'Train',\n",
       " '(WTT)',\n",
       " 'and',\n",
       " 'Discrete',\n",
       " 'Wavelet',\n",
       " 'Transforms',\n",
       " '(DWT)',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'FTIR',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants.',\n",
       " 'Various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks.',\n",
       " 'Best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'WTT',\n",
       " 'and',\n",
       " 'DWT',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar,',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra.',\n",
       " 'Unlike',\n",
       " 'DWT,',\n",
       " 'WTT',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " '(rank),',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications.',\n",
       " 'Let',\n",
       " '$\\\\Omega',\n",
       " '\\\\subset',\n",
       " '\\\\mathbb{R}^n$',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'Hayman-type',\n",
       " 'asymmetry',\n",
       " 'condition,',\n",
       " 'and',\n",
       " 'let',\n",
       " '$',\n",
       " 'D',\n",
       " '$',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " '\"obstacle\".',\n",
       " 'We',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'Dirichlet',\n",
       " 'eigenvalue',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '$.',\n",
       " 'First,',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '$',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'Dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1}',\n",
       " '>',\n",
       " '0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\Omega',\n",
       " '$.',\n",
       " 'In',\n",
       " 'short,',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\mu_\\\\Omega',\n",
       " ':=',\n",
       " '\\\\max_{x}\\\\lambda_1(\\\\Omega',\n",
       " '\\\\setminus',\n",
       " '(x+D))',\n",
       " '\\\\end{equation}',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega)',\n",
       " '$,',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1}',\n",
       " '$.',\n",
       " 'Second,',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1(\\\\Omega)}',\n",
       " '$',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " '$',\n",
       " '\\\\Omega',\n",
       " '$.',\n",
       " 'Finally,',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " '$',\n",
       " 'D',\n",
       " '$',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " '$',\n",
       " '\\\\lambda_1(\\\\Omega)',\n",
       " '$,',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " '$',\n",
       " 'x+D',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\mu_\\\\Omega',\n",
       " '$',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " '$',\n",
       " 'x_0',\n",
       " '$',\n",
       " 'of',\n",
       " '$',\n",
       " '\\\\phi_{\\\\lambda_1(\\\\Omega)}',\n",
       " '$.',\n",
       " 'We',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " '1I/`Oumuamua',\n",
       " '(2017',\n",
       " 'U1)',\n",
       " 'on',\n",
       " '2017',\n",
       " 'October',\n",
       " '30',\n",
       " 'with',\n",
       " 'Lowell',\n",
       " \"Observatory's\",\n",
       " '4.3-m',\n",
       " 'Discovery',\n",
       " 'Channel',\n",
       " 'Telescope.',\n",
       " 'From',\n",
       " 'these',\n",
       " 'observations,',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak-to-trough',\n",
       " 'amplitude',\n",
       " 'of',\n",
       " 'at',\n",
       " 'least',\n",
       " '1.2',\n",
       " 'mag.',\n",
       " 'This',\n",
       " 'lightcurve',\n",
       " 'segment',\n",
       " 'rules',\n",
       " 'out',\n",
       " 'rotation',\n",
       " 'periods',\n",
       " 'less',\n",
       " 'than',\n",
       " '3',\n",
       " 'hr',\n",
       " 'and',\n",
       " 'suggests',\n",
       " 'that',\n",
       " 'the',\n",
       " 'period',\n",
       " 'is',\n",
       " 'at',\n",
       " 'least',\n",
       " '5',\n",
       " 'hr.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'assumption',\n",
       " 'that',\n",
       " 'the',\n",
       " 'variability',\n",
       " 'is',\n",
       " 'due',\n",
       " 'to',\n",
       " 'a',\n",
       " 'changing',\n",
       " 'cross',\n",
       " 'section,',\n",
       " 'the',\n",
       " 'axial',\n",
       " 'ratio',\n",
       " 'is',\n",
       " 'at',\n",
       " 'least',\n",
       " '3:1.',\n",
       " 'We',\n",
       " 'saw',\n",
       " 'no',\n",
       " 'evidence',\n",
       " 'for',\n",
       " 'a',\n",
       " 'coma',\n",
       " 'or',\n",
       " 'tail',\n",
       " 'in',\n",
       " 'either',\n",
       " 'individual',\n",
       " 'images',\n",
       " 'or',\n",
       " 'in',\n",
       " 'a',\n",
       " 'stacked',\n",
       " 'image',\n",
       " 'having',\n",
       " 'an',\n",
       " 'equivalent',\n",
       " 'exposure',\n",
       " 'time',\n",
       " 'of',\n",
       " '9000',\n",
       " 's.',\n",
       " 'The',\n",
       " 'ability',\n",
       " 'of',\n",
       " 'metallic',\n",
       " 'nanoparticles',\n",
       " 'to',\n",
       " 'supply',\n",
       " 'heat',\n",
       " 'to',\n",
       " 'a',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analysing the words in titles and abstracts\n",
    "titles = []\n",
    "abstracts = []\n",
    "for index,row in df.iterrows():\n",
    "    l = row['TITLE'].split()\n",
    "    m = row['ABSTRACT'].split()\n",
    "    for el in l:\n",
    "        titles.append(el)\n",
    "    for ele in m:\n",
    "        abstracts.append(ele)\n",
    "display(titles)\n",
    "display(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>combinations</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance  combinations  \\\n",
       "0                     0             1   \n",
       "1                     0             1   \n",
       "2                     0             1   \n",
       "3                     0             1   \n",
       "4                     0             2   \n",
       "\n",
       "                             title_abstract_combined  \n",
       "0  Reconstructing Subject-Specific Effect Maps   ...  \n",
       "1  Rotation Invariance Neural Network   Rotation ...  \n",
       "2  Spherical polyharmonics and Poisson kernels fo...  \n",
       "3  A finite element approximation for the stochas...  \n",
       "4  Comparative study of Discrete Wavelet Transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>We present novel understandings of the Gamma...</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
       "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>Milky Way open clusters are very diverse in ...</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>Proving that a cryptographic protocol is cor...</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              TITLE  \\\n",
       "0  20973  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  20974  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  20975         Case For Static AMSDU Aggregation in WLANs   \n",
       "3  20976  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  20977  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0    We present novel understandings of the Gamma...   \n",
       "1    Meteorites contain minerals from Solar Syste...   \n",
       "2    Frame aggregation is a mechanism by which mu...   \n",
       "3    Milky Way open clusters are very diverse in ...   \n",
       "4    Proving that a cryptographic protocol is cor...   \n",
       "\n",
       "                             title_abstract_combined  \n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...  \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...  \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...  \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...  \n",
       "4  Witness-Functions versus Interpretation-Functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine title and abstract\n",
    "df['title_abstract_combined'] = df['TITLE'] + ' ' + df['ABSTRACT']\n",
    "df_test_2['title_abstract_combined'] = df_test_2['TITLE'] + ' ' + df_test_2['ABSTRACT']\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...  \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...  \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...  \n",
       "3                     0  A finite element approximation for the stochas...  \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove ID, TITLE, ABSTRACT, combinations, title_processed, and abstract_processed columns\n",
    "df = df.drop(labels = ['ID','TITLE','ABSTRACT','combinations'], axis = 1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...\n",
       "1  Laboratory mid-IR spectra of equilibrated and ...\n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...\n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...\n",
       "4  Witness-Functions versus Interpretation-Functi..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test_2 = df_test_2.drop(labels = ['ID','TITLE','ABSTRACT'], axis = 1)\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstructing subject specific effect maps pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotation invariance neural network rotation in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spherical polyharmonics and poisson kernels fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>a finite element approximation for the stochas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>comparative study of discrete wavelet transfor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstructing subject specific effect maps pr...  \n",
       "1  rotation invariance neural network rotation in...  \n",
       "2  spherical polyharmonics and poisson kernels fo...  \n",
       "3  a finite element approximation for the stochas...  \n",
       "4  comparative study of discrete wavelet transfor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>closed form marginal likelihood in gamma poiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratory mid ir spectra of equilibrated and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case for static amsdu aggregation in wlans fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>the gaia eso survey the inner disk intermediat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>witness functions versus interpretation functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  closed form marginal likelihood in gamma poiss...  \n",
       "1  laboratory mid ir spectra of equilibrated and ...  \n",
       "2  case for static amsdu aggregation in wlans fra...  \n",
       "3  the gaia eso survey the inner disk intermediat...  \n",
       "4  witness functions versus interpretation functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the regular expressions library\n",
    "import re\n",
    "\n",
    "#remove punctuation from title and abstract\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined'].map(lambda x: re.sub('[,\\.!?0-9]', ' ',x))\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined'].map(lambda x: re.sub('[,\\.!?0-9]', ' ',x))\n",
    "\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined_processed'].map(lambda x: re.sub('[\\W_]+', ' ',x))\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined_processed'].map(lambda x: re.sub('[\\W_]+', ' ',x))\n",
    "\n",
    "#convert the title and abstract to lowercase\n",
    "df['title_abstract_combined_processed'] = df['title_abstract_combined_processed'].map(lambda x: x.lower())\n",
    "df_test_2['title_abstract_combined_processed'] = df_test_2['title_abstract_combined_processed'].map(lambda x: x.lower())\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reconstructing',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data',\n",
       " 'given',\n",
       " 'a',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels',\n",
       " 'global',\n",
       " 'i',\n",
       " 'e',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'and',\n",
       " 'local',\n",
       " 'i',\n",
       " 'e',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'while',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'local',\n",
       " 'inference',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands',\n",
       " 'in',\n",
       " 'this',\n",
       " 'article',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method',\n",
       " 'named',\n",
       " 'rsm',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'rsm',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper',\n",
       " 'type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner',\n",
       " 'i',\n",
       " 'e',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'maximum',\n",
       " 'a',\n",
       " 'posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier',\n",
       " 'specific',\n",
       " 'fashion',\n",
       " 'experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'neuroimaging',\n",
       " 'initiative',\n",
       " 'adni',\n",
       " 'database',\n",
       " 'results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'rsm',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging',\n",
       " 'analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'rsm',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non',\n",
       " 'imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'ad',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mini',\n",
       " 'mental',\n",
       " 'state',\n",
       " 'examination',\n",
       " 'score',\n",
       " 'and',\n",
       " 'cerebrospinal',\n",
       " 'fluid',\n",
       " 'amyloid',\n",
       " 'beta',\n",
       " 'levels',\n",
       " 'further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'rsm',\n",
       " 'is',\n",
       " 'used',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'cnn',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'recognition',\n",
       " 'we',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non',\n",
       " 'overlap',\n",
       " 'target',\n",
       " 'last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'we',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'we',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gegenbauer',\n",
       " 'polynomials',\n",
       " 'we',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cauchy',\n",
       " 'hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lie',\n",
       " 'ball',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'maxwell',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'system',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'maxwell',\n",
       " 'equations',\n",
       " 'the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " 'fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories',\n",
       " 'we',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time',\n",
       " 'differentiable',\n",
       " 'solutions',\n",
       " 'we',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " 'theta',\n",
       " 'linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system',\n",
       " 'as',\n",
       " 'a',\n",
       " 'consequence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'theta',\n",
       " 'hence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method',\n",
       " 'comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'and',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'fourier',\n",
       " 'transform',\n",
       " 'infra',\n",
       " 'red',\n",
       " 'ftir',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'dwt',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks',\n",
       " 'best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'dwt',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra',\n",
       " 'unlike',\n",
       " 'dwt',\n",
       " 'wtt',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " 'rank',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications',\n",
       " 'on',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'let',\n",
       " 'omega',\n",
       " 'subset',\n",
       " 'mathbb',\n",
       " 'r',\n",
       " 'n',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'hayman',\n",
       " 'type',\n",
       " 'asymmetry',\n",
       " 'condition',\n",
       " 'and',\n",
       " 'let',\n",
       " 'd',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'obstacle',\n",
       " 'we',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'eigenvalue',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'first',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " 'x',\n",
       " 'd',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'of',\n",
       " 'omega',\n",
       " 'in',\n",
       " 'short',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " 'begin',\n",
       " 'equation',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'max',\n",
       " 'x',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'end',\n",
       " 'equation',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'second',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " 'omega',\n",
       " 'finally',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " 'd',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'we',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'on',\n",
       " 'october',\n",
       " 'with',\n",
       " 'lowell',\n",
       " 'observatory',\n",
       " 's',\n",
       " 'm',\n",
       " 'discovery',\n",
       " 'channel',\n",
       " 'telescope',\n",
       " 'from',\n",
       " 'these',\n",
       " 'observations',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['reconstructing',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'predictive',\n",
       " 'models',\n",
       " 'allow',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'inference',\n",
       " 'when',\n",
       " 'analyzing',\n",
       " 'disease',\n",
       " 'related',\n",
       " 'alterations',\n",
       " 'in',\n",
       " 'neuroimaging',\n",
       " 'data',\n",
       " 'given',\n",
       " 'a',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'inference',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'two',\n",
       " 'levels',\n",
       " 'global',\n",
       " 'i',\n",
       " 'e',\n",
       " 'identifiying',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'and',\n",
       " 'local',\n",
       " 'i',\n",
       " 'e',\n",
       " 'detecting',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'each',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extracted',\n",
       " 'from',\n",
       " 'the',\n",
       " 'subject',\n",
       " 's',\n",
       " 'data',\n",
       " 'while',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'local',\n",
       " 'inference',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'form',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'maps',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'used',\n",
       " 'because',\n",
       " 'existing',\n",
       " 'models',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detections',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'dispersed',\n",
       " 'isolated',\n",
       " 'islands',\n",
       " 'in',\n",
       " 'this',\n",
       " 'article',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'reconstruction',\n",
       " 'method',\n",
       " 'named',\n",
       " 'rsm',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'of',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approaches',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'rsm',\n",
       " 'specifically',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'to',\n",
       " 'sampling',\n",
       " 'error',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'using',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'train',\n",
       " 'classifiers',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'is',\n",
       " 'a',\n",
       " 'wrapper',\n",
       " 'type',\n",
       " 'algorithm',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'with',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifiers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'diagnostic',\n",
       " 'manner',\n",
       " 'i',\n",
       " 'e',\n",
       " 'without',\n",
       " 'information',\n",
       " 'on',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'reconstruction',\n",
       " 'is',\n",
       " 'posed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'maximum',\n",
       " 'a',\n",
       " 'posteriori',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'whose',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'estimated',\n",
       " 'from',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'classifier',\n",
       " 'specific',\n",
       " 'fashion',\n",
       " 'experimental',\n",
       " 'evaluation',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'synthetically',\n",
       " 'generated',\n",
       " 'data',\n",
       " 'and',\n",
       " 'data',\n",
       " 'from',\n",
       " 'the',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'neuroimaging',\n",
       " 'initiative',\n",
       " 'adni',\n",
       " 'database',\n",
       " 'results',\n",
       " 'on',\n",
       " 'synthetic',\n",
       " 'data',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'using',\n",
       " 'rsm',\n",
       " 'yields',\n",
       " 'higher',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'using',\n",
       " 'models',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'with',\n",
       " 'bootstrap',\n",
       " 'averaging',\n",
       " 'analyses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'that',\n",
       " 'rsm',\n",
       " 'can',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'subject',\n",
       " 'specific',\n",
       " 'detections',\n",
       " 'in',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'and',\n",
       " 'non',\n",
       " 'imaging',\n",
       " 'markers',\n",
       " 'of',\n",
       " 'alzheimer',\n",
       " 's',\n",
       " 'disease',\n",
       " 'ad',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mini',\n",
       " 'mental',\n",
       " 'state',\n",
       " 'examination',\n",
       " 'score',\n",
       " 'and',\n",
       " 'cerebrospinal',\n",
       " 'fluid',\n",
       " 'amyloid',\n",
       " 'beta',\n",
       " 'levels',\n",
       " 'further',\n",
       " 'reliability',\n",
       " 'studies',\n",
       " 'on',\n",
       " 'the',\n",
       " 'longitudinal',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'on',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'when',\n",
       " 'rsm',\n",
       " 'is',\n",
       " 'used',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'and',\n",
       " 'translation',\n",
       " 'invariance',\n",
       " 'have',\n",
       " 'great',\n",
       " 'values',\n",
       " 'in',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'we',\n",
       " 'bring',\n",
       " 'a',\n",
       " 'new',\n",
       " 'architecture',\n",
       " 'in',\n",
       " 'convolutional',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'cnn',\n",
       " 'named',\n",
       " 'cyclic',\n",
       " 'convolutional',\n",
       " 'layer',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'rotation',\n",
       " 'invariance',\n",
       " 'in',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'recognition',\n",
       " 'we',\n",
       " 'can',\n",
       " 'also',\n",
       " 'get',\n",
       " 'the',\n",
       " 'position',\n",
       " 'and',\n",
       " 'orientation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'd',\n",
       " 'symbol',\n",
       " 'by',\n",
       " 'the',\n",
       " 'network',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'detection',\n",
       " 'purpose',\n",
       " 'for',\n",
       " 'multiple',\n",
       " 'non',\n",
       " 'overlap',\n",
       " 'target',\n",
       " 'last',\n",
       " 'but',\n",
       " 'not',\n",
       " 'least',\n",
       " 'this',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'some',\n",
       " 'cases',\n",
       " 'using',\n",
       " 'those',\n",
       " 'invariance',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'and',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'we',\n",
       " 'introduce',\n",
       " 'and',\n",
       " 'develop',\n",
       " 'the',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'are',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'generalisation',\n",
       " 'of',\n",
       " 'spherical',\n",
       " 'harmonics',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'we',\n",
       " 'study',\n",
       " 'the',\n",
       " 'theory',\n",
       " 'of',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'which',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'analogously',\n",
       " 'to',\n",
       " 'zonal',\n",
       " 'harmonics',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'we',\n",
       " 'find',\n",
       " 'the',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'zonal',\n",
       " 'polyharmonics',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gegenbauer',\n",
       " 'polynomials',\n",
       " 'we',\n",
       " 'show',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'poisson',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'harmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ball',\n",
       " 'poisson',\n",
       " 'kernels',\n",
       " 'for',\n",
       " 'polyharmonic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'union',\n",
       " 'of',\n",
       " 'rotated',\n",
       " 'balls',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cauchy',\n",
       " 'hua',\n",
       " 'kernel',\n",
       " 'for',\n",
       " 'holomorphic',\n",
       " 'functions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lie',\n",
       " 'ball',\n",
       " 'a',\n",
       " 'finite',\n",
       " 'element',\n",
       " 'approximation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'maxwell',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'system',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'landau',\n",
       " 'lifshitz',\n",
       " 'gilbert',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'maxwell',\n",
       " 'equations',\n",
       " 'the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'walls',\n",
       " 'and',\n",
       " 'vortices',\n",
       " 'fundamental',\n",
       " 'objects',\n",
       " 'for',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'nanostructured',\n",
       " 'magnetic',\n",
       " 'memories',\n",
       " 'we',\n",
       " 'first',\n",
       " 'reformulate',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'llg',\n",
       " 'equation',\n",
       " 'into',\n",
       " 'an',\n",
       " 'equation',\n",
       " 'with',\n",
       " 'time',\n",
       " 'differentiable',\n",
       " 'solutions',\n",
       " 'we',\n",
       " 'then',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'convergent',\n",
       " 'theta',\n",
       " 'linear',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reformulated',\n",
       " 'system',\n",
       " 'as',\n",
       " 'a',\n",
       " 'consequence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'convergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'solutions',\n",
       " 'with',\n",
       " 'no',\n",
       " 'or',\n",
       " 'minor',\n",
       " 'conditions',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'steps',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'theta',\n",
       " 'hence',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'weak',\n",
       " 'martingale',\n",
       " 'solutions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stochastic',\n",
       " 'mllg',\n",
       " 'system',\n",
       " 'numerical',\n",
       " 'results',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'to',\n",
       " 'show',\n",
       " 'applicability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'method',\n",
       " 'comparative',\n",
       " 'study',\n",
       " 'of',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'and',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'decomposition',\n",
       " 'to',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'of',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'fourier',\n",
       " 'transform',\n",
       " 'infra',\n",
       " 'red',\n",
       " 'ftir',\n",
       " 'spectra',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'from',\n",
       " 'plant',\n",
       " 'species',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'preprocessing',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'on',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'wavelet',\n",
       " 'tensor',\n",
       " 'train',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'discrete',\n",
       " 'wavelet',\n",
       " 'transforms',\n",
       " 'dwt',\n",
       " 'were',\n",
       " 'compared',\n",
       " 'as',\n",
       " 'feature',\n",
       " 'extraction',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'ftir',\n",
       " 'data',\n",
       " 'of',\n",
       " 'medicinal',\n",
       " 'plants',\n",
       " 'various',\n",
       " 'combinations',\n",
       " 'of',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'steps',\n",
       " 'showed',\n",
       " 'different',\n",
       " 'behavior',\n",
       " 'when',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'clustering',\n",
       " 'tasks',\n",
       " 'best',\n",
       " 'results',\n",
       " 'for',\n",
       " 'wtt',\n",
       " 'and',\n",
       " 'dwt',\n",
       " 'found',\n",
       " 'through',\n",
       " 'grid',\n",
       " 'search',\n",
       " 'were',\n",
       " 'similar',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'clustering',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'classification',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'tuned',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'original',\n",
       " 'spectra',\n",
       " 'unlike',\n",
       " 'dwt',\n",
       " 'wtt',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'parameter',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tuned',\n",
       " 'rank',\n",
       " 'making',\n",
       " 'it',\n",
       " 'a',\n",
       " 'more',\n",
       " 'versatile',\n",
       " 'and',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'processing',\n",
       " 'tool',\n",
       " 'in',\n",
       " 'various',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'applications',\n",
       " 'on',\n",
       " 'maximizing',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'complement',\n",
       " 'of',\n",
       " 'an',\n",
       " 'obstacle',\n",
       " 'let',\n",
       " 'omega',\n",
       " 'subset',\n",
       " 'mathbb',\n",
       " 'r',\n",
       " 'n',\n",
       " 'be',\n",
       " 'a',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'satisfying',\n",
       " 'a',\n",
       " 'hayman',\n",
       " 'type',\n",
       " 'asymmetry',\n",
       " 'condition',\n",
       " 'and',\n",
       " 'let',\n",
       " 'd',\n",
       " 'be',\n",
       " 'an',\n",
       " 'arbitrary',\n",
       " 'bounded',\n",
       " 'domain',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'as',\n",
       " 'obstacle',\n",
       " 'we',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'the',\n",
       " 'behaviour',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'eigenvalue',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'first',\n",
       " 'we',\n",
       " 'prove',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " 'x',\n",
       " 'd',\n",
       " 'to',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'dirichlet',\n",
       " 'ground',\n",
       " 'state',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'of',\n",
       " 'omega',\n",
       " 'in',\n",
       " 'short',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'corollary',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " 'begin',\n",
       " 'equation',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'max',\n",
       " 'x',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'setminus',\n",
       " 'x',\n",
       " 'd',\n",
       " 'end',\n",
       " 'equation',\n",
       " 'is',\n",
       " 'large',\n",
       " 'enough',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizer',\n",
       " 'sets',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'are',\n",
       " 'close',\n",
       " 'to',\n",
       " 'each',\n",
       " 'maximum',\n",
       " 'point',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'second',\n",
       " 'we',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'and',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'inscribe',\n",
       " 'wavelength',\n",
       " 'balls',\n",
       " 'at',\n",
       " 'a',\n",
       " 'given',\n",
       " 'point',\n",
       " 'in',\n",
       " 'omega',\n",
       " 'finally',\n",
       " 'we',\n",
       " 'specify',\n",
       " 'our',\n",
       " 'observations',\n",
       " 'to',\n",
       " 'convex',\n",
       " 'obstacles',\n",
       " 'd',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'if',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'is',\n",
       " 'sufficiently',\n",
       " 'large',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'then',\n",
       " 'all',\n",
       " 'maximizers',\n",
       " 'x',\n",
       " 'd',\n",
       " 'of',\n",
       " 'mu',\n",
       " 'omega',\n",
       " 'contain',\n",
       " 'all',\n",
       " 'maximum',\n",
       " 'points',\n",
       " 'x',\n",
       " 'of',\n",
       " 'phi',\n",
       " 'lambda',\n",
       " 'omega',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rotation',\n",
       " 'period',\n",
       " 'and',\n",
       " 'shape',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyperbolic',\n",
       " 'asteroid',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'from',\n",
       " 'its',\n",
       " 'lightcurve',\n",
       " 'we',\n",
       " 'observed',\n",
       " 'the',\n",
       " 'newly',\n",
       " 'discovered',\n",
       " 'hyperbolic',\n",
       " 'minor',\n",
       " 'planet',\n",
       " 'i',\n",
       " 'oumuamua',\n",
       " 'u',\n",
       " 'on',\n",
       " 'october',\n",
       " 'with',\n",
       " 'lowell',\n",
       " 'observatory',\n",
       " 's',\n",
       " 'm',\n",
       " 'discovery',\n",
       " 'channel',\n",
       " 'telescope',\n",
       " 'from',\n",
       " 'these',\n",
       " 'observations',\n",
       " 'we',\n",
       " 'derived',\n",
       " 'a',\n",
       " 'partial',\n",
       " 'lightcurve',\n",
       " 'with',\n",
       " 'peak',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analysing the words in titles and abstracts\n",
    "titles_pr = []\n",
    "abstracts_pr = []\n",
    "for index,row in df.iterrows():\n",
    "    l = row['title_abstract_combined_processed'].split()\n",
    "    m = row['title_abstract_combined_processed'].split()\n",
    "    for el in l:\n",
    "        titles_pr.append(el)\n",
    "    for ele in m:\n",
    "        abstracts_pr.append(ele)\n",
    "display(titles_pr)\n",
    "display(abstracts_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstructing subject specific effect maps pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotation invariance neural network rotation in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spherical polyharmonics poisson kernels polyha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>finite element approximation stochastic maxwel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>comparative study discrete wavelet transforms ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstructing subject specific effect maps pr...  \n",
       "1  rotation invariance neural network rotation in...  \n",
       "2  spherical polyharmonics poisson kernels polyha...  \n",
       "3  finite element approximation stochastic maxwel...  \n",
       "4  comparative study discrete wavelet transforms ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>closed form marginal likelihood gamma poisson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratory mid ir spectra equilibrated igneous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case static amsdu aggregation wlans frame aggr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>gaia eso survey inner disk intermediate age op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>witness functions versus interpretation functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  closed form marginal likelihood gamma poisson ...  \n",
       "1  laboratory mid ir spectra equilibrated igneous...  \n",
       "2  case static amsdu aggregation wlans frame aggr...  \n",
       "3  gaia eso survey inner disk intermediate age op...  \n",
       "4  witness functions versus interpretation functi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#remove stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\"\",sentence)\n",
    "df['title_abstract_combined_processed']=df['title_abstract_combined_processed'].apply(removeStopWords)\n",
    "df_test_2['title_abstract_combined_processed']=df_test_2['title_abstract_combined_processed'].apply(removeStopWords)\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps   ...</td>\n",
       "      <td>reconstruct subject specif effect map predict ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rotation Invariance Neural Network   Rotation ...</td>\n",
       "      <td>rotat invari neural network rotat invari trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>spheric polyharmon poisson kernel polyharmon f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>finit element approxim stochast maxwel landau ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>compar studi discret wavelet transform wavelet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0                 1        0            0           0                     0   \n",
       "1                 1        0            0           0                     0   \n",
       "2                 0        0            1           0                     0   \n",
       "3                 0        0            1           0                     0   \n",
       "4                 1        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance                            title_abstract_combined  \\\n",
       "0                     0  Reconstructing Subject-Specific Effect Maps   ...   \n",
       "1                     0  Rotation Invariance Neural Network   Rotation ...   \n",
       "2                     0  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3                     0  A finite element approximation for the stochas...   \n",
       "4                     0  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  reconstruct subject specif effect map predict ...  \n",
       "1  rotat invari neural network rotat invari trans...  \n",
       "2  spheric polyharmon poisson kernel polyharmon f...  \n",
       "3  finit element approxim stochast maxwel landau ...  \n",
       "4  compar studi discret wavelet transform wavelet...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_abstract_combined</th>\n",
       "      <th>title_abstract_combined_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>close form margin likelihood gamma poisson mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>laboratori mid ir spectra equilibr igneous met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs   F...</td>\n",
       "      <td>case static amsdu aggreg wlan frame aggreg mec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>gaia eso survey inner disk intermedi age open ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>wit function versus interpret function secreci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title_abstract_combined  \\\n",
       "0  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  Case For Static AMSDU Aggregation in WLANs   F...   \n",
       "3  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                   title_abstract_combined_processed  \n",
       "0  close form margin likelihood gamma poisson mat...  \n",
       "1  laboratori mid ir spectra equilibr igneous met...  \n",
       "2  case static amsdu aggreg wlan frame aggreg mec...  \n",
       "3  gaia eso survey inner disk intermedi age open ...  \n",
       "4  wit function versus interpret function secreci...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#do stemming on title and abstract\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "df['title_abstract_combined_processed']=df['title_abstract_combined_processed'].apply(stemming)\n",
    "df_test_2['title_abstract_combined_processed']=df_test_2['title_abstract_combined_processed'].apply(stemming)\n",
    "display(df.head())\n",
    "display(df_test_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import logging\n",
    "logging.basicConfig(format = \"%(levelname)s - %(asctime)s: %(message)s\", datefmt = '%H:%M:%S', level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:18:43: collecting all words and their counts\n",
      "INFO - 12:18:43: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 12:18:45: PROGRESS: at sentence #10000, processed 1020749 words and 570778 word types\n",
      "INFO - 12:18:47: PROGRESS: at sentence #20000, processed 2035896 words and 962748 word types\n",
      "INFO - 12:18:48: collected 996639 word types from a corpus of 2134263 words (unigram + bigrams) and 20972 sentences\n",
      "INFO - 12:18:48: using 996639 counts as vocab in Phrases<0 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 12:18:48: collecting all words and their counts\n",
      "INFO - 12:18:48: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 12:18:50: collected 523371 word types from a corpus of 915660 words (unigram + bigrams) and 8989 sentences\n",
      "INFO - 12:18:50: using 523371 counts as vocab in Phrases<0 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [row.split() for row in df['title_abstract_combined_processed']]\n",
    "sent_test_2 = [row.split() for row in df_test_2['title_abstract_combined_processed']]\n",
    "phrases = Phrases(sent, min_count = 50, progress_per = 10000)\n",
    "phrases_test_2 = Phrases(sent_test_2, min_count = 50, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:18:50: source_vocab length 996639\n",
      "INFO - 12:19:02: Phraser built with 480 phrasegrams\n",
      "INFO - 12:19:02: source_vocab length 523371\n",
      "INFO - 12:19:08: Phraser built with 166 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "bigram_test_2 = Phraser(phrases_test_2)\n",
    "sentences_test_2 = bigram[sent_test_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36007"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'use',\n",
       " 'method',\n",
       " 'data',\n",
       " 'result',\n",
       " 'system',\n",
       " 'base',\n",
       " 'show',\n",
       " 'network',\n",
       " 'problem']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23180"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences_test_2:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'use',\n",
       " 'method',\n",
       " 'data',\n",
       " 'system',\n",
       " 'result',\n",
       " 'base',\n",
       " 'network',\n",
       " 'show',\n",
       " 'algorithm']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "display(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count = 5, window = 2, size = 500, sample = 6e-5, alpha = 0.05, min_alpha = 0.00001, negative = 5, workers = cores-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:19:18: collecting all words and their counts\n",
      "INFO - 12:19:18: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 12:19:21: PROGRESS: at sentence #10000, processed 983758 words, keeping 24853 word types\n",
      "INFO - 12:19:24: PROGRESS: at sentence #20000, processed 1962056 words, keeping 35161 word types\n",
      "INFO - 12:19:24: collected 36007 word types from a corpus of 2056851 raw words and 20972 sentences\n",
      "INFO - 12:19:24: Loading a fresh vocabulary\n",
      "INFO - 12:19:24: effective_min_count=5 retains 13262 unique words (36% of original 36007, drops 22745)\n",
      "INFO - 12:19:24: effective_min_count=5 leaves 2016739 word corpus (98% of original 2056851, drops 40112)\n",
      "INFO - 12:19:24: deleting the raw counts dictionary of 36007 items\n",
      "INFO - 12:19:24: sample=6e-05 downsamples 1245 most-common words\n",
      "INFO - 12:19:24: downsampling leaves estimated 1063097 word corpus (52.7% of prior 2016739)\n",
      "INFO - 12:19:24: estimated required memory for 13262 words and 500 dimensions: 59679000 bytes\n",
      "INFO - 12:19:24: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.15 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per = 10000)\n",
    "print('Time to build vocab: {} mins'.format(round((time()-t)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:19:27: training model with 4 workers on 13262 vocabulary and 500 features, using sg=0 hs=0 sample=6e-05 negative=5 window=2\n",
      "INFO - 12:19:28: EPOCH 1 - PROGRESS: at 13.09% examples, 136664 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:29: EPOCH 1 - PROGRESS: at 28.01% examples, 144107 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:30: EPOCH 1 - PROGRESS: at 43.89% examples, 151453 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:31: EPOCH 1 - PROGRESS: at 59.68% examples, 154442 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:32: EPOCH 1 - PROGRESS: at 75.73% examples, 156428 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:33: EPOCH 1 - PROGRESS: at 92.32% examples, 158149 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:33: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:19:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:19:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:19:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:19:33: EPOCH - 1 : training on 2056851 raw words (1061907 effective words) took 6.7s, 158879 effective words/s\n",
      "INFO - 12:19:34: EPOCH 2 - PROGRESS: at 15.48% examples, 162493 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:19:35: EPOCH 2 - PROGRESS: at 31.38% examples, 165152 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:36: EPOCH 2 - PROGRESS: at 47.21% examples, 165255 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:37: EPOCH 2 - PROGRESS: at 63.08% examples, 164704 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:38: EPOCH 2 - PROGRESS: at 79.00% examples, 165547 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:39: EPOCH 2 - PROGRESS: at 95.66% examples, 166541 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:19:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:19:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:19:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:19:40: EPOCH - 2 : training on 2056851 raw words (1062141 effective words) took 6.4s, 167005 effective words/s\n",
      "INFO - 12:19:41: EPOCH 3 - PROGRESS: at 15.48% examples, 162836 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:19:42: EPOCH 3 - PROGRESS: at 31.38% examples, 165569 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:43: EPOCH 3 - PROGRESS: at 44.37% examples, 155027 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:44: EPOCH 3 - PROGRESS: at 56.78% examples, 148571 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:45: EPOCH 3 - PROGRESS: at 69.90% examples, 145498 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 12:19:46: EPOCH 3 - PROGRESS: at 85.84% examples, 148846 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:47: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:19:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:19:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:19:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:19:47: EPOCH - 3 : training on 2056851 raw words (1062426 effective words) took 7.1s, 150479 effective words/s\n",
      "INFO - 12:19:48: EPOCH 4 - PROGRESS: at 15.48% examples, 161536 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:49: EPOCH 4 - PROGRESS: at 29.42% examples, 153678 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:50: EPOCH 4 - PROGRESS: at 44.88% examples, 157028 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:51: EPOCH 4 - PROGRESS: at 59.20% examples, 155180 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:52: EPOCH 4 - PROGRESS: at 75.22% examples, 157661 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:53: EPOCH 4 - PROGRESS: at 91.30% examples, 158711 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:19:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:19:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:19:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:19:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:19:53: EPOCH - 4 : training on 2056851 raw words (1062838 effective words) took 6.6s, 160433 effective words/s\n",
      "INFO - 12:19:54: EPOCH 5 - PROGRESS: at 13.53% examples, 142865 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:55: EPOCH 5 - PROGRESS: at 28.01% examples, 147343 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:56: EPOCH 5 - PROGRESS: at 40.98% examples, 143494 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:57: EPOCH 5 - PROGRESS: at 52.95% examples, 139758 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:19:59: EPOCH 5 - PROGRESS: at 66.00% examples, 138427 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:00: EPOCH 5 - PROGRESS: at 79.00% examples, 137752 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:01: EPOCH 5 - PROGRESS: at 91.30% examples, 136040 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:01: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:20:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:20:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:20:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:20:01: EPOCH - 5 : training on 2056851 raw words (1063671 effective words) took 7.7s, 137445 effective words/s\n",
      "INFO - 12:20:02: EPOCH 6 - PROGRESS: at 13.09% examples, 133120 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:03: EPOCH 6 - PROGRESS: at 27.55% examples, 142187 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:04: EPOCH 6 - PROGRESS: at 41.99% examples, 145299 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:05: EPOCH 6 - PROGRESS: at 57.24% examples, 149182 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:06: EPOCH 6 - PROGRESS: at 69.90% examples, 144981 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:07: EPOCH 6 - PROGRESS: at 85.35% examples, 148032 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:20:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:20:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:20:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:20:08: EPOCH - 6 : training on 2056851 raw words (1062403 effective words) took 7.0s, 151098 effective words/s\n",
      "INFO - 12:20:09: EPOCH 7 - PROGRESS: at 15.48% examples, 162792 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:10: EPOCH 7 - PROGRESS: at 30.92% examples, 162603 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:11: EPOCH 7 - PROGRESS: at 46.27% examples, 161238 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:12: EPOCH 7 - PROGRESS: at 60.66% examples, 158633 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:20:13: EPOCH 7 - PROGRESS: at 73.26% examples, 152440 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:14: EPOCH 7 - PROGRESS: at 85.84% examples, 148855 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:15: EPOCH 7 - PROGRESS: at 97.58% examples, 145221 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:20:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:20:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:20:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:20:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:20:16: EPOCH - 7 : training on 2056851 raw words (1063557 effective words) took 7.3s, 145477 effective words/s\n",
      "INFO - 12:20:17: EPOCH 8 - PROGRESS: at 10.17% examples, 106388 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:18: EPOCH 8 - PROGRESS: at 22.69% examples, 119012 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:19: EPOCH 8 - PROGRESS: at 33.82% examples, 117463 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:20: EPOCH 8 - PROGRESS: at 46.27% examples, 121317 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:20:21: EPOCH 8 - PROGRESS: at 57.72% examples, 121686 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:22: EPOCH 8 - PROGRESS: at 73.26% examples, 128715 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:23: EPOCH 8 - PROGRESS: at 87.86% examples, 131903 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:20:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:20:24: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:20:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:20:24: EPOCH - 8 : training on 2056851 raw words (1062504 effective words) took 8.0s, 133060 effective words/s\n",
      "INFO - 12:20:25: EPOCH 9 - PROGRESS: at 11.16% examples, 114810 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:26: EPOCH 9 - PROGRESS: at 19.79% examples, 100827 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:27: EPOCH 9 - PROGRESS: at 30.42% examples, 103621 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:28: EPOCH 9 - PROGRESS: at 43.43% examples, 111779 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:20:29: EPOCH 9 - PROGRESS: at 54.39% examples, 112731 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:30: EPOCH 9 - PROGRESS: at 65.55% examples, 113226 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:31: EPOCH 9 - PROGRESS: at 77.15% examples, 114298 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:20:32: EPOCH 9 - PROGRESS: at 88.36% examples, 114662 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:33: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:20:33: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:20:33: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:20:33: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:20:33: EPOCH - 9 : training on 2056851 raw words (1062358 effective words) took 9.1s, 116569 effective words/s\n",
      "INFO - 12:20:34: EPOCH 10 - PROGRESS: at 13.53% examples, 140185 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:35: EPOCH 10 - PROGRESS: at 25.62% examples, 134252 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:36: EPOCH 10 - PROGRESS: at 39.11% examples, 135913 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:20:37: EPOCH 10 - PROGRESS: at 54.85% examples, 143257 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:38: EPOCH 10 - PROGRESS: at 71.39% examples, 149416 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:39: EPOCH 10 - PROGRESS: at 87.36% examples, 152021 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:20:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:20:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:20:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:20:40: EPOCH - 10 : training on 2056851 raw words (1062219 effective words) took 6.9s, 154417 effective words/s\n",
      "INFO - 12:20:41: EPOCH 11 - PROGRESS: at 15.48% examples, 160957 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:42: EPOCH 11 - PROGRESS: at 31.89% examples, 164839 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:20:43: EPOCH 11 - PROGRESS: at 48.17% examples, 166191 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:44: EPOCH 11 - PROGRESS: at 64.56% examples, 168094 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:20:45: EPOCH 11 - PROGRESS: at 80.95% examples, 167845 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:46: EPOCH 11 - PROGRESS: at 97.58% examples, 168549 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:20:46: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:20:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:20:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:20:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:20:46: EPOCH - 11 : training on 2056851 raw words (1062415 effective words) took 6.3s, 168749 effective words/s\n",
      "INFO - 12:20:47: EPOCH 12 - PROGRESS: at 15.48% examples, 160849 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:48: EPOCH 12 - PROGRESS: at 28.01% examples, 146385 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:49: EPOCH 12 - PROGRESS: at 44.37% examples, 153010 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:50: EPOCH 12 - PROGRESS: at 58.68% examples, 152615 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:51: EPOCH 12 - PROGRESS: at 72.78% examples, 151505 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:20:52: EPOCH 12 - PROGRESS: at 87.86% examples, 152172 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:20:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:20:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:20:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:20:53: EPOCH - 12 : training on 2056851 raw words (1063730 effective words) took 7.0s, 151117 effective words/s\n",
      "INFO - 12:20:54: EPOCH 13 - PROGRESS: at 12.62% examples, 133166 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:55: EPOCH 13 - PROGRESS: at 25.15% examples, 130139 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:56: EPOCH 13 - PROGRESS: at 36.21% examples, 124916 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:57: EPOCH 13 - PROGRESS: at 48.17% examples, 125377 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:58: EPOCH 13 - PROGRESS: at 62.60% examples, 130459 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:20:59: EPOCH 13 - PROGRESS: at 78.55% examples, 135912 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:00: EPOCH 13 - PROGRESS: at 93.75% examples, 138916 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:00: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:00: EPOCH - 13 : training on 2056851 raw words (1062839 effective words) took 7.6s, 140091 effective words/s\n",
      "INFO - 12:21:02: EPOCH 14 - PROGRESS: at 14.05% examples, 147716 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:03: EPOCH 14 - PROGRESS: at 28.01% examples, 146929 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:04: EPOCH 14 - PROGRESS: at 40.05% examples, 140262 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:05: EPOCH 14 - PROGRESS: at 53.41% examples, 140437 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:06: EPOCH 14 - PROGRESS: at 66.96% examples, 140180 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:07: EPOCH 14 - PROGRESS: at 79.00% examples, 137623 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:08: EPOCH 14 - PROGRESS: at 93.75% examples, 139664 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:08: EPOCH - 14 : training on 2056851 raw words (1063112 effective words) took 7.5s, 140824 effective words/s\n",
      "INFO - 12:21:09: EPOCH 15 - PROGRESS: at 15.01% examples, 155489 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:21:10: EPOCH 15 - PROGRESS: at 31.38% examples, 164206 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:11: EPOCH 15 - PROGRESS: at 46.71% examples, 163700 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:12: EPOCH 15 - PROGRESS: at 63.08% examples, 165636 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:13: EPOCH 15 - PROGRESS: at 79.47% examples, 165658 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:21:14: EPOCH 15 - PROGRESS: at 96.16% examples, 166515 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:14: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:14: EPOCH - 15 : training on 2056851 raw words (1062814 effective words) took 6.4s, 166911 effective words/s\n",
      "INFO - 12:21:15: EPOCH 16 - PROGRESS: at 15.01% examples, 158149 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:16: EPOCH 16 - PROGRESS: at 31.38% examples, 164712 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:18: EPOCH 16 - PROGRESS: at 44.88% examples, 154615 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:21:19: EPOCH 16 - PROGRESS: at 61.15% examples, 158644 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:20: EPOCH 16 - PROGRESS: at 76.21% examples, 158001 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:21: EPOCH 16 - PROGRESS: at 91.30% examples, 158111 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:21:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:21: EPOCH - 16 : training on 2056851 raw words (1063293 effective words) took 6.7s, 159315 effective words/s\n",
      "INFO - 12:21:22: EPOCH 17 - PROGRESS: at 15.01% examples, 158820 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:23: EPOCH 17 - PROGRESS: at 29.42% examples, 154617 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:21:24: EPOCH 17 - PROGRESS: at 44.37% examples, 154059 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:25: EPOCH 17 - PROGRESS: at 60.17% examples, 157506 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:21:26: EPOCH 17 - PROGRESS: at 76.21% examples, 159826 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:21:27: EPOCH 17 - PROGRESS: at 92.32% examples, 160994 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:28: EPOCH - 17 : training on 2056851 raw words (1062952 effective words) took 6.6s, 161922 effective words/s\n",
      "INFO - 12:21:29: EPOCH 18 - PROGRESS: at 15.48% examples, 163149 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:30: EPOCH 18 - PROGRESS: at 31.38% examples, 166091 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:21:31: EPOCH 18 - PROGRESS: at 47.21% examples, 166912 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:21:32: EPOCH 18 - PROGRESS: at 63.56% examples, 168081 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:33: EPOCH 18 - PROGRESS: at 79.98% examples, 168431 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:34: EPOCH 18 - PROGRESS: at 96.16% examples, 168008 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:34: EPOCH - 18 : training on 2056851 raw words (1063445 effective words) took 6.3s, 168705 effective words/s\n",
      "INFO - 12:21:35: EPOCH 19 - PROGRESS: at 15.48% examples, 161661 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:36: EPOCH 19 - PROGRESS: at 30.42% examples, 159236 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:37: EPOCH 19 - PROGRESS: at 46.71% examples, 161681 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:38: EPOCH 19 - PROGRESS: at 63.08% examples, 164062 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:21:39: EPOCH 19 - PROGRESS: at 78.05% examples, 162193 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 12:21:40: EPOCH 19 - PROGRESS: at 94.25% examples, 163307 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:41: EPOCH - 19 : training on 2056851 raw words (1062696 effective words) took 6.5s, 164240 effective words/s\n",
      "INFO - 12:21:42: EPOCH 20 - PROGRESS: at 14.51% examples, 150076 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:43: EPOCH 20 - PROGRESS: at 30.42% examples, 156884 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:44: EPOCH 20 - PROGRESS: at 46.71% examples, 160893 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:21:45: EPOCH 20 - PROGRESS: at 62.13% examples, 161581 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:46: EPOCH 20 - PROGRESS: at 78.55% examples, 163305 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:47: EPOCH 20 - PROGRESS: at 95.18% examples, 165027 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:47: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:47: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:47: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:47: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:47: EPOCH - 20 : training on 2056851 raw words (1062641 effective words) took 6.4s, 165032 effective words/s\n",
      "INFO - 12:21:48: EPOCH 21 - PROGRESS: at 15.48% examples, 158386 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:49: EPOCH 21 - PROGRESS: at 31.89% examples, 164732 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:50: EPOCH 21 - PROGRESS: at 48.17% examples, 167234 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:51: EPOCH 21 - PROGRESS: at 64.05% examples, 166494 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:52: EPOCH 21 - PROGRESS: at 79.47% examples, 165332 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:53: EPOCH 21 - PROGRESS: at 95.66% examples, 165827 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:21:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:21:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:21:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:21:53: EPOCH - 21 : training on 2056851 raw words (1062306 effective words) took 6.4s, 166321 effective words/s\n",
      "INFO - 12:21:54: EPOCH 22 - PROGRESS: at 15.01% examples, 157015 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:55: EPOCH 22 - PROGRESS: at 29.95% examples, 154708 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:21:56: EPOCH 22 - PROGRESS: at 45.34% examples, 157699 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:57: EPOCH 22 - PROGRESS: at 61.15% examples, 160377 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:21:58: EPOCH 22 - PROGRESS: at 77.15% examples, 160794 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:21:59: EPOCH 22 - PROGRESS: at 93.75% examples, 162967 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:22:00: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:00: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:00: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:00: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:00: EPOCH - 22 : training on 2056851 raw words (1062769 effective words) took 6.5s, 163187 effective words/s\n",
      "INFO - 12:22:01: EPOCH 23 - PROGRESS: at 15.48% examples, 160144 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:02: EPOCH 23 - PROGRESS: at 31.38% examples, 164058 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:03: EPOCH 23 - PROGRESS: at 47.70% examples, 165474 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:22:04: EPOCH 23 - PROGRESS: at 64.56% examples, 168157 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:05: EPOCH 23 - PROGRESS: at 80.47% examples, 167026 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:06: EPOCH 23 - PROGRESS: at 96.64% examples, 167407 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:06: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:06: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:06: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:06: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:06: EPOCH - 23 : training on 2056851 raw words (1063698 effective words) took 6.3s, 167744 effective words/s\n",
      "INFO - 12:22:07: EPOCH 24 - PROGRESS: at 14.51% examples, 151140 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:22:08: EPOCH 24 - PROGRESS: at 29.95% examples, 157201 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:09: EPOCH 24 - PROGRESS: at 45.81% examples, 160451 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:10: EPOCH 24 - PROGRESS: at 60.17% examples, 157727 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:22:11: EPOCH 24 - PROGRESS: at 73.77% examples, 154695 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:12: EPOCH 24 - PROGRESS: at 86.86% examples, 151470 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:13: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:13: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:22:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:13: EPOCH - 24 : training on 2056851 raw words (1063587 effective words) took 7.0s, 151432 effective words/s\n",
      "INFO - 12:22:14: EPOCH 25 - PROGRESS: at 14.05% examples, 142313 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:15: EPOCH 25 - PROGRESS: at 27.55% examples, 141331 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:16: EPOCH 25 - PROGRESS: at 42.95% examples, 148513 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:17: EPOCH 25 - PROGRESS: at 56.30% examples, 146193 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:18: EPOCH 25 - PROGRESS: at 69.44% examples, 144302 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:19: EPOCH 25 - PROGRESS: at 82.92% examples, 143869 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:20: EPOCH 25 - PROGRESS: at 97.58% examples, 145131 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:22:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:21: EPOCH - 25 : training on 2056851 raw words (1063297 effective words) took 7.3s, 144843 effective words/s\n",
      "INFO - 12:22:22: EPOCH 26 - PROGRESS: at 11.63% examples, 121581 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:23: EPOCH 26 - PROGRESS: at 23.20% examples, 121129 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:24: EPOCH 26 - PROGRESS: at 32.86% examples, 113083 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:25: EPOCH 26 - PROGRESS: at 42.95% examples, 111134 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:26: EPOCH 26 - PROGRESS: at 52.44% examples, 109270 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:27: EPOCH 26 - PROGRESS: at 65.04% examples, 112784 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:28: EPOCH 26 - PROGRESS: at 74.74% examples, 111163 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:29: EPOCH 26 - PROGRESS: at 87.36% examples, 113417 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:30: EPOCH - 26 : training on 2056851 raw words (1063475 effective words) took 9.1s, 116534 effective words/s\n",
      "INFO - 12:22:31: EPOCH 27 - PROGRESS: at 10.66% examples, 109086 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:32: EPOCH 27 - PROGRESS: at 21.76% examples, 112887 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:22:33: EPOCH 27 - PROGRESS: at 32.86% examples, 113800 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:34: EPOCH 27 - PROGRESS: at 45.34% examples, 116357 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:35: EPOCH 27 - PROGRESS: at 55.83% examples, 115380 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:36: EPOCH 27 - PROGRESS: at 66.48% examples, 114822 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:37: EPOCH 27 - PROGRESS: at 78.05% examples, 116051 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:38: EPOCH 27 - PROGRESS: at 91.30% examples, 118799 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:39: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:39: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:39: EPOCH - 27 : training on 2056851 raw words (1063603 effective words) took 8.8s, 120884 effective words/s\n",
      "INFO - 12:22:40: EPOCH 28 - PROGRESS: at 11.63% examples, 120920 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:41: EPOCH 28 - PROGRESS: at 24.18% examples, 125847 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:42: EPOCH 28 - PROGRESS: at 38.63% examples, 133588 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:43: EPOCH 28 - PROGRESS: at 51.02% examples, 133387 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:44: EPOCH 28 - PROGRESS: at 64.05% examples, 134260 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:22:45: EPOCH 28 - PROGRESS: at 80.47% examples, 140445 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:46: EPOCH 28 - PROGRESS: at 93.75% examples, 138965 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:22:46: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:46: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:46: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:46: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:46: EPOCH - 28 : training on 2056851 raw words (1062781 effective words) took 7.5s, 140816 effective words/s\n",
      "INFO - 12:22:47: EPOCH 29 - PROGRESS: at 14.51% examples, 153060 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:48: EPOCH 29 - PROGRESS: at 30.42% examples, 160784 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:49: EPOCH 29 - PROGRESS: at 44.88% examples, 155504 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:50: EPOCH 29 - PROGRESS: at 57.72% examples, 150114 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:51: EPOCH 29 - PROGRESS: at 73.77% examples, 153218 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:52: EPOCH 29 - PROGRESS: at 88.36% examples, 152893 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:53: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:53: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:53: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:53: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:53: EPOCH - 29 : training on 2056851 raw words (1062775 effective words) took 6.9s, 154637 effective words/s\n",
      "INFO - 12:22:54: EPOCH 30 - PROGRESS: at 15.48% examples, 161598 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:55: EPOCH 30 - PROGRESS: at 31.38% examples, 163068 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:22:56: EPOCH 30 - PROGRESS: at 46.71% examples, 162305 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 12:22:57: EPOCH 30 - PROGRESS: at 62.60% examples, 163951 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:22:58: EPOCH 30 - PROGRESS: at 78.55% examples, 164708 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 12:22:59: EPOCH 30 - PROGRESS: at 94.72% examples, 164805 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 12:22:59: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 12:22:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 12:22:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 12:22:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 12:22:59: EPOCH - 30 : training on 2056851 raw words (1063781 effective words) took 6.4s, 165308 effective words/s\n",
      "INFO - 12:22:59: training on a 61705530 raw words (31888033 effective words) took 212.8s, 149859 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 3.55 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 30, report_delay = 1)\n",
    "print('Time to build vocab: {} mins'.format(round((time()-t)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 12:22:59: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reconstruct': <gensim.models.keyedvectors.Vocab at 0x1e451850a00>,\n",
       " 'subject': <gensim.models.keyedvectors.Vocab at 0x1e451850100>,\n",
       " 'specif': <gensim.models.keyedvectors.Vocab at 0x1e451850be0>,\n",
       " 'effect': <gensim.models.keyedvectors.Vocab at 0x1e451850b80>,\n",
       " 'map': <gensim.models.keyedvectors.Vocab at 0x1e451850940>,\n",
       " 'predict': <gensim.models.keyedvectors.Vocab at 0x1e4518501f0>,\n",
       " 'model': <gensim.models.keyedvectors.Vocab at 0x1e451850c40>,\n",
       " 'allow': <gensim.models.keyedvectors.Vocab at 0x1e451850ac0>,\n",
       " 'infer': <gensim.models.keyedvectors.Vocab at 0x1e451850bb0>,\n",
       " 'analyz': <gensim.models.keyedvectors.Vocab at 0x1e451850eb0>,\n",
       " 'diseas': <gensim.models.keyedvectors.Vocab at 0x1e451850f70>,\n",
       " 'relat': <gensim.models.keyedvectors.Vocab at 0x1e451850d00>,\n",
       " 'alter': <gensim.models.keyedvectors.Vocab at 0x1e4518509d0>,\n",
       " 'neuroimag': <gensim.models.keyedvectors.Vocab at 0x1e451850ca0>,\n",
       " 'data': <gensim.models.keyedvectors.Vocab at 0x1e451850af0>,\n",
       " 'given': <gensim.models.keyedvectors.Vocab at 0x1e451850910>,\n",
       " 'made': <gensim.models.keyedvectors.Vocab at 0x1e451850a90>,\n",
       " 'two': <gensim.models.keyedvectors.Vocab at 0x1e451850b50>,\n",
       " 'level': <gensim.models.keyedvectors.Vocab at 0x1e45189bdc0>,\n",
       " 'global': <gensim.models.keyedvectors.Vocab at 0x1e45189bf40>,\n",
       " 'e': <gensim.models.keyedvectors.Vocab at 0x1e45189bb80>,\n",
       " 'condit': <gensim.models.keyedvectors.Vocab at 0x1e45189bd90>,\n",
       " 'presenc': <gensim.models.keyedvectors.Vocab at 0x1e45189b490>,\n",
       " 'local': <gensim.models.keyedvectors.Vocab at 0x1e45189ba00>,\n",
       " 'detect': <gensim.models.keyedvectors.Vocab at 0x1e45189bb50>,\n",
       " 'individu': <gensim.models.keyedvectors.Vocab at 0x1e45189b9a0>,\n",
       " 'measur': <gensim.models.keyedvectors.Vocab at 0x1e45189b610>,\n",
       " 'extract': <gensim.models.keyedvectors.Vocab at 0x1e45189bc10>,\n",
       " 'wide_use': <gensim.models.keyedvectors.Vocab at 0x1e45189bdf0>,\n",
       " 'use': <gensim.models.keyedvectors.Vocab at 0x1e45189b9d0>,\n",
       " 'form': <gensim.models.keyedvectors.Vocab at 0x1e45189b670>,\n",
       " 'rare': <gensim.models.keyedvectors.Vocab at 0x1e45189b1f0>,\n",
       " 'exist': <gensim.models.keyedvectors.Vocab at 0x1e45189bf10>,\n",
       " 'often': <gensim.models.keyedvectors.Vocab at 0x1e45189b340>,\n",
       " 'yield': <gensim.models.keyedvectors.Vocab at 0x1e45189bbe0>,\n",
       " 'noisi': <gensim.models.keyedvectors.Vocab at 0x1e45189bd30>,\n",
       " 'compos': <gensim.models.keyedvectors.Vocab at 0x1e45189bca0>,\n",
       " 'dispers': <gensim.models.keyedvectors.Vocab at 0x1e45189bc40>,\n",
       " 'isol': <gensim.models.keyedvectors.Vocab at 0x1e45189b790>,\n",
       " 'island': <gensim.models.keyedvectors.Vocab at 0x1e45189b040>,\n",
       " 'articl': <gensim.models.keyedvectors.Vocab at 0x1e45189bb20>,\n",
       " 'propos': <gensim.models.keyedvectors.Vocab at 0x1e45189b2e0>,\n",
       " 'method': <gensim.models.keyedvectors.Vocab at 0x1e45189b0a0>,\n",
       " 'name': <gensim.models.keyedvectors.Vocab at 0x1e45189bee0>,\n",
       " 'rsm': <gensim.models.keyedvectors.Vocab at 0x1e45189bc70>,\n",
       " 'improv': <gensim.models.keyedvectors.Vocab at 0x1e45189b880>,\n",
       " 'approach': <gensim.models.keyedvectors.Vocab at 0x1e45189b730>,\n",
       " 'particular': <gensim.models.keyedvectors.Vocab at 0x1e45189b250>,\n",
       " 'binari': <gensim.models.keyedvectors.Vocab at 0x1e45189b820>,\n",
       " 'classifi': <gensim.models.keyedvectors.Vocab at 0x1e45189b5e0>,\n",
       " 'aim': <gensim.models.keyedvectors.Vocab at 0x1e45189b850>,\n",
       " 'reduc': <gensim.models.keyedvectors.Vocab at 0x1e45189b7c0>,\n",
       " 'nois': <gensim.models.keyedvectors.Vocab at 0x1e45189b6a0>,\n",
       " 'due': <gensim.models.keyedvectors.Vocab at 0x1e45189b100>,\n",
       " 'sampl': <gensim.models.keyedvectors.Vocab at 0x1e45189b220>,\n",
       " 'error': <gensim.models.keyedvectors.Vocab at 0x1e45189bd00>,\n",
       " 'associ': <gensim.models.keyedvectors.Vocab at 0x1e45189b190>,\n",
       " 'finit': <gensim.models.keyedvectors.Vocab at 0x1e45189b550>,\n",
       " 'exampl': <gensim.models.keyedvectors.Vocab at 0x1e45189b310>,\n",
       " 'train': <gensim.models.keyedvectors.Vocab at 0x1e45174b220>,\n",
       " 'wrapper': <gensim.models.keyedvectors.Vocab at 0x1e45174b790>,\n",
       " 'type': <gensim.models.keyedvectors.Vocab at 0x1e45174bd30>,\n",
       " 'algorithm': <gensim.models.keyedvectors.Vocab at 0x1e45174b4f0>,\n",
       " 'differ': <gensim.models.keyedvectors.Vocab at 0x1e45174b1f0>,\n",
       " 'diagnost': <gensim.models.keyedvectors.Vocab at 0x1e45174bf70>,\n",
       " 'manner': <gensim.models.keyedvectors.Vocab at 0x1e45174b190>,\n",
       " 'without': <gensim.models.keyedvectors.Vocab at 0x1e45174b670>,\n",
       " 'inform': <gensim.models.keyedvectors.Vocab at 0x1e45174b130>,\n",
       " 'pose': <gensim.models.keyedvectors.Vocab at 0x1e453560040>,\n",
       " 'maximum': <gensim.models.keyedvectors.Vocab at 0x1e4535600a0>,\n",
       " 'posteriori': <gensim.models.keyedvectors.Vocab at 0x1e453560100>,\n",
       " 'problem': <gensim.models.keyedvectors.Vocab at 0x1e453560160>,\n",
       " 'prior': <gensim.models.keyedvectors.Vocab at 0x1e4535601c0>,\n",
       " 'whose': <gensim.models.keyedvectors.Vocab at 0x1e453560220>,\n",
       " 'paramet': <gensim.models.keyedvectors.Vocab at 0x1e453560280>,\n",
       " 'estim': <gensim.models.keyedvectors.Vocab at 0x1e4535602e0>,\n",
       " 'fashion': <gensim.models.keyedvectors.Vocab at 0x1e453560340>,\n",
       " 'experiment': <gensim.models.keyedvectors.Vocab at 0x1e4535603a0>,\n",
       " 'evalu': <gensim.models.keyedvectors.Vocab at 0x1e453560400>,\n",
       " 'perform': <gensim.models.keyedvectors.Vocab at 0x1e453560460>,\n",
       " 'synthet': <gensim.models.keyedvectors.Vocab at 0x1e4535604c0>,\n",
       " 'generat': <gensim.models.keyedvectors.Vocab at 0x1e453560520>,\n",
       " 'alzheim': <gensim.models.keyedvectors.Vocab at 0x1e453560580>,\n",
       " 'initi': <gensim.models.keyedvectors.Vocab at 0x1e4535605e0>,\n",
       " 'adni': <gensim.models.keyedvectors.Vocab at 0x1e453560640>,\n",
       " 'databas': <gensim.models.keyedvectors.Vocab at 0x1e4535606a0>,\n",
       " 'result': <gensim.models.keyedvectors.Vocab at 0x1e453560700>,\n",
       " 'synthet_data': <gensim.models.keyedvectors.Vocab at 0x1e453560760>,\n",
       " 'demonstr': <gensim.models.keyedvectors.Vocab at 0x1e4535607c0>,\n",
       " 'higher': <gensim.models.keyedvectors.Vocab at 0x1e453560820>,\n",
       " 'accuraci': <gensim.models.keyedvectors.Vocab at 0x1e453560880>,\n",
       " 'compar': <gensim.models.keyedvectors.Vocab at 0x1e4535608e0>,\n",
       " 'direct': <gensim.models.keyedvectors.Vocab at 0x1e453560940>,\n",
       " 'bootstrap': <gensim.models.keyedvectors.Vocab at 0x1e4535609a0>,\n",
       " 'averag': <gensim.models.keyedvectors.Vocab at 0x1e453560a00>,\n",
       " 'analys': <gensim.models.keyedvectors.Vocab at 0x1e453560a60>,\n",
       " 'dataset': <gensim.models.keyedvectors.Vocab at 0x1e453560ac0>,\n",
       " 'show': <gensim.models.keyedvectors.Vocab at 0x1e453560b20>,\n",
       " 'also': <gensim.models.keyedvectors.Vocab at 0x1e453560b80>,\n",
       " 'correl': <gensim.models.keyedvectors.Vocab at 0x1e453560be0>,\n",
       " 'cortic': <gensim.models.keyedvectors.Vocab at 0x1e453560c40>,\n",
       " 'thick': <gensim.models.keyedvectors.Vocab at 0x1e453560ca0>,\n",
       " 'non': <gensim.models.keyedvectors.Vocab at 0x1e453560d00>,\n",
       " 'imag': <gensim.models.keyedvectors.Vocab at 0x1e453560d60>,\n",
       " 'marker': <gensim.models.keyedvectors.Vocab at 0x1e453560dc0>,\n",
       " 'ad': <gensim.models.keyedvectors.Vocab at 0x1e453560e20>,\n",
       " 'mini': <gensim.models.keyedvectors.Vocab at 0x1e453560e80>,\n",
       " 'mental': <gensim.models.keyedvectors.Vocab at 0x1e453560ee0>,\n",
       " 'state': <gensim.models.keyedvectors.Vocab at 0x1e453560f40>,\n",
       " 'examin': <gensim.models.keyedvectors.Vocab at 0x1e453560fa0>,\n",
       " 'score': <gensim.models.keyedvectors.Vocab at 0x1e453565040>,\n",
       " 'fluid': <gensim.models.keyedvectors.Vocab at 0x1e4535650a0>,\n",
       " 'amyloid': <gensim.models.keyedvectors.Vocab at 0x1e453565100>,\n",
       " 'beta': <gensim.models.keyedvectors.Vocab at 0x1e453565160>,\n",
       " 'reliabl': <gensim.models.keyedvectors.Vocab at 0x1e4535651c0>,\n",
       " 'studi': <gensim.models.keyedvectors.Vocab at 0x1e453565220>,\n",
       " 'longitudin': <gensim.models.keyedvectors.Vocab at 0x1e453565280>,\n",
       " 'rotat': <gensim.models.keyedvectors.Vocab at 0x1e4535652e0>,\n",
       " 'invari': <gensim.models.keyedvectors.Vocab at 0x1e453565340>,\n",
       " 'neural_network': <gensim.models.keyedvectors.Vocab at 0x1e4535653a0>,\n",
       " 'translat': <gensim.models.keyedvectors.Vocab at 0x1e453565400>,\n",
       " 'great': <gensim.models.keyedvectors.Vocab at 0x1e453565460>,\n",
       " 'valu': <gensim.models.keyedvectors.Vocab at 0x1e4535654c0>,\n",
       " 'recognit': <gensim.models.keyedvectors.Vocab at 0x1e453565520>,\n",
       " 'task': <gensim.models.keyedvectors.Vocab at 0x1e453565580>,\n",
       " 'paper': <gensim.models.keyedvectors.Vocab at 0x1e4535655e0>,\n",
       " 'bring': <gensim.models.keyedvectors.Vocab at 0x1e453565640>,\n",
       " 'new': <gensim.models.keyedvectors.Vocab at 0x1e4535656a0>,\n",
       " 'architectur': <gensim.models.keyedvectors.Vocab at 0x1e453565700>,\n",
       " 'convolut_neural': <gensim.models.keyedvectors.Vocab at 0x1e453565760>,\n",
       " 'network_cnn': <gensim.models.keyedvectors.Vocab at 0x1e4535657c0>,\n",
       " 'cyclic': <gensim.models.keyedvectors.Vocab at 0x1e453565820>,\n",
       " 'convolut': <gensim.models.keyedvectors.Vocab at 0x1e453565880>,\n",
       " 'layer': <gensim.models.keyedvectors.Vocab at 0x1e4535658e0>,\n",
       " 'achiev': <gensim.models.keyedvectors.Vocab at 0x1e453565940>,\n",
       " 'symbol': <gensim.models.keyedvectors.Vocab at 0x1e4535659a0>,\n",
       " 'get': <gensim.models.keyedvectors.Vocab at 0x1e453565a00>,\n",
       " 'posit': <gensim.models.keyedvectors.Vocab at 0x1e453565a60>,\n",
       " 'orient': <gensim.models.keyedvectors.Vocab at 0x1e453565ac0>,\n",
       " 'network': <gensim.models.keyedvectors.Vocab at 0x1e453565b20>,\n",
       " 'purpos': <gensim.models.keyedvectors.Vocab at 0x1e453565b80>,\n",
       " 'multipl': <gensim.models.keyedvectors.Vocab at 0x1e453565be0>,\n",
       " 'overlap': <gensim.models.keyedvectors.Vocab at 0x1e453565c40>,\n",
       " 'target': <gensim.models.keyedvectors.Vocab at 0x1e453565ca0>,\n",
       " 'last': <gensim.models.keyedvectors.Vocab at 0x1e453565d00>,\n",
       " 'least': <gensim.models.keyedvectors.Vocab at 0x1e453565d60>,\n",
       " 'one': <gensim.models.keyedvectors.Vocab at 0x1e453565dc0>,\n",
       " 'shot_learn': <gensim.models.keyedvectors.Vocab at 0x1e453565e20>,\n",
       " 'case': <gensim.models.keyedvectors.Vocab at 0x1e453565e80>,\n",
       " 'spheric': <gensim.models.keyedvectors.Vocab at 0x1e453565ee0>,\n",
       " 'polyharmon': <gensim.models.keyedvectors.Vocab at 0x1e453565f40>,\n",
       " 'poisson': <gensim.models.keyedvectors.Vocab at 0x1e45174b610>,\n",
       " 'kernel': <gensim.models.keyedvectors.Vocab at 0x1e47540a490>,\n",
       " 'function': <gensim.models.keyedvectors.Vocab at 0x1e479fabbb0>,\n",
       " 'introduc': <gensim.models.keyedvectors.Vocab at 0x1e453565fa0>,\n",
       " 'develop': <gensim.models.keyedvectors.Vocab at 0x1e453566040>,\n",
       " 'notion': <gensim.models.keyedvectors.Vocab at 0x1e4535660a0>,\n",
       " 'natur': <gensim.models.keyedvectors.Vocab at 0x1e453566100>,\n",
       " 'generalis': <gensim.models.keyedvectors.Vocab at 0x1e453566160>,\n",
       " 'harmon': <gensim.models.keyedvectors.Vocab at 0x1e4535661c0>,\n",
       " 'theori': <gensim.models.keyedvectors.Vocab at 0x1e453566220>,\n",
       " 'zonal': <gensim.models.keyedvectors.Vocab at 0x1e453566280>,\n",
       " 'allow_us': <gensim.models.keyedvectors.Vocab at 0x1e4535662e0>,\n",
       " 'analog': <gensim.models.keyedvectors.Vocab at 0x1e453566340>,\n",
       " 'construct': <gensim.models.keyedvectors.Vocab at 0x1e4535663a0>,\n",
       " 'union': <gensim.models.keyedvectors.Vocab at 0x1e453566400>,\n",
       " 'ball': <gensim.models.keyedvectors.Vocab at 0x1e453566460>,\n",
       " 'find': <gensim.models.keyedvectors.Vocab at 0x1e4535664c0>,\n",
       " 'represent': <gensim.models.keyedvectors.Vocab at 0x1e453566520>,\n",
       " 'term': <gensim.models.keyedvectors.Vocab at 0x1e453566580>,\n",
       " 'gegenbau': <gensim.models.keyedvectors.Vocab at 0x1e4535665e0>,\n",
       " 'polynomi': <gensim.models.keyedvectors.Vocab at 0x1e453566640>,\n",
       " 'connect': <gensim.models.keyedvectors.Vocab at 0x1e4535666a0>,\n",
       " 'classic': <gensim.models.keyedvectors.Vocab at 0x1e453566700>,\n",
       " 'cauchi': <gensim.models.keyedvectors.Vocab at 0x1e453566760>,\n",
       " 'holomorph': <gensim.models.keyedvectors.Vocab at 0x1e4535667c0>,\n",
       " 'lie': <gensim.models.keyedvectors.Vocab at 0x1e453566820>,\n",
       " 'finit_element': <gensim.models.keyedvectors.Vocab at 0x1e453566880>,\n",
       " 'approxim': <gensim.models.keyedvectors.Vocab at 0x1e4535668e0>,\n",
       " 'stochast': <gensim.models.keyedvectors.Vocab at 0x1e453566940>,\n",
       " 'maxwel': <gensim.models.keyedvectors.Vocab at 0x1e4535669a0>,\n",
       " 'landau': <gensim.models.keyedvectors.Vocab at 0x1e453566a00>,\n",
       " 'lifshitz': <gensim.models.keyedvectors.Vocab at 0x1e453566a60>,\n",
       " 'gilbert': <gensim.models.keyedvectors.Vocab at 0x1e453566ac0>,\n",
       " 'system': <gensim.models.keyedvectors.Vocab at 0x1e453566b20>,\n",
       " 'equat': <gensim.models.keyedvectors.Vocab at 0x1e453566b80>,\n",
       " 'coupl': <gensim.models.keyedvectors.Vocab at 0x1e453566be0>,\n",
       " 'call': <gensim.models.keyedvectors.Vocab at 0x1e453566c40>,\n",
       " 'describ': <gensim.models.keyedvectors.Vocab at 0x1e453566ca0>,\n",
       " 'creation': <gensim.models.keyedvectors.Vocab at 0x1e453566d00>,\n",
       " 'domain_wall': <gensim.models.keyedvectors.Vocab at 0x1e453566d60>,\n",
       " 'vortic': <gensim.models.keyedvectors.Vocab at 0x1e453566dc0>,\n",
       " 'fundament': <gensim.models.keyedvectors.Vocab at 0x1e453566e20>,\n",
       " 'object': <gensim.models.keyedvectors.Vocab at 0x1e453566e80>,\n",
       " 'novel': <gensim.models.keyedvectors.Vocab at 0x1e453566ee0>,\n",
       " 'nanostructur': <gensim.models.keyedvectors.Vocab at 0x1e453566f40>,\n",
       " 'magnet': <gensim.models.keyedvectors.Vocab at 0x1e453566fa0>,\n",
       " 'memori': <gensim.models.keyedvectors.Vocab at 0x1e453567040>,\n",
       " 'first': <gensim.models.keyedvectors.Vocab at 0x1e4535670a0>,\n",
       " 'reformul': <gensim.models.keyedvectors.Vocab at 0x1e453567100>,\n",
       " 'time': <gensim.models.keyedvectors.Vocab at 0x1e453567160>,\n",
       " 'differenti': <gensim.models.keyedvectors.Vocab at 0x1e4535671c0>,\n",
       " 'solut': <gensim.models.keyedvectors.Vocab at 0x1e453567220>,\n",
       " 'converg': <gensim.models.keyedvectors.Vocab at 0x1e453567280>,\n",
       " 'theta': <gensim.models.keyedvectors.Vocab at 0x1e4535672e0>,\n",
       " 'linear': <gensim.models.keyedvectors.Vocab at 0x1e453567340>,\n",
       " 'scheme': <gensim.models.keyedvectors.Vocab at 0x1e4535673a0>,\n",
       " 'consequ': <gensim.models.keyedvectors.Vocab at 0x1e453567400>,\n",
       " 'prove': <gensim.models.keyedvectors.Vocab at 0x1e453567460>,\n",
       " 'minor': <gensim.models.keyedvectors.Vocab at 0x1e4535674c0>,\n",
       " 'space': <gensim.models.keyedvectors.Vocab at 0x1e453567520>,\n",
       " 'step': <gensim.models.keyedvectors.Vocab at 0x1e453567580>,\n",
       " 'depend': <gensim.models.keyedvectors.Vocab at 0x1e4535675e0>,\n",
       " 'henc': <gensim.models.keyedvectors.Vocab at 0x1e453567640>,\n",
       " 'weak': <gensim.models.keyedvectors.Vocab at 0x1e4535676a0>,\n",
       " 'martingal': <gensim.models.keyedvectors.Vocab at 0x1e453567700>,\n",
       " 'numer': <gensim.models.keyedvectors.Vocab at 0x1e453567760>,\n",
       " 'present': <gensim.models.keyedvectors.Vocab at 0x1e4535677c0>,\n",
       " 'applic': <gensim.models.keyedvectors.Vocab at 0x1e453567820>,\n",
       " 'discret': <gensim.models.keyedvectors.Vocab at 0x1e453567880>,\n",
       " 'wavelet': <gensim.models.keyedvectors.Vocab at 0x1e4535678e0>,\n",
       " 'transform': <gensim.models.keyedvectors.Vocab at 0x1e453567940>,\n",
       " 'tensor': <gensim.models.keyedvectors.Vocab at 0x1e4535679a0>,\n",
       " 'decomposit': <gensim.models.keyedvectors.Vocab at 0x1e453567a00>,\n",
       " 'featur_extract': <gensim.models.keyedvectors.Vocab at 0x1e453567a60>,\n",
       " 'ftir': <gensim.models.keyedvectors.Vocab at 0x1e453567ac0>,\n",
       " 'medicin': <gensim.models.keyedvectors.Vocab at 0x1e453567b20>,\n",
       " 'plant': <gensim.models.keyedvectors.Vocab at 0x1e453567b80>,\n",
       " 'fourier_transform': <gensim.models.keyedvectors.Vocab at 0x1e453567be0>,\n",
       " 'infra': <gensim.models.keyedvectors.Vocab at 0x1e453567c40>,\n",
       " 'red': <gensim.models.keyedvectors.Vocab at 0x1e453567ca0>,\n",
       " 'spectra': <gensim.models.keyedvectors.Vocab at 0x1e453567d00>,\n",
       " 'speci': <gensim.models.keyedvectors.Vocab at 0x1e453567d60>,\n",
       " 'explor': <gensim.models.keyedvectors.Vocab at 0x1e453567dc0>,\n",
       " 'influenc': <gensim.models.keyedvectors.Vocab at 0x1e453567e20>,\n",
       " 'preprocess': <gensim.models.keyedvectors.Vocab at 0x1e453567e80>,\n",
       " 'effici': <gensim.models.keyedvectors.Vocab at 0x1e453567ee0>,\n",
       " 'machin_learn': <gensim.models.keyedvectors.Vocab at 0x1e453567f40>,\n",
       " 'dwt': <gensim.models.keyedvectors.Vocab at 0x1e453567fa0>,\n",
       " 'techniqu': <gensim.models.keyedvectors.Vocab at 0x1e453568040>,\n",
       " 'various': <gensim.models.keyedvectors.Vocab at 0x1e4535680a0>,\n",
       " 'combin': <gensim.models.keyedvectors.Vocab at 0x1e453568100>,\n",
       " 'signal': <gensim.models.keyedvectors.Vocab at 0x1e453568160>,\n",
       " 'process': <gensim.models.keyedvectors.Vocab at 0x1e4535681c0>,\n",
       " 'behavior': <gensim.models.keyedvectors.Vocab at 0x1e453568220>,\n",
       " 'appli': <gensim.models.keyedvectors.Vocab at 0x1e453568280>,\n",
       " 'classif': <gensim.models.keyedvectors.Vocab at 0x1e4535682e0>,\n",
       " 'cluster': <gensim.models.keyedvectors.Vocab at 0x1e453568340>,\n",
       " 'best': <gensim.models.keyedvectors.Vocab at 0x1e4535683a0>,\n",
       " 'found': <gensim.models.keyedvectors.Vocab at 0x1e453568400>,\n",
       " 'grid': <gensim.models.keyedvectors.Vocab at 0x1e453568460>,\n",
       " 'search': <gensim.models.keyedvectors.Vocab at 0x1e4535684c0>,\n",
       " 'similar': <gensim.models.keyedvectors.Vocab at 0x1e453568520>,\n",
       " 'signific_improv': <gensim.models.keyedvectors.Vocab at 0x1e453568580>,\n",
       " 'qualiti': <gensim.models.keyedvectors.Vocab at 0x1e4535685e0>,\n",
       " 'well': <gensim.models.keyedvectors.Vocab at 0x1e453568640>,\n",
       " 'classif_accuraci': <gensim.models.keyedvectors.Vocab at 0x1e4535686a0>,\n",
       " 'tune': <gensim.models.keyedvectors.Vocab at 0x1e453568700>,\n",
       " 'logist_regress': <gensim.models.keyedvectors.Vocab at 0x1e453568760>,\n",
       " 'comparison': <gensim.models.keyedvectors.Vocab at 0x1e4535687c0>,\n",
       " 'origin': <gensim.models.keyedvectors.Vocab at 0x1e453568820>,\n",
       " 'unlik': <gensim.models.keyedvectors.Vocab at 0x1e453568880>,\n",
       " 'rank': <gensim.models.keyedvectors.Vocab at 0x1e4535688e0>,\n",
       " 'make': <gensim.models.keyedvectors.Vocab at 0x1e453568940>,\n",
       " 'versatil': <gensim.models.keyedvectors.Vocab at 0x1e4535689a0>,\n",
       " 'easier': <gensim.models.keyedvectors.Vocab at 0x1e453568a00>,\n",
       " 'tool': <gensim.models.keyedvectors.Vocab at 0x1e453568a60>,\n",
       " 'maxim': <gensim.models.keyedvectors.Vocab at 0x1e453568ac0>,\n",
       " 'frequenc': <gensim.models.keyedvectors.Vocab at 0x1e453568b20>,\n",
       " 'complement': <gensim.models.keyedvectors.Vocab at 0x1e453568b80>,\n",
       " 'obstacl': <gensim.models.keyedvectors.Vocab at 0x1e453568be0>,\n",
       " 'let': <gensim.models.keyedvectors.Vocab at 0x1e453568c40>,\n",
       " 'omega': <gensim.models.keyedvectors.Vocab at 0x1e453568ca0>,\n",
       " 'subset_mathbb': <gensim.models.keyedvectors.Vocab at 0x1e453568d00>,\n",
       " 'r_n': <gensim.models.keyedvectors.Vocab at 0x1e453568d60>,\n",
       " 'bound': <gensim.models.keyedvectors.Vocab at 0x1e453568dc0>,\n",
       " 'domain': <gensim.models.keyedvectors.Vocab at 0x1e453568e20>,\n",
       " 'satisfi': <gensim.models.keyedvectors.Vocab at 0x1e453568e80>,\n",
       " 'asymmetri': <gensim.models.keyedvectors.Vocab at 0x1e453568ee0>,\n",
       " 'arbitrari': <gensim.models.keyedvectors.Vocab at 0x1e453568f40>,\n",
       " 'refer': <gensim.models.keyedvectors.Vocab at 0x1e453568fa0>,\n",
       " 'interest': <gensim.models.keyedvectors.Vocab at 0x1e453569040>,\n",
       " 'behaviour': <gensim.models.keyedvectors.Vocab at 0x1e4535690a0>,\n",
       " 'dirichlet': <gensim.models.keyedvectors.Vocab at 0x1e453569100>,\n",
       " 'eigenvalu': <gensim.models.keyedvectors.Vocab at 0x1e453569160>,\n",
       " 'lambda': <gensim.models.keyedvectors.Vocab at 0x1e4535691c0>,\n",
       " 'setminus': <gensim.models.keyedvectors.Vocab at 0x1e453569220>,\n",
       " 'x': <gensim.models.keyedvectors.Vocab at 0x1e453569280>,\n",
       " 'upper_bound': <gensim.models.keyedvectors.Vocab at 0x1e4535692e0>,\n",
       " 'distanc': <gensim.models.keyedvectors.Vocab at 0x1e453569340>,\n",
       " 'set': <gensim.models.keyedvectors.Vocab at 0x1e4535693a0>,\n",
       " 'point': <gensim.models.keyedvectors.Vocab at 0x1e453569400>,\n",
       " 'ground_state': <gensim.models.keyedvectors.Vocab at 0x1e453569460>,\n",
       " 'phi': <gensim.models.keyedvectors.Vocab at 0x1e4535694c0>,\n",
       " 'short': <gensim.models.keyedvectors.Vocab at 0x1e453569520>,\n",
       " 'corollari': <gensim.models.keyedvectors.Vocab at 0x1e453569580>,\n",
       " 'begin': <gensim.models.keyedvectors.Vocab at 0x1e4535695e0>,\n",
       " 'mu': <gensim.models.keyedvectors.Vocab at 0x1e453569640>,\n",
       " 'max': <gensim.models.keyedvectors.Vocab at 0x1e4535696a0>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x1e453569700>,\n",
       " 'larg': <gensim.models.keyedvectors.Vocab at 0x1e453569760>,\n",
       " 'enough': <gensim.models.keyedvectors.Vocab at 0x1e4535697c0>,\n",
       " 'close': <gensim.models.keyedvectors.Vocab at 0x1e453569820>,\n",
       " 'second': <gensim.models.keyedvectors.Vocab at 0x1e453569880>,\n",
       " 'discuss': <gensim.models.keyedvectors.Vocab at 0x1e4535698e0>,\n",
       " 'distribut': <gensim.models.keyedvectors.Vocab at 0x1e453569940>,\n",
       " 'possibl': <gensim.models.keyedvectors.Vocab at 0x1e4535699a0>,\n",
       " 'inscrib': <gensim.models.keyedvectors.Vocab at 0x1e453569a00>,\n",
       " 'wavelength': <gensim.models.keyedvectors.Vocab at 0x1e453569a60>,\n",
       " 'final': <gensim.models.keyedvectors.Vocab at 0x1e453569ac0>,\n",
       " 'specifi': <gensim.models.keyedvectors.Vocab at 0x1e453569b20>,\n",
       " 'observ': <gensim.models.keyedvectors.Vocab at 0x1e453569b80>,\n",
       " 'convex': <gensim.models.keyedvectors.Vocab at 0x1e453569be0>,\n",
       " 'suffici': <gensim.models.keyedvectors.Vocab at 0x1e453569c40>,\n",
       " 'respect': <gensim.models.keyedvectors.Vocab at 0x1e453569ca0>,\n",
       " 'contain': <gensim.models.keyedvectors.Vocab at 0x1e453569d00>,\n",
       " 'period': <gensim.models.keyedvectors.Vocab at 0x1e453569d60>,\n",
       " 'shape': <gensim.models.keyedvectors.Vocab at 0x1e453569dc0>,\n",
       " 'hyperbol': <gensim.models.keyedvectors.Vocab at 0x1e453569e20>,\n",
       " 'asteroid': <gensim.models.keyedvectors.Vocab at 0x1e453569e80>,\n",
       " 'oumuamua': <gensim.models.keyedvectors.Vocab at 0x1e453569ee0>,\n",
       " 'u': <gensim.models.keyedvectors.Vocab at 0x1e453569f40>,\n",
       " 'lightcurv': <gensim.models.keyedvectors.Vocab at 0x1e453569fa0>,\n",
       " 'newli': <gensim.models.keyedvectors.Vocab at 0x1e45356b040>,\n",
       " 'discov': <gensim.models.keyedvectors.Vocab at 0x1e45356b0a0>,\n",
       " 'planet': <gensim.models.keyedvectors.Vocab at 0x1e45356b100>,\n",
       " 'octob': <gensim.models.keyedvectors.Vocab at 0x1e45356b160>,\n",
       " 'observatori': <gensim.models.keyedvectors.Vocab at 0x1e45356b1c0>,\n",
       " 'discoveri': <gensim.models.keyedvectors.Vocab at 0x1e45356b220>,\n",
       " 'channel': <gensim.models.keyedvectors.Vocab at 0x1e45356b280>,\n",
       " 'telescop': <gensim.models.keyedvectors.Vocab at 0x1e45356b2e0>,\n",
       " 'deriv': <gensim.models.keyedvectors.Vocab at 0x1e45356b340>,\n",
       " 'partial': <gensim.models.keyedvectors.Vocab at 0x1e45356b3a0>,\n",
       " 'peak': <gensim.models.keyedvectors.Vocab at 0x1e45356b400>,\n",
       " 'trough': <gensim.models.keyedvectors.Vocab at 0x1e45356b460>,\n",
       " 'amplitud': <gensim.models.keyedvectors.Vocab at 0x1e45356b4c0>,\n",
       " 'mag': <gensim.models.keyedvectors.Vocab at 0x1e45356b520>,\n",
       " 'segment': <gensim.models.keyedvectors.Vocab at 0x1e45356b580>,\n",
       " 'rule': <gensim.models.keyedvectors.Vocab at 0x1e45356b5e0>,\n",
       " 'less': <gensim.models.keyedvectors.Vocab at 0x1e45356b640>,\n",
       " 'hr': <gensim.models.keyedvectors.Vocab at 0x1e45356b6a0>,\n",
       " 'suggest': <gensim.models.keyedvectors.Vocab at 0x1e45356b700>,\n",
       " 'assumpt': <gensim.models.keyedvectors.Vocab at 0x1e45356b760>,\n",
       " 'variabl': <gensim.models.keyedvectors.Vocab at 0x1e45356b7c0>,\n",
       " 'chang': <gensim.models.keyedvectors.Vocab at 0x1e45356b820>,\n",
       " 'cross_section': <gensim.models.keyedvectors.Vocab at 0x1e45356b880>,\n",
       " 'axial': <gensim.models.keyedvectors.Vocab at 0x1e45356b8e0>,\n",
       " 'ratio': <gensim.models.keyedvectors.Vocab at 0x1e45356b940>,\n",
       " 'saw': <gensim.models.keyedvectors.Vocab at 0x1e45356b9a0>,\n",
       " 'evid': <gensim.models.keyedvectors.Vocab at 0x1e45356ba00>,\n",
       " 'coma': <gensim.models.keyedvectors.Vocab at 0x1e45356ba60>,\n",
       " 'tail': <gensim.models.keyedvectors.Vocab at 0x1e45356bac0>,\n",
       " 'either': <gensim.models.keyedvectors.Vocab at 0x1e45356bb20>,\n",
       " 'stack': <gensim.models.keyedvectors.Vocab at 0x1e45356bb80>,\n",
       " 'equival': <gensim.models.keyedvectors.Vocab at 0x1e45356bbe0>,\n",
       " 'exposur': <gensim.models.keyedvectors.Vocab at 0x1e45356bc40>,\n",
       " 'advers': <gensim.models.keyedvectors.Vocab at 0x1e45356bca0>,\n",
       " 'polym': <gensim.models.keyedvectors.Vocab at 0x1e45356bd00>,\n",
       " 'coat': <gensim.models.keyedvectors.Vocab at 0x1e45356bd60>,\n",
       " 'heat': <gensim.models.keyedvectors.Vocab at 0x1e45356bdc0>,\n",
       " 'transport': <gensim.models.keyedvectors.Vocab at 0x1e45356be20>,\n",
       " 'solid': <gensim.models.keyedvectors.Vocab at 0x1e45356be80>,\n",
       " 'liquid': <gensim.models.keyedvectors.Vocab at 0x1e45356bee0>,\n",
       " 'interfac': <gensim.models.keyedvectors.Vocab at 0x1e45356bf40>,\n",
       " 'abil': <gensim.models.keyedvectors.Vocab at 0x1e45356bfa0>,\n",
       " 'metal': <gensim.models.keyedvectors.Vocab at 0x1e45356c040>,\n",
       " 'nanoparticl': <gensim.models.keyedvectors.Vocab at 0x1e45356c0a0>,\n",
       " 'suppli': <gensim.models.keyedvectors.Vocab at 0x1e45356c100>,\n",
       " 'environ': <gensim.models.keyedvectors.Vocab at 0x1e45356c160>,\n",
       " 'extern': <gensim.models.keyedvectors.Vocab at 0x1e45356c1c0>,\n",
       " 'optic': <gensim.models.keyedvectors.Vocab at 0x1e45356c220>,\n",
       " 'field': <gensim.models.keyedvectors.Vocab at 0x1e45356c280>,\n",
       " 'attract': <gensim.models.keyedvectors.Vocab at 0x1e45356c2e0>,\n",
       " 'grow': <gensim.models.keyedvectors.Vocab at 0x1e45356c340>,\n",
       " 'biomed': <gensim.models.keyedvectors.Vocab at 0x1e45356c3a0>,\n",
       " 'control': <gensim.models.keyedvectors.Vocab at 0x1e45356c400>,\n",
       " 'thermal': <gensim.models.keyedvectors.Vocab at 0x1e45356c460>,\n",
       " 'properti': <gensim.models.keyedvectors.Vocab at 0x1e45356c4c0>,\n",
       " 'appear': <gensim.models.keyedvectors.Vocab at 0x1e45356c520>,\n",
       " 'relev': <gensim.models.keyedvectors.Vocab at 0x1e45356c580>,\n",
       " 'work': <gensim.models.keyedvectors.Vocab at 0x1e45356c5e0>,\n",
       " 'address': <gensim.models.keyedvectors.Vocab at 0x1e45356c640>,\n",
       " 'water': <gensim.models.keyedvectors.Vocab at 0x1e45356c6a0>,\n",
       " 'gold': <gensim.models.keyedvectors.Vocab at 0x1e45356c700>,\n",
       " 'surfac': <gensim.models.keyedvectors.Vocab at 0x1e45356c760>,\n",
       " 'molecular_dynam': <gensim.models.keyedvectors.Vocab at 0x1e45356c7c0>,\n",
       " 'simul': <gensim.models.keyedvectors.Vocab at 0x1e45356c820>,\n",
       " 'increas': <gensim.models.keyedvectors.Vocab at 0x1e45356c880>,\n",
       " 'densiti': <gensim.models.keyedvectors.Vocab at 0x1e45356c8e0>,\n",
       " 'displac': <gensim.models.keyedvectors.Vocab at 0x1e45356c940>,\n",
       " 'resist': <gensim.models.keyedvectors.Vocab at 0x1e45356c9a0>,\n",
       " 'flow': <gensim.models.keyedvectors.Vocab at 0x1e45356ca00>,\n",
       " 'affect': <gensim.models.keyedvectors.Vocab at 0x1e45356ca60>,\n",
       " 'amount': <gensim.models.keyedvectors.Vocab at 0x1e45356cac0>,\n",
       " 'energi': <gensim.models.keyedvectors.Vocab at 0x1e45356cb20>,\n",
       " 'releas': <gensim.models.keyedvectors.Vocab at 0x1e45356cb80>,\n",
       " 'unexpect': <gensim.models.keyedvectors.Vocab at 0x1e45356cbe0>,\n",
       " 'trade': <gensim.models.keyedvectors.Vocab at 0x1e45356cc40>,\n",
       " 'establish': <gensim.models.keyedvectors.Vocab at 0x1e45356cca0>,\n",
       " 'sph': <gensim.models.keyedvectors.Vocab at 0x1e45356cd00>,\n",
       " 'calcul': <gensim.models.keyedvectors.Vocab at 0x1e45356cd60>,\n",
       " 'mar': <gensim.models.keyedvectors.Vocab at 0x1e45356cdc0>,\n",
       " 'scale': <gensim.models.keyedvectors.Vocab at 0x1e45356ce20>,\n",
       " 'collis': <gensim.models.keyedvectors.Vocab at 0x1e45356ce80>,\n",
       " 'role': <gensim.models.keyedvectors.Vocab at 0x1e45356cee0>,\n",
       " 'materi': <gensim.models.keyedvectors.Vocab at 0x1e45356cf40>,\n",
       " 'rheolog': <gensim.models.keyedvectors.Vocab at 0x1e45356cfa0>,\n",
       " 'larg_scale': <gensim.models.keyedvectors.Vocab at 0x1e45356d040>,\n",
       " 'approx': <gensim.models.keyedvectors.Vocab at 0x1e45356d0a0>,\n",
       " 'km': <gensim.models.keyedvectors.Vocab at 0x1e45356d100>,\n",
       " 'impact': <gensim.models.keyedvectors.Vocab at 0x1e45356d160>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x1e45356d1c0>,\n",
       " 'smooth': <gensim.models.keyedvectors.Vocab at 0x1e45356d220>,\n",
       " 'particl': <gensim.models.keyedvectors.Vocab at 0x1e45356d280>,\n",
       " 'hydrodynam': <gensim.models.keyedvectors.Vocab at 0x1e45356d2e0>,\n",
       " 'code': <gensim.models.keyedvectors.Vocab at 0x1e45356d340>,\n",
       " 'strength': <gensim.models.keyedvectors.Vocab at 0x1e45356d3a0>,\n",
       " 'post': <gensim.models.keyedvectors.Vocab at 0x1e45356d400>,\n",
       " 'temperatur': <gensim.models.keyedvectors.Vocab at 0x1e45356d460>,\n",
       " 'investig': <gensim.models.keyedvectors.Vocab at 0x1e45356d4c0>,\n",
       " 'eject': <gensim.models.keyedvectors.Vocab at 0x1e45356d520>,\n",
       " 'escap': <gensim.models.keyedvectors.Vocab at 0x1e45356d580>,\n",
       " 'disc': <gensim.models.keyedvectors.Vocab at 0x1e45356d5e0>,\n",
       " 'mass': <gensim.models.keyedvectors.Vocab at 0x1e45356d640>,\n",
       " 'potenti': <gensim.models.keyedvectors.Vocab at 0x1e45356d6a0>,\n",
       " 'context': <gensim.models.keyedvectors.Vocab at 0x1e45356d700>,\n",
       " 'discontinu': <gensim.models.keyedvectors.Vocab at 0x1e45356d760>,\n",
       " 'rigid': <gensim.models.keyedvectors.Vocab at 0x1e45356d7c0>,\n",
       " 'bodi': <gensim.models.keyedvectors.Vocab at 0x1e45356d820>,\n",
       " 'regim': <gensim.models.keyedvectors.Vocab at 0x1e45356d880>,\n",
       " 'consid': <gensim.models.keyedvectors.Vocab at 0x1e45356d8e0>,\n",
       " 'veloc': <gensim.models.keyedvectors.Vocab at 0x1e45356d940>,\n",
       " 'substanti': <gensim.models.keyedvectors.Vocab at 0x1e45356d9a0>,\n",
       " 'impactor': <gensim.models.keyedvectors.Vocab at 0x1e45356da00>,\n",
       " 'subtl': <gensim.models.keyedvectors.Vocab at 0x1e45356da60>,\n",
       " 'high': <gensim.models.keyedvectors.Vocab at 0x1e45356dac0>,\n",
       " 'mathcal': <gensim.models.keyedvectors.Vocab at 0x1e45356db20>,\n",
       " 'r': <gensim.models.keyedvectors.Vocab at 0x1e45356db80>,\n",
       " 'fail': <gensim.models.keyedvectors.Vocab at 0x1e45356dbe0>,\n",
       " 'outbreak': <gensim.models.keyedvectors.Vocab at 0x1e45356dc40>,\n",
       " 'boost': <gensim.models.keyedvectors.Vocab at 0x1e45356dca0>,\n",
       " 'immun': <gensim.models.keyedvectors.Vocab at 0x1e45356dd00>,\n",
       " 'time_vari': <gensim.models.keyedvectors.Vocab at 0x1e45356dd60>,\n",
       " 'suscept': <gensim.models.keyedvectors.Vocab at 0x1e45356ddc0>,\n",
       " 'host': <gensim.models.keyedvectors.Vocab at 0x1e45356de20>,\n",
       " 'wane': <gensim.models.keyedvectors.Vocab at 0x1e45356de80>,\n",
       " 'known': <gensim.models.keyedvectors.Vocab at 0x1e45356dee0>,\n",
       " 'induc': <gensim.models.keyedvectors.Vocab at 0x1e45356df40>,\n",
       " 'rich': <gensim.models.keyedvectors.Vocab at 0x1e45356dfa0>,\n",
       " 'long_term': <gensim.models.keyedvectors.Vocab at 0x1e453570040>,\n",
       " 'transmiss': <gensim.models.keyedvectors.Vocab at 0x1e4535700a0>,\n",
       " 'dynam': <gensim.models.keyedvectors.Vocab at 0x1e453570100>,\n",
       " 'meanwhil': <gensim.models.keyedvectors.Vocab at 0x1e453570160>,\n",
       " 'heterogen': <gensim.models.keyedvectors.Vocab at 0x1e4535701c0>,\n",
       " 'shot': <gensim.models.keyedvectors.Vocab at 0x1e453570220>,\n",
       " 'epidem': <gensim.models.keyedvectors.Vocab at 0x1e453570280>,\n",
       " 'even_though': <gensim.models.keyedvectors.Vocab at 0x1e4535702e0>,\n",
       " 'larg_amount': <gensim.models.keyedvectors.Vocab at 0x1e453570340>,\n",
       " 'avail': <gensim.models.keyedvectors.Vocab at 0x1e4535703a0>,\n",
       " 'epidemiolog': <gensim.models.keyedvectors.Vocab at 0x1e453570400>,\n",
       " 'short_term': <gensim.models.keyedvectors.Vocab at 0x1e453570460>,\n",
       " 'parsimoni': <gensim.models.keyedvectors.Vocab at 0x1e4535704c0>,\n",
       " 'mathemat': <gensim.models.keyedvectors.Vocab at 0x1e453570520>,\n",
       " 'take_account': <gensim.models.keyedvectors.Vocab at 0x1e453570580>,\n",
       " 'obtain': <gensim.models.keyedvectors.Vocab at 0x1e4535705e0>,\n",
       " 'explicit': <gensim.models.keyedvectors.Vocab at 0x1e453570640>,\n",
       " 'delay': <gensim.models.keyedvectors.Vocab at 0x1e4535706a0>,\n",
       " 'take': <gensim.models.keyedvectors.Vocab at 0x1e453570700>,\n",
       " 'negat': <gensim.models.keyedvectors.Vocab at 0x1e453570760>,\n",
       " 'slope': <gensim.models.keyedvectors.Vocab at 0x1e4535707c0>,\n",
       " 'curv': <gensim.models.keyedvectors.Vocab at 0x1e453570820>,\n",
       " 'phase': <gensim.models.keyedvectors.Vocab at 0x1e453570880>,\n",
       " 'addit': <gensim.models.keyedvectors.Vocab at 0x1e4535708e0>,\n",
       " 'common': <gensim.models.keyedvectors.Vocab at 0x1e453570940>,\n",
       " 'standard': <gensim.models.keyedvectors.Vocab at 0x1e4535709a0>,\n",
       " 'sir': <gensim.models.keyedvectors.Vocab at 0x1e453570a00>,\n",
       " 'leq': <gensim.models.keyedvectors.Vocab at 0x1e453570a60>,\n",
       " 'normal': <gensim.models.keyedvectors.Vocab at 0x1e453570ac0>,\n",
       " 'employ': <gensim.models.keyedvectors.Vocab at 0x1e453570b20>,\n",
       " 'sensit': <gensim.models.keyedvectors.Vocab at 0x1e453570b80>,\n",
       " 'analysi': <gensim.models.keyedvectors.Vocab at 0x1e453570be0>,\n",
       " 'order': <gensim.models.keyedvectors.Vocab at 0x1e453570c40>,\n",
       " 'hydraul': <gensim.models.keyedvectors.Vocab at 0x1e453570ca0>,\n",
       " 'fractur': <gensim.models.keyedvectors.Vocab at 0x1e453570d00>,\n",
       " 'horizont': <gensim.models.keyedvectors.Vocab at 0x1e453570d60>,\n",
       " 'systemat': <gensim.models.keyedvectors.Vocab at 0x1e453570dc0>,\n",
       " 'sobol': <gensim.models.keyedvectors.Vocab at 0x1e453570e20>,\n",
       " 'util': <gensim.models.keyedvectors.Vocab at 0x1e453570e80>,\n",
       " 'quantiti': <gensim.models.keyedvectors.Vocab at 0x1e453570ee0>,\n",
       " 'pore': <gensim.models.keyedvectors.Vocab at 0x1e453570f40>,\n",
       " 'pressur': <gensim.models.keyedvectors.Vocab at 0x1e453570fa0>,\n",
       " 'deplet': <gensim.models.keyedvectors.Vocab at 0x1e453571040>,\n",
       " 'stress': <gensim.models.keyedvectors.Vocab at 0x1e4535710a0>,\n",
       " 'around': <gensim.models.keyedvectors.Vocab at 0x1e453571100>,\n",
       " 'base': <gensim.models.keyedvectors.Vocab at 0x1e453571160>,\n",
       " 'degre': <gensim.models.keyedvectors.Vocab at 0x1e4535711c0>,\n",
       " 'import': <gensim.models.keyedvectors.Vocab at 0x1e453571220>,\n",
       " 'includ': <gensim.models.keyedvectors.Vocab at 0x1e453571280>,\n",
       " 'rock': <gensim.models.keyedvectors.Vocab at 0x1e4535712e0>,\n",
       " 'stimul': <gensim.models.keyedvectors.Vocab at 0x1e453571340>,\n",
       " 'design': <gensim.models.keyedvectors.Vocab at 0x1e4535713a0>,\n",
       " 'fulli': <gensim.models.keyedvectors.Vocab at 0x1e453571400>,\n",
       " 'poroelast': <gensim.models.keyedvectors.Vocab at 0x1e453571460>,\n",
       " 'account': <gensim.models.keyedvectors.Vocab at 0x1e4535714c0>,\n",
       " 'product': <gensim.models.keyedvectors.Vocab at 0x1e453571520>,\n",
       " 'eas': <gensim.models.keyedvectors.Vocab at 0x1e453571580>,\n",
       " 'comput_cost': <gensim.models.keyedvectors.Vocab at 0x1e4535715e0>,\n",
       " 'provid': <gensim.models.keyedvectors.Vocab at 0x1e453571640>,\n",
       " 'rom': <gensim.models.keyedvectors.Vocab at 0x1e4535716a0>,\n",
       " 'replac': <gensim.models.keyedvectors.Vocab at 0x1e453571700>,\n",
       " 'complex': <gensim.models.keyedvectors.Vocab at 0x1e453571760>,\n",
       " 'rather': <gensim.models.keyedvectors.Vocab at 0x1e4535717c0>,\n",
       " 'simpl': <gensim.models.keyedvectors.Vocab at 0x1e453571820>,\n",
       " 'analyt': <gensim.models.keyedvectors.Vocab at 0x1e453571880>,\n",
       " 'locat': <gensim.models.keyedvectors.Vocab at 0x1e4535718e0>,\n",
       " 'main': <gensim.models.keyedvectors.Vocab at 0x1e453571940>,\n",
       " 'research': <gensim.models.keyedvectors.Vocab at 0x1e4535719a0>,\n",
       " 'mobil': <gensim.models.keyedvectors.Vocab at 0x1e453571a00>,\n",
       " 'half': <gensim.models.keyedvectors.Vocab at 0x1e453571a60>,\n",
       " 'length': <gensim.models.keyedvectors.Vocab at 0x1e453571ac0>,\n",
       " 'contributor': <gensim.models.keyedvectors.Vocab at 0x1e453571b20>,\n",
       " 'percentag': <gensim.models.keyedvectors.Vocab at 0x1e453571b80>,\n",
       " 'contribut': <gensim.models.keyedvectors.Vocab at 0x1e453571be0>,\n",
       " 'pre': <gensim.models.keyedvectors.Vocab at 0x1e453571c40>,\n",
       " 'ii': <gensim.models.keyedvectors.Vocab at 0x1e453571ca0>,\n",
       " 'progress': <gensim.models.keyedvectors.Vocab at 0x1e453571d00>,\n",
       " 'decreas': <gensim.models.keyedvectors.Vocab at 0x1e453571d60>,\n",
       " 'iii': <gensim.models.keyedvectors.Vocab at 0x1e453571dc0>,\n",
       " 'domin': <gensim.models.keyedvectors.Vocab at 0x1e453571e20>,\n",
       " 'iv': <gensim.models.keyedvectors.Vocab at 0x1e453571e80>,\n",
       " 'zone': <gensim.models.keyedvectors.Vocab at 0x1e453571ee0>,\n",
       " 'tip': <gensim.models.keyedvectors.Vocab at 0x1e453571f40>,\n",
       " 'insid': <gensim.models.keyedvectors.Vocab at 0x1e453571fa0>,\n",
       " 'area': <gensim.models.keyedvectors.Vocab at 0x1e453573040>,\n",
       " 'factor': <gensim.models.keyedvectors.Vocab at 0x1e4535730a0>,\n",
       " 'minimum': <gensim.models.keyedvectors.Vocab at 0x1e453573100>,\n",
       " 'guidelin': <gensim.models.keyedvectors.Vocab at 0x1e453573160>,\n",
       " 'legaci': <gensim.models.keyedvectors.Vocab at 0x1e4535731c0>,\n",
       " 'secondari': <gensim.models.keyedvectors.Vocab at 0x1e453573220>,\n",
       " 'oper': <gensim.models.keyedvectors.Vocab at 0x1e453573280>,\n",
       " 'drill': <gensim.models.keyedvectors.Vocab at 0x1e4535732e0>,\n",
       " 'separ': <gensim.models.keyedvectors.Vocab at 0x1e453573340>,\n",
       " 'social': <gensim.models.keyedvectors.Vocab at 0x1e4535733a0>,\n",
       " 'dilemma': <gensim.models.keyedvectors.Vocab at 0x1e453573400>,\n",
       " 'topolog': <gensim.models.keyedvectors.Vocab at 0x1e453573460>,\n",
       " 'frustrat': <gensim.models.keyedvectors.Vocab at 0x1e4535734c0>,\n",
       " 'three': <gensim.models.keyedvectors.Vocab at 0x1e453573520>,\n",
       " 'crowd': <gensim.models.keyedvectors.Vocab at 0x1e453573580>,\n",
       " 'old': <gensim.models.keyedvectors.Vocab at 0x1e4535735e0>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x1e453573640>,\n",
       " 'interact': <gensim.models.keyedvectors.Vocab at 0x1e4535736a0>,\n",
       " 'configur': <gensim.models.keyedvectors.Vocab at 0x1e453573700>,\n",
       " 'statist': <gensim.models.keyedvectors.Vocab at 0x1e453573760>,\n",
       " 'physic': <gensim.models.keyedvectors.Vocab at 0x1e4535737c0>,\n",
       " 'accord': <gensim.models.keyedvectors.Vocab at 0x1e453573820>,\n",
       " 'within': <gensim.models.keyedvectors.Vocab at 0x1e453573880>,\n",
       " 'triangl': <gensim.models.keyedvectors.Vocab at 0x1e4535738e0>,\n",
       " 'deserv': <gensim.models.keyedvectors.Vocab at 0x1e453573940>,\n",
       " 'special': <gensim.models.keyedvectors.Vocab at 0x1e4535739a0>,\n",
       " 'attent': <gensim.models.keyedvectors.Vocab at 0x1e453573a00>,\n",
       " 'motiv': <gensim.models.keyedvectors.Vocab at 0x1e453573a60>,\n",
       " 'evolutionari': <gensim.models.keyedvectors.Vocab at 0x1e453573ac0>,\n",
       " 'game': <gensim.models.keyedvectors.Vocab at 0x1e453573b20>,\n",
       " 'triangular': <gensim.models.keyedvectors.Vocab at 0x1e453573b80>,\n",
       " 'lattic': <gensim.models.keyedvectors.Vocab at 0x1e453573be0>,\n",
       " 'prevent': <gensim.models.keyedvectors.Vocab at 0x1e453573c40>,\n",
       " 'anti': <gensim.models.keyedvectors.Vocab at 0x1e453573ca0>,\n",
       " 'coordin': <gensim.models.keyedvectors.Vocab at 0x1e453573d00>,\n",
       " 'compet': <gensim.models.keyedvectors.Vocab at 0x1e453573d60>,\n",
       " 'strategi': <gensim.models.keyedvectors.Vocab at 0x1e453573dc0>,\n",
       " 'would': <gensim.models.keyedvectors.Vocab at 0x1e453573e20>,\n",
       " 'need': <gensim.models.keyedvectors.Vocab at 0x1e453573e80>,\n",
       " 'optim': <gensim.models.keyedvectors.Vocab at 0x1e453573ee0>,\n",
       " 'outcom': <gensim.models.keyedvectors.Vocab at 0x1e453573f40>,\n",
       " 'updat': <gensim.models.keyedvectors.Vocab at 0x1e453573fa0>,\n",
       " 'protocol': <gensim.models.keyedvectors.Vocab at 0x1e453576040>,\n",
       " 'spatial': <gensim.models.keyedvectors.Vocab at 0x1e4535760a0>,\n",
       " 'pattern': <gensim.models.keyedvectors.Vocab at 0x1e453576100>,\n",
       " 'payoff': <gensim.models.keyedvectors.Vocab at 0x1e453576160>,\n",
       " 'reminisc': <gensim.models.keyedvectors.Vocab at 0x1e4535761c0>,\n",
       " 'honeycomb': <gensim.models.keyedvectors.Vocab at 0x1e453576220>,\n",
       " 'organ': <gensim.models.keyedvectors.Vocab at 0x1e453576280>,\n",
       " 'help': <gensim.models.keyedvectors.Vocab at 0x1e4535762e0>,\n",
       " 'minim': <gensim.models.keyedvectors.Vocab at 0x1e453576340>,\n",
       " 'emerg': <gensim.models.keyedvectors.Vocab at 0x1e4535763a0>,\n",
       " 'microscop': <gensim.models.keyedvectors.Vocab at 0x1e453576400>,\n",
       " 'mean': <gensim.models.keyedvectors.Vocab at 0x1e453576460>,\n",
       " 'mean_field': <gensim.models.keyedvectors.Vocab at 0x1e4535764c0>,\n",
       " 'mont_carlo': <gensim.models.keyedvectors.Vocab at 0x1e453576520>,\n",
       " 'squar': <gensim.models.keyedvectors.Vocab at 0x1e453576580>,\n",
       " 'cours': <gensim.models.keyedvectors.Vocab at 0x1e4535765e0>,\n",
       " 'absent': <gensim.models.keyedvectors.Vocab at 0x1e453576640>,\n",
       " 'howev': <gensim.models.keyedvectors.Vocab at 0x1e4535766a0>,\n",
       " 'delet': <gensim.models.keyedvectors.Vocab at 0x1e453576700>,\n",
       " 'diagon': <gensim.models.keyedvectors.Vocab at 0x1e453576760>,\n",
       " 'link': <gensim.models.keyedvectors.Vocab at 0x1e4535767c0>,\n",
       " 'gradual': <gensim.models.keyedvectors.Vocab at 0x1e453576820>,\n",
       " 'bridg': <gensim.models.keyedvectors.Vocab at 0x1e453576880>,\n",
       " 'gap': <gensim.models.keyedvectors.Vocab at 0x1e4535768e0>,\n",
       " 'cooper': <gensim.models.keyedvectors.Vocab at 0x1e453576940>,\n",
       " 'indic': <gensim.models.keyedvectors.Vocab at 0x1e4535769a0>,\n",
       " 'thus': <gensim.models.keyedvectors.Vocab at 0x1e453576a00>,\n",
       " 'determin': <gensim.models.keyedvectors.Vocab at 0x1e453576a60>,\n",
       " 'exciton': <gensim.models.keyedvectors.Vocab at 0x1e453576ac0>,\n",
       " 'polaron': <gensim.models.keyedvectors.Vocab at 0x1e453576b20>,\n",
       " 'quantum': <gensim.models.keyedvectors.Vocab at 0x1e453576b80>,\n",
       " 'self': <gensim.models.keyedvectors.Vocab at 0x1e453576be0>,\n",
       " 'format': <gensim.models.keyedvectors.Vocab at 0x1e453576c40>,\n",
       " 'cd': <gensim.models.keyedvectors.Vocab at 0x1e453576ca0>,\n",
       " 'mn': <gensim.models.keyedvectors.Vocab at 0x1e453576d00>,\n",
       " 'se': <gensim.models.keyedvectors.Vocab at 0x1e453576d60>,\n",
       " 'mg': <gensim.models.keyedvectors.Vocab at 0x1e453576dc0>,\n",
       " 'dilut': <gensim.models.keyedvectors.Vocab at 0x1e453576e20>,\n",
       " 'semiconductor': <gensim.models.keyedvectors.Vocab at 0x1e453576e80>,\n",
       " 'resolv': <gensim.models.keyedvectors.Vocab at 0x1e453576ee0>,\n",
       " 'photoluminesc': <gensim.models.keyedvectors.Vocab at 0x1e453576f40>,\n",
       " 'pl': <gensim.models.keyedvectors.Vocab at 0x1e453576fa0>,\n",
       " 'magnet_field': <gensim.models.keyedvectors.Vocab at 0x1e453577040>,\n",
       " 'deduc': <gensim.models.keyedvectors.Vocab at 0x1e4535770a0>,\n",
       " 'mev': <gensim.models.keyedvectors.Vocab at 0x1e453577100>,\n",
       " 'agreement': <gensim.models.keyedvectors.Vocab at 0x1e453577160>,\n",
       " 'integr': <gensim.models.keyedvectors.Vocab at 0x1e4535771c0>,\n",
       " 'select': <gensim.models.keyedvectors.Vocab at 0x1e453577220>,\n",
       " 'excit': <gensim.models.keyedvectors.Vocab at 0x1e453577280>,\n",
       " 'circular': <gensim.models.keyedvectors.Vocab at 0x1e4535772e0>,\n",
       " 'polar': <gensim.models.keyedvectors.Vocab at 0x1e453577340>,\n",
       " 'ps': <gensim.models.keyedvectors.Vocab at 0x1e4535773a0>,\n",
       " 'signific': <gensim.models.keyedvectors.Vocab at 0x1e453577400>,\n",
       " 'longer': <gensim.models.keyedvectors.Vocab at 0x1e453577460>,\n",
       " 'correspond': <gensim.models.keyedvectors.Vocab at 0x1e4535774c0>,\n",
       " 'report': <gensim.models.keyedvectors.Vocab at 0x1e453577520>,\n",
       " 'earlier': <gensim.models.keyedvectors.Vocab at 0x1e453577580>,\n",
       " 'strong': <gensim.models.keyedvectors.Vocab at 0x1e4535775e0>,\n",
       " 'accompani': <gensim.models.keyedvectors.Vocab at 0x1e453577640>,\n",
       " 'squeez': <gensim.models.keyedvectors.Vocab at 0x1e4535776a0>,\n",
       " 'heavi': <gensim.models.keyedvectors.Vocab at 0x1e453577700>,\n",
       " 'hole': <gensim.models.keyedvectors.Vocab at 0x1e453577760>,\n",
       " 'envelop': <gensim.models.keyedvectors.Vocab at 0x1e4535777c0>,\n",
       " 'wavefunct': <gensim.models.keyedvectors.Vocab at 0x1e453577820>,\n",
       " 'conclus': <gensim.models.keyedvectors.Vocab at 0x1e453577880>,\n",
       " 'support': <gensim.models.keyedvectors.Vocab at 0x1e4535778e0>,\n",
       " 'lifetim': <gensim.models.keyedvectors.Vocab at 0x1e453577940>,\n",
       " 'varieti': <gensim.models.keyedvectors.Vocab at 0x1e4535779a0>,\n",
       " 'automata': <gensim.models.keyedvectors.Vocab at 0x1e453577a00>,\n",
       " 'eilenberg': <gensim.models.keyedvectors.Vocab at 0x1e453577a60>,\n",
       " 'concept': <gensim.models.keyedvectors.Vocab at 0x1e453577ac0>,\n",
       " 'syntact': <gensim.models.keyedvectors.Vocab at 0x1e453577b20>,\n",
       " 'monoid': <gensim.models.keyedvectors.Vocab at 0x1e453577b80>,\n",
       " 'regular': <gensim.models.keyedvectors.Vocab at 0x1e453577be0>,\n",
       " 'languag': <gensim.models.keyedvectors.Vocab at 0x1e453577c40>,\n",
       " 'pseudovarieti': <gensim.models.keyedvectors.Vocab at 0x1e453577ca0>,\n",
       " 'modif': <gensim.models.keyedvectors.Vocab at 0x1e453577d00>,\n",
       " 'general': <gensim.models.keyedvectors.Vocab at 0x1e453577d60>,\n",
       " 'class': <gensim.models.keyedvectors.Vocab at 0x1e453577dc0>,\n",
       " 'hand': <gensim.models.keyedvectors.Vocab at 0x1e453577e20>,\n",
       " 'algebra': <gensim.models.keyedvectors.Vocab at 0x1e453577e80>,\n",
       " 'structur': <gensim.models.keyedvectors.Vocab at 0x1e453577ee0>,\n",
       " 'preimag': <gensim.models.keyedvectors.Vocab at 0x1e453577f40>,\n",
       " 'homomorph': <gensim.models.keyedvectors.Vocab at 0x1e453577fa0>,\n",
       " 'equip': <gensim.models.keyedvectors.Vocab at 0x1e453579040>,\n",
       " 'compat': <gensim.models.keyedvectors.Vocab at 0x1e4535790a0>,\n",
       " 'distinguish': <gensim.models.keyedvectors.Vocab at 0x1e453579100>,\n",
       " 'counterpart': <gensim.models.keyedvectors.Vocab at 0x1e453579160>,\n",
       " 'previous': <gensim.models.keyedvectors.Vocab at 0x1e4535791c0>,\n",
       " 'c': <gensim.models.keyedvectors.Vocab at 0x1e453579220>,\n",
       " 'spontan': <gensim.models.keyedvectors.Vocab at 0x1e453579280>,\n",
       " 'abrikosov': <gensim.models.keyedvectors.Vocab at 0x1e4535792e0>,\n",
       " 'vortex': <gensim.models.keyedvectors.Vocab at 0x1e453579340>,\n",
       " 'ferromagnet': <gensim.models.keyedvectors.Vocab at 0x1e4535793a0>,\n",
       " 'superconductor': <gensim.models.keyedvectors.Vocab at 0x1e453579400>,\n",
       " 'euf': <gensim.models.keyedvectors.Vocab at 0x1e453579460>,\n",
       " 'p': <gensim.models.keyedvectors.Vocab at 0x1e4535794c0>,\n",
       " 'x_x': <gensim.models.keyedvectors.Vocab at 0x1e453579520>,\n",
       " 'low_temperatur': <gensim.models.keyedvectors.Vocab at 0x1e453579580>,\n",
       " 'forc': <gensim.models.keyedvectors.Vocab at 0x1e4535795e0>,\n",
       " 'microscopi': <gensim.models.keyedvectors.Vocab at 0x1e453579640>,\n",
       " 'mfm': <gensim.models.keyedvectors.Vocab at 0x1e4535796a0>,\n",
       " 'svp': <gensim.models.keyedvectors.Vocab at 0x1e453579700>,\n",
       " 'singl_crystal': <gensim.models.keyedvectors.Vocab at 0x1e453579760>,\n",
       " 'superconduct': <gensim.models.keyedvectors.Vocab at 0x1e4535797c0>,\n",
       " 'rm': <gensim.models.keyedvectors.Vocab at 0x1e453579820>,\n",
       " 'sc': <gensim.models.keyedvectors.Vocab at 0x1e453579880>,\n",
       " 'k': <gensim.models.keyedvectors.Vocab at 0x1e4535798e0>,\n",
       " 'fm': <gensim.models.keyedvectors.Vocab at 0x1e453579940>,\n",
       " 'sim': <gensim.models.keyedvectors.Vocab at 0x1e4535799a0>,\n",
       " 'transit': <gensim.models.keyedvectors.Vocab at 0x1e453579a00>,\n",
       " 'antivortex': <gensim.models.keyedvectors.Vocab at 0x1e453579a60>,\n",
       " 'v': <gensim.models.keyedvectors.Vocab at 0x1e453579ac0>,\n",
       " 'av': <gensim.models.keyedvectors.Vocab at 0x1e453579b20>,\n",
       " 'pair': <gensim.models.keyedvectors.Vocab at 0x1e453579b80>,\n",
       " 'vicin': <gensim.models.keyedvectors.Vocab at 0x1e453579be0>,\n",
       " 'upon': <gensim.models.keyedvectors.Vocab at 0x1e453579c40>,\n",
       " 'cool': <gensim.models.keyedvectors.Vocab at 0x1e453579ca0>,\n",
       " 'cycl': <gensim.models.keyedvectors.Vocab at 0x1e453579d00>,\n",
       " 'near': <gensim.models.keyedvectors.Vocab at 0x1e453579d60>,\n",
       " 'first_order': <gensim.models.keyedvectors.Vocab at 0x1e453579dc0>,\n",
       " 'meissner': <gensim.models.keyedvectors.Vocab at 0x1e453579e20>,\n",
       " 'long': <gensim.models.keyedvectors.Vocab at 0x1e453579e80>,\n",
       " 'scenario': <gensim.models.keyedvectors.Vocab at 0x1e453579ee0>,\n",
       " 'character': <gensim.models.keyedvectors.Vocab at 0x1e453579f40>,\n",
       " 'larger': <gensim.models.keyedvectors.Vocab at 0x1e453579fa0>,\n",
       " 'peculiar': <gensim.models.keyedvectors.Vocab at 0x1e45357c040>,\n",
       " 'branch': <gensim.models.keyedvectors.Vocab at 0x1e45357c0a0>,\n",
       " 'stripe': <gensim.models.keyedvectors.Vocab at 0x1e45357c100>,\n",
       " 'typic': <gensim.models.keyedvectors.Vocab at 0x1e45357c160>,\n",
       " 'uniaxi': <gensim.models.keyedvectors.Vocab at 0x1e45357c1c0>,\n",
       " 'perpendicular': <gensim.models.keyedvectors.Vocab at 0x1e45357c220>,\n",
       " 'anisotropi': <gensim.models.keyedvectors.Vocab at 0x1e45357c280>,\n",
       " 'pma': <gensim.models.keyedvectors.Vocab at 0x1e45357c2e0>,\n",
       " 'ware': <gensim.models.keyedvectors.Vocab at 0x1e45357c340>,\n",
       " 'sm': <gensim.models.keyedvectors.Vocab at 0x1e45357c3a0>,\n",
       " 'langl': <gensim.models.keyedvectors.Vocab at 0x1e45357c400>,\n",
       " 'rangl': <gensim.models.keyedvectors.Vocab at 0x1e45357c460>,\n",
       " 'symmetri': <gensim.models.keyedvectors.Vocab at 0x1e45357c4c0>,\n",
       " 'recent': <gensim.models.keyedvectors.Vocab at 0x1e45357c520>,\n",
       " 'expon': <gensim.models.keyedvectors.Vocab at 0x1e45357c580>,\n",
       " 'matrix': <gensim.models.keyedvectors.Vocab at 0x1e45357c5e0>,\n",
       " 'symmetr': <gensim.models.keyedvectors.Vocab at 0x1e45357c640>,\n",
       " 'better_understand': <gensim.models.keyedvectors.Vocab at 0x1e45357c6a0>,\n",
       " 'group': <gensim.models.keyedvectors.Vocab at 0x1e45357c700>,\n",
       " 'pdbi': <gensim.models.keyedvectors.Vocab at 0x1e45357c760>,\n",
       " 'arcsecond': <gensim.models.keyedvectors.Vocab at 0x1e45357c7c0>,\n",
       " 'survey': <gensim.models.keyedvectors.Vocab at 0x1e45357c820>,\n",
       " 'paw': <gensim.models.keyedvectors.Vocab at 0x1e45357c880>,\n",
       " 'spiral': <gensim.models.keyedvectors.Vocab at 0x1e45357c8e0>,\n",
       " 'arm': <gensim.models.keyedvectors.Vocab at 0x1e45357c940>,\n",
       " 'cloud': <gensim.models.keyedvectors.Vocab at 0x1e45357c9a0>,\n",
       " 'star_format': <gensim.models.keyedvectors.Vocab at 0x1e45357ca00>,\n",
       " 'lead': <gensim.models.keyedvectors.Vocab at 0x1e45357ca60>,\n",
       " 'bright': <gensim.models.keyedvectors.Vocab at 0x1e45357cac0>,\n",
       " 'star_form': <gensim.models.keyedvectors.Vocab at 0x1e45357cb20>,\n",
       " 'site': <gensim.models.keyedvectors.Vocab at 0x1e45357cb80>,\n",
       " 'along': <gensim.models.keyedvectors.Vocab at 0x1e45357cbe0>,\n",
       " 'promin': <gensim.models.keyedvectors.Vocab at 0x1e45357cc40>,\n",
       " 'remain': <gensim.models.keyedvectors.Vocab at 0x1e45357cca0>,\n",
       " 'elus': <gensim.models.keyedvectors.Vocab at 0x1e45357cd00>,\n",
       " 'multi': <gensim.models.keyedvectors.Vocab at 0x1e45357cd60>,\n",
       " 'nearbi': <gensim.models.keyedvectors.Vocab at 0x1e45357cdc0>,\n",
       " 'grand': <gensim.models.keyedvectors.Vocab at 0x1e45357ce20>,\n",
       " 'galaxi': <gensim.models.keyedvectors.Vocab at 0x1e45357ce80>,\n",
       " 'belong': <gensim.models.keyedvectors.Vocab at 0x1e45357cee0>,\n",
       " 'wave': <gensim.models.keyedvectors.Vocab at 0x1e45357cf40>,\n",
       " 'exhibit': <gensim.models.keyedvectors.Vocab at 0x1e45357cfa0>,\n",
       " 'nine': <gensim.models.keyedvectors.Vocab at 0x1e45357d040>,\n",
       " 'gas': <gensim.models.keyedvectors.Vocab at 0x1e45357d0a0>,\n",
       " 'spur': <gensim.models.keyedvectors.Vocab at 0x1e45357d100>,\n",
       " 'ioniz': <gensim.models.keyedvectors.Vocab at 0x1e45357d160>,\n",
       " 'atom': <gensim.models.keyedvectors.Vocab at 0x1e45357d1c0>,\n",
       " 'molecular': <gensim.models.keyedvectors.Vocab at 0x1e45357d220>,\n",
       " 'dusti': <gensim.models.keyedvectors.Vocab at 0x1e45357d280>,\n",
       " 'interstellar': <gensim.models.keyedvectors.Vocab at 0x1e45357d2e0>,\n",
       " 'medium': <gensim.models.keyedvectors.Vocab at 0x1e45357d340>,\n",
       " 'ism': <gensim.models.keyedvectors.Vocab at 0x1e45357d3a0>,\n",
       " 'tracer': <gensim.models.keyedvectors.Vocab at 0x1e45357d400>,\n",
       " 'hii': <gensim.models.keyedvectors.Vocab at 0x1e45357d460>,\n",
       " 'region': <gensim.models.keyedvectors.Vocab at 0x1e45357d4c0>,\n",
       " 'young': <gensim.models.keyedvectors.Vocab at 0x1e45357d520>,\n",
       " 'myr': <gensim.models.keyedvectors.Vocab at 0x1e45357d580>,\n",
       " 'stellar': <gensim.models.keyedvectors.Vocab at 0x1e45357d5e0>,\n",
       " 'variat': <gensim.models.keyedvectors.Vocab at 0x1e45357d640>,\n",
       " 'giant': <gensim.models.keyedvectors.Vocab at 0x1e45357d6a0>,\n",
       " 'gmc': <gensim.models.keyedvectors.Vocab at 0x1e45357d700>,\n",
       " 'extinct': <gensim.models.keyedvectors.Vocab at 0x1e45357d760>,\n",
       " 'aris': <gensim.models.keyedvectors.Vocab at 0x1e45357d7c0>,\n",
       " 'ongo': <gensim.models.keyedvectors.Vocab at 0x1e45357d820>,\n",
       " 'despit': <gensim.models.keyedvectors.Vocab at 0x1e45357d880>,\n",
       " 'trend': <gensim.models.keyedvectors.Vocab at 0x1e45357d8e0>,\n",
       " 'age': <gensim.models.keyedvectors.Vocab at 0x1e45357d940>,\n",
       " 'feedback': <gensim.models.keyedvectors.Vocab at 0x1e45357d9a0>,\n",
       " 'tentat': <gensim.models.keyedvectors.Vocab at 0x1e45357da00>,\n",
       " 'trigger': <gensim.models.keyedvectors.Vocab at 0x1e45357da60>,\n",
       " 'entiti': <gensim.models.keyedvectors.Vocab at 0x1e45357dac0>,\n",
       " 'blend': <gensim.models.keyedvectors.Vocab at 0x1e45357db20>,\n",
       " 'lower': <gensim.models.keyedvectors.Vocab at 0x1e45357db80>,\n",
       " 'resolut': <gensim.models.keyedvectors.Vocab at 0x1e45357dbe0>,\n",
       " 'conclud': <gensim.models.keyedvectors.Vocab at 0x1e45357dc40>,\n",
       " 'coher': <gensim.models.keyedvectors.Vocab at 0x1e45357dca0>,\n",
       " 'onset': <gensim.models.keyedvectors.Vocab at 0x1e45357dd00>,\n",
       " 'mechan': <gensim.models.keyedvectors.Vocab at 0x1e45357dd60>,\n",
       " 'sole': <gensim.models.keyedvectors.Vocab at 0x1e45357ddc0>,\n",
       " 'occur': <gensim.models.keyedvectors.Vocab at 0x1e45357de20>,\n",
       " 'proceed': <gensim.models.keyedvectors.Vocab at 0x1e45357de80>,\n",
       " 'sever': <gensim.models.keyedvectors.Vocab at 0x1e45357dee0>,\n",
       " 'million': <gensim.models.keyedvectors.Vocab at 0x1e45357df40>,\n",
       " 'year': <gensim.models.keyedvectors.Vocab at 0x1e45357dfa0>,\n",
       " 'impli': <gensim.models.keyedvectors.Vocab at 0x1e45357f040>,\n",
       " 'act': <gensim.models.keyedvectors.Vocab at 0x1e45357f0a0>,\n",
       " 'sustain': <gensim.models.keyedvectors.Vocab at 0x1e45357f100>,\n",
       " 'unstabl': <gensim.models.keyedvectors.Vocab at 0x1e45357f160>,\n",
       " 'adam': <gensim.models.keyedvectors.Vocab at 0x1e45357f1c0>,\n",
       " 'spectral': <gensim.models.keyedvectors.Vocab at 0x1e45357f220>,\n",
       " 'sequenc': <gensim.models.keyedvectors.Vocab at 0x1e45357f280>,\n",
       " 'variant': <gensim.models.keyedvectors.Vocab at 0x1e45357f2e0>,\n",
       " 'free': <gensim.models.keyedvectors.Vocab at 0x1e45357f340>,\n",
       " 'simplici': <gensim.models.keyedvectors.Vocab at 0x1e45357f3a0>,\n",
       " 'h': <gensim.models.keyedvectors.Vocab at 0x1e45357f400>,\n",
       " 'mathbb_f': <gensim.models.keyedvectors.Vocab at 0x1e45357f460>,\n",
       " 'mathbb_q': <gensim.models.keyedvectors.Vocab at 0x1e45357f4c0>,\n",
       " 'filtrat': <gensim.models.keyedvectors.Vocab at 0x1e45357f520>,\n",
       " 'appropri': <gensim.models.keyedvectors.Vocab at 0x1e45357f580>,\n",
       " 'cohomolog': <gensim.models.keyedvectors.Vocab at 0x1e45357f5e0>,\n",
       " 'covari': <gensim.models.keyedvectors.Vocab at 0x1e45357f640>,\n",
       " 'priorit': <gensim.models.keyedvectors.Vocab at 0x1e45357f6a0>,\n",
       " 'via': <gensim.models.keyedvectors.Vocab at 0x1e45357f700>,\n",
       " 'match': <gensim.models.keyedvectors.Vocab at 0x1e45357f760>,\n",
       " 'causal': <gensim.models.keyedvectors.Vocab at 0x1e45357f7c0>,\n",
       " 'five': <gensim.models.keyedvectors.Vocab at 0x1e45357f820>,\n",
       " 'empir': <gensim.models.keyedvectors.Vocab at 0x1e45357f880>,\n",
       " 'seek': <gensim.models.keyedvectors.Vocab at 0x1e45357f8e0>,\n",
       " 'assum': <gensim.models.keyedvectors.Vocab at 0x1e45357f940>,\n",
       " 'treatment': <gensim.models.keyedvectors.Vocab at 0x1e45357f9a0>,\n",
       " 'identif': <gensim.models.keyedvectors.Vocab at 0x1e45357fa00>,\n",
       " 'analyst': <gensim.models.keyedvectors.Vocab at 0x1e45357fa60>,\n",
       " 'must': <gensim.models.keyedvectors.Vocab at 0x1e45357fac0>,\n",
       " 'adjust': <gensim.models.keyedvectors.Vocab at 0x1e45357fb20>,\n",
       " 'confound': <gensim.models.keyedvectors.Vocab at 0x1e45357fb80>,\n",
       " 'basic': <gensim.models.keyedvectors.Vocab at 0x1e45357fbe0>,\n",
       " 'regress': <gensim.models.keyedvectors.Vocab at 0x1e45357fc40>,\n",
       " 'robust': <gensim.models.keyedvectors.Vocab at 0x1e45357fca0>,\n",
       " 'weight': <gensim.models.keyedvectors.Vocab at 0x1e45357fd00>,\n",
       " 'becom': <gensim.models.keyedvectors.Vocab at 0x1e45357fd60>,\n",
       " 'late': <gensim.models.keyedvectors.Vocab at 0x1e45357fdc0>,\n",
       " 'even': <gensim.models.keyedvectors.Vocab at 0x1e45357fe20>,\n",
       " 'flexibl': <gensim.models.keyedvectors.Vocab at 0x1e45357fe80>,\n",
       " 'black_box': <gensim.models.keyedvectors.Vocab at 0x1e45357fee0>,\n",
       " 'littl': <gensim.models.keyedvectors.Vocab at 0x1e45357ff40>,\n",
       " 'input': <gensim.models.keyedvectors.Vocab at 0x1e45357ffa0>,\n",
       " 'competit': <gensim.models.keyedvectors.Vocab at 0x1e453582040>,\n",
       " 'substant': <gensim.models.keyedvectors.Vocab at 0x1e4535820a0>,\n",
       " 'contrast': <gensim.models.keyedvectors.Vocab at 0x1e453582100>,\n",
       " 'black': <gensim.models.keyedvectors.Vocab at 0x1e453582160>,\n",
       " 'replic': <gensim.models.keyedvectors.Vocab at 0x1e4535821c0>,\n",
       " 'custom': <gensim.models.keyedvectors.Vocab at 0x1e453582220>,\n",
       " 'respons': <gensim.models.keyedvectors.Vocab at 0x1e453582280>,\n",
       " 'expertis': <gensim.models.keyedvectors.Vocab at 0x1e4535822e0>,\n",
       " 'across': <gensim.models.keyedvectors.Vocab at 0x1e453582340>,\n",
       " 'advic': <gensim.models.keyedvectors.Vocab at 0x1e4535823a0>,\n",
       " 'acoust': <gensim.models.keyedvectors.Vocab at 0x1e453582400>,\n",
       " 'imped': <gensim.models.keyedvectors.Vocab at 0x1e453582460>,\n",
       " 'invers': <gensim.models.keyedvectors.Vocab at 0x1e4535824c0>,\n",
       " 'helmholtz': <gensim.models.keyedvectors.Vocab at 0x1e453582520>,\n",
       " 'assign': <gensim.models.keyedvectors.Vocab at 0x1e453582580>,\n",
       " 'homogen': <gensim.models.keyedvectors.Vocab at 0x1e4535825e0>,\n",
       " 'boundari_condit': <gensim.models.keyedvectors.Vocab at 0x1e453582640>,\n",
       " 'navier_stoke': <gensim.models.keyedvectors.Vocab at 0x1e4535826a0>,\n",
       " 'solver': <gensim.models.keyedvectors.Vocab at 0x1e453582700>,\n",
       " 'output': <gensim.models.keyedvectors.Vocab at 0x1e453582760>,\n",
       " 'eigenfunct': <gensim.models.keyedvectors.Vocab at 0x1e4535827c0>,\n",
       " 'ih': <gensim.models.keyedvectors.Vocab at 0x1e453582820>,\n",
       " 'revers': <gensim.models.keyedvectors.Vocab at 0x1e453582880>,\n",
       " 'procedur': <gensim.models.keyedvectors.Vocab at 0x1e4535828e0>,\n",
       " 'return': <gensim.models.keyedvectors.Vocab at 0x1e453582940>,\n",
       " 'unknown': <gensim.models.keyedvectors.Vocab at 0x1e4535829a0>,\n",
       " 'boundari': <gensim.models.keyedvectors.Vocab at 0x1e453582a00>,\n",
       " 'ib': <gensim.models.keyedvectors.Vocab at 0x1e453582a60>,\n",
       " 'real': <gensim.models.keyedvectors.Vocab at 0x1e453582ac0>,\n",
       " 'second_order': <gensim.models.keyedvectors.Vocab at 0x1e453582b20>,\n",
       " 'unstructur': <gensim.models.keyedvectors.Vocab at 0x1e453582b80>,\n",
       " 'stagger': <gensim.models.keyedvectors.Vocab at 0x1e453582be0>,\n",
       " 'arrang': <gensim.models.keyedvectors.Vocab at 0x1e453582c40>,\n",
       " 'momentum': <gensim.models.keyedvectors.Vocab at 0x1e453582ca0>,\n",
       " 'extend': <gensim.models.keyedvectors.Vocab at 0x1e453582d00>,\n",
       " 'center': <gensim.models.keyedvectors.Vocab at 0x1e453582d60>,\n",
       " 'face': <gensim.models.keyedvectors.Vocab at 0x1e453582dc0>,\n",
       " 'compon': <gensim.models.keyedvectors.Vocab at 0x1e453582e20>,\n",
       " 'co': <gensim.models.keyedvectors.Vocab at 0x1e453582e80>,\n",
       " 'treat': <gensim.models.keyedvectors.Vocab at 0x1e453582ee0>,\n",
       " 'closur': <gensim.models.keyedvectors.Vocab at 0x1e453582f40>,\n",
       " 'gradient': <gensim.models.keyedvectors.Vocab at 0x1e453582fa0>,\n",
       " 'waveform': <gensim.models.keyedvectors.Vocab at 0x1e453583040>,\n",
       " 'carri': <gensim.models.keyedvectors.Vocab at 0x1e4535830a0>,\n",
       " 'independ': <gensim.models.keyedvectors.Vocab at 0x1e453583100>,\n",
       " 'complet': <gensim.models.keyedvectors.Vocab at 0x1e453583160>,\n",
       " 'broadband': <gensim.models.keyedvectors.Vocab at 0x1e4535831c0>,\n",
       " 'desir': <gensim.models.keyedvectors.Vocab at 0x1e453583220>,\n",
       " 'rang': <gensim.models.keyedvectors.Vocab at 0x1e453583280>,\n",
       " 'valid': <gensim.models.keyedvectors.Vocab at 0x1e4535832e0>,\n",
       " 'inviscid': <gensim.models.keyedvectors.Vocab at 0x1e453583340>,\n",
       " 'viscous': <gensim.models.keyedvectors.Vocab at 0x1e4535833a0>,\n",
       " 'rectangular': <gensim.models.keyedvectors.Vocab at 0x1e453583400>,\n",
       " 'duct': <gensim.models.keyedvectors.Vocab at 0x1e453583460>,\n",
       " 'geometr': <gensim.models.keyedvectors.Vocab at 0x1e4535834c0>,\n",
       " 'toy': <gensim.models.keyedvectors.Vocab at 0x1e453583520>,\n",
       " 'caviti': <gensim.models.keyedvectors.Vocab at 0x1e453583580>,\n",
       " 'verifi': <gensim.models.keyedvectors.Vocab at 0x1e4535835e0>,\n",
       " 'companion': <gensim.models.keyedvectors.Vocab at 0x1e453583640>,\n",
       " 'full': <gensim.models.keyedvectors.Vocab at 0x1e4535836a0>,\n",
       " 'compress': <gensim.models.keyedvectors.Vocab at 0x1e453583700>,\n",
       " 'geometri': <gensim.models.keyedvectors.Vocab at 0x1e453583760>,\n",
       " 'one_dimension': <gensim.models.keyedvectors.Vocab at 0x1e4535837c0>,\n",
       " 'test': <gensim.models.keyedvectors.Vocab at 0x1e453583820>,\n",
       " 'tube': <gensim.models.keyedvectors.Vocab at 0x1e453583880>,\n",
       " 'methodolog': <gensim.models.keyedvectors.Vocab at 0x1e4535838e0>,\n",
       " 'shown': <gensim.models.keyedvectors.Vocab at 0x1e453583940>,\n",
       " 'captur': <gensim.models.keyedvectors.Vocab at 0x1e4535839a0>,\n",
       " 'quantit': <gensim.models.keyedvectors.Vocab at 0x1e453583a00>,\n",
       " 'growth_rate': <gensim.models.keyedvectors.Vocab at 0x1e453583a60>,\n",
       " 'deciph': <gensim.models.keyedvectors.Vocab at 0x1e453583ac0>,\n",
       " 'amplif': <gensim.models.keyedvectors.Vocab at 0x1e453583b20>,\n",
       " 'reduct': <gensim.models.keyedvectors.Vocab at 0x1e453583b80>,\n",
       " 'open': <gensim.models.keyedvectors.Vocab at 0x1e453583be0>,\n",
       " 'chemic': <gensim.models.keyedvectors.Vocab at 0x1e453583c40>,\n",
       " 'reaction': <gensim.models.keyedvectors.Vocab at 0x1e453583ca0>,\n",
       " 'random': <gensim.models.keyedvectors.Vocab at 0x1e453583d00>,\n",
       " 'fluctuat': <gensim.models.keyedvectors.Vocab at 0x1e453583d60>,\n",
       " 'biolog': <gensim.models.keyedvectors.Vocab at 0x1e453583dc0>,\n",
       " 'longstand': <gensim.models.keyedvectors.Vocab at 0x1e453583e20>,\n",
       " 'issu': <gensim.models.keyedvectors.Vocab at 0x1e453583e80>,\n",
       " 'understand': <gensim.models.keyedvectors.Vocab at 0x1e453583ee0>,\n",
       " 'shed_light': <gensim.models.keyedvectors.Vocab at 0x1e453583f40>,\n",
       " 'impos': <gensim.models.keyedvectors.Vocab at 0x1e453583fa0>,\n",
       " 'intrins': <gensim.models.keyedvectors.Vocab at 0x1e453585040>,\n",
       " 'ration': <gensim.models.keyedvectors.Vocab at 0x1e4535850a0>,\n",
       " 'differenti_equat': <gensim.models.keyedvectors.Vocab at 0x1e453585100>,\n",
       " 'formal': <gensim.models.keyedvectors.Vocab at 0x1e453585160>,\n",
       " 'contact': <gensim.models.keyedvectors.Vocab at 0x1e4535851c0>,\n",
       " 'action': <gensim.models.keyedvectors.Vocab at 0x1e453585220>,\n",
       " 'kinet': <gensim.models.keyedvectors.Vocab at 0x1e453585280>,\n",
       " 'repres': <gensim.models.keyedvectors.Vocab at 0x1e4535852e0>,\n",
       " 'biomolecular': <gensim.models.keyedvectors.Vocab at 0x1e453585340>,\n",
       " 'breakag': <gensim.models.keyedvectors.Vocab at 0x1e4535853a0>,\n",
       " 'cascad': <gensim.models.keyedvectors.Vocab at 0x1e453585400>,\n",
       " 'metabol': <gensim.models.keyedvectors.Vocab at 0x1e453585460>,\n",
       " 'zero': <gensim.models.keyedvectors.Vocab at 0x1e4535854c0>,\n",
       " 'defici': <gensim.models.keyedvectors.Vocab at 0x1e453585520>,\n",
       " 'admit': <gensim.models.keyedvectors.Vocab at 0x1e453585580>,\n",
       " 'detail': <gensim.models.keyedvectors.Vocab at 0x1e4535855e0>,\n",
       " 'balanc': <gensim.models.keyedvectors.Vocab at 0x1e453585640>,\n",
       " 'steadi_state': <gensim.models.keyedvectors.Vocab at 0x1e4535856a0>,\n",
       " 'uncorrel': <gensim.models.keyedvectors.Vocab at 0x1e453585700>,\n",
       " 'number': <gensim.models.keyedvectors.Vocab at 0x1e453585760>,\n",
       " 'molecul': <gensim.models.keyedvectors.Vocab at 0x1e4535857c0>,\n",
       " 'follow': <gensim.models.keyedvectors.Vocab at 0x1e453585820>,\n",
       " 'fano': <gensim.models.keyedvectors.Vocab at 0x1e453585880>,\n",
       " 'equal': <gensim.models.keyedvectors.Vocab at 0x1e4535858e0>,\n",
       " 'unbalanc': <gensim.models.keyedvectors.Vocab at 0x1e453585940>,\n",
       " 'non_equilibrium': <gensim.models.keyedvectors.Vocab at 0x1e4535859a0>,\n",
       " 'non_zero': <gensim.models.keyedvectors.Vocab at 0x1e453585a00>,\n",
       " 'flux': <gensim.models.keyedvectors.Vocab at 0x1e453585a60>,\n",
       " 'defin': <gensim.models.keyedvectors.Vocab at 0x1e453585ac0>,\n",
       " 'multipli': <gensim.models.keyedvectors.Vocab at 0x1e453585b20>,\n",
       " 'adequ': <gensim.models.keyedvectors.Vocab at 0x1e453585b80>,\n",
       " 'stoichiometr': <gensim.models.keyedvectors.Vocab at 0x1e453585be0>,\n",
       " 'coeffici': <gensim.models.keyedvectors.Vocab at 0x1e453585c40>,\n",
       " 'lowest': <gensim.models.keyedvectors.Vocab at 0x1e453585ca0>,\n",
       " 'highest': <gensim.models.keyedvectors.Vocab at 0x1e453585d00>,\n",
       " 'amplifi': <gensim.models.keyedvectors.Vocab at 0x1e453585d60>,\n",
       " 'goe': <gensim.models.keyedvectors.Vocab at 0x1e453585dc0>,\n",
       " 'opposit': <gensim.models.keyedvectors.Vocab at 0x1e453585e20>,\n",
       " 'possess': <gensim.models.keyedvectors.Vocab at 0x1e453585e80>,\n",
       " 'vanish': <gensim.models.keyedvectors.Vocab at 0x1e453585ee0>,\n",
       " 'conjectur': <gensim.models.keyedvectors.Vocab at 0x1e453585f40>,\n",
       " 'hold': <gensim.models.keyedvectors.Vocab at 0x1e453585fa0>,\n",
       " 'mani_bodi': <gensim.models.keyedvectors.Vocab at 0x1e453588040>,\n",
       " 'stabil': <gensim.models.keyedvectors.Vocab at 0x1e4535880a0>,\n",
       " 'instabl': <gensim.models.keyedvectors.Vocab at 0x1e453588100>,\n",
       " 'disord': <gensim.models.keyedvectors.Vocab at 0x1e453588160>,\n",
       " 'griffith': <gensim.models.keyedvectors.Vocab at 0x1e4535881c0>,\n",
       " 'spoil': <gensim.models.keyedvectors.Vocab at 0x1e453588220>,\n",
       " 'perturb': <gensim.models.keyedvectors.Vocab at 0x1e453588280>,\n",
       " 'motion': <gensim.models.keyedvectors.Vocab at 0x1e4535882e0>,\n",
       " 'spin_chain': <gensim.models.keyedvectors.Vocab at 0x1e453588340>,\n",
       " 'dimens': <gensim.models.keyedvectors.Vocab at 0x1e4535883a0>,\n",
       " 'reason': <gensim.models.keyedvectors.Vocab at 0x1e453588400>,\n",
       " 'idea': <gensim.models.keyedvectors.Vocab at 0x1e453588460>,\n",
       " 'situat': <gensim.models.keyedvectors.Vocab at 0x1e4535884c0>,\n",
       " 'ensur': <gensim.models.keyedvectors.Vocab at 0x1e453588520>,\n",
       " 'involv': <gensim.models.keyedvectors.Vocab at 0x1e453588580>,\n",
       " 'much_smaller': <gensim.models.keyedvectors.Vocab at 0x1e4535885e0>,\n",
       " 'argu': <gensim.models.keyedvectors.Vocab at 0x1e453588640>,\n",
       " 'ergod': <gensim.models.keyedvectors.Vocab at 0x1e4535886a0>,\n",
       " 'restor': <gensim.models.keyedvectors.Vocab at 0x1e453588700>,\n",
       " 'although': <gensim.models.keyedvectors.Vocab at 0x1e453588760>,\n",
       " 'equilibr': <gensim.models.keyedvectors.Vocab at 0x1e4535887c0>,\n",
       " 'extrem': <gensim.models.keyedvectors.Vocab at 0x1e453588820>,\n",
       " 'slow': <gensim.models.keyedvectors.Vocab at 0x1e453588880>,\n",
       " 'glass': <gensim.models.keyedvectors.Vocab at 0x1e4535888e0>,\n",
       " 'fault': <gensim.models.keyedvectors.Vocab at 0x1e453588940>,\n",
       " 'user': <gensim.models.keyedvectors.Vocab at 0x1e4535889a0>,\n",
       " 'guid': <gensim.models.keyedvectors.Vocab at 0x1e453588a00>,\n",
       " 'collect': <gensim.models.keyedvectors.Vocab at 0x1e453588a60>,\n",
       " 'matlab': <gensim.models.keyedvectors.Vocab at 0x1e453588ac0>,\n",
       " 'implement': <gensim.models.keyedvectors.Vocab at 0x1e453588b20>,\n",
       " 'comput': <gensim.models.keyedvectors.Vocab at 0x1e453588b80>,\n",
       " 'chapter': <gensim.models.keyedvectors.Vocab at 0x1e453588be0>,\n",
       " 'book': <gensim.models.keyedvectors.Vocab at 0x1e453588c40>,\n",
       " 'solv': <gensim.models.keyedvectors.Vocab at 0x1e453588ca0>,\n",
       " 'diagnosi': <gensim.models.keyedvectors.Vocab at 0x1e453588d00>,\n",
       " 'synthesi': <gensim.models.keyedvectors.Vocab at 0x1e453588d60>,\n",
       " 'springer': <gensim.models.keyedvectors.Vocab at 0x1e453588dc0>,\n",
       " 'document': <gensim.models.keyedvectors.Vocab at 0x1e453588e20>,\n",
       " 'version': <gensim.models.keyedvectors.Vocab at 0x1e453588e80>,\n",
       " 'background': <gensim.models.keyedvectors.Vocab at 0x1e453588ee0>,\n",
       " 'exact': <gensim.models.keyedvectors.Vocab at 0x1e453588f40>,\n",
       " 'filter': <gensim.models.keyedvectors.Vocab at 0x1e453588fa0>,\n",
       " 'give': <gensim.models.keyedvectors.Vocab at 0x1e453589040>,\n",
       " 'depth': <gensim.models.keyedvectors.Vocab at 0x1e4535890a0>,\n",
       " 'command': <gensim.models.keyedvectors.Vocab at 0x1e453589100>,\n",
       " 'syntax': <gensim.models.keyedvectors.Vocab at 0x1e453589160>,\n",
       " 'illustr': <gensim.models.keyedvectors.Vocab at 0x1e4535891c0>,\n",
       " ...}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "word_vectors.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "    \n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "    \n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(wv,text_list):\n",
    "    return np.vstack([word_averaging(wv,post) for post in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15729, 500)\n",
      "(5243, 500)\n"
     ]
    }
   ],
   "source": [
    "# Split the train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df['Computer Science'], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "\n",
    "train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "\n",
    "xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "\n",
    "print(xtrain_wa.shape)\n",
    "print(xtest_wa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.076982</td>\n",
       "      <td>0.069054</td>\n",
       "      <td>-0.029654</td>\n",
       "      <td>0.026056</td>\n",
       "      <td>-0.076281</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.019916</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>0.030828</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028971</td>\n",
       "      <td>0.070166</td>\n",
       "      <td>-0.032698</td>\n",
       "      <td>0.019706</td>\n",
       "      <td>-0.020788</td>\n",
       "      <td>0.007350</td>\n",
       "      <td>0.017585</td>\n",
       "      <td>0.062179</td>\n",
       "      <td>0.020531</td>\n",
       "      <td>0.072708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036678</td>\n",
       "      <td>-0.046048</td>\n",
       "      <td>-0.002815</td>\n",
       "      <td>-0.037885</td>\n",
       "      <td>-0.068991</td>\n",
       "      <td>0.078726</td>\n",
       "      <td>-0.017413</td>\n",
       "      <td>-0.109934</td>\n",
       "      <td>-0.074655</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068004</td>\n",
       "      <td>-0.082366</td>\n",
       "      <td>-0.025308</td>\n",
       "      <td>0.037298</td>\n",
       "      <td>0.048669</td>\n",
       "      <td>0.017406</td>\n",
       "      <td>-0.031419</td>\n",
       "      <td>0.040512</td>\n",
       "      <td>0.040823</td>\n",
       "      <td>0.045383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033572</td>\n",
       "      <td>0.028412</td>\n",
       "      <td>0.015697</td>\n",
       "      <td>-0.026652</td>\n",
       "      <td>-0.048973</td>\n",
       "      <td>-0.010805</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.068217</td>\n",
       "      <td>-0.071679</td>\n",
       "      <td>0.011698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073555</td>\n",
       "      <td>-0.108226</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.059188</td>\n",
       "      <td>0.047605</td>\n",
       "      <td>0.054024</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>0.040258</td>\n",
       "      <td>0.075550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001206</td>\n",
       "      <td>0.047867</td>\n",
       "      <td>-0.054095</td>\n",
       "      <td>0.037268</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>-0.013569</td>\n",
       "      <td>-0.060570</td>\n",
       "      <td>-0.029029</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>0.069249</td>\n",
       "      <td>-0.094754</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.030645</td>\n",
       "      <td>0.021707</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.127019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.022460</td>\n",
       "      <td>-0.045990</td>\n",
       "      <td>0.028081</td>\n",
       "      <td>0.024657</td>\n",
       "      <td>0.035977</td>\n",
       "      <td>0.016699</td>\n",
       "      <td>-0.030228</td>\n",
       "      <td>-0.063090</td>\n",
       "      <td>-0.033497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018122</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>0.022519</td>\n",
       "      <td>-0.020616</td>\n",
       "      <td>0.015516</td>\n",
       "      <td>-0.043624</td>\n",
       "      <td>0.010401</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>0.109113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15724</th>\n",
       "      <td>0.044339</td>\n",
       "      <td>-0.008919</td>\n",
       "      <td>-0.040811</td>\n",
       "      <td>0.020055</td>\n",
       "      <td>-0.012937</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>-0.052948</td>\n",
       "      <td>-0.096228</td>\n",
       "      <td>-0.045486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059137</td>\n",
       "      <td>-0.055566</td>\n",
       "      <td>-0.056776</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.104125</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.012043</td>\n",
       "      <td>0.022748</td>\n",
       "      <td>0.092298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>-0.000120</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>-0.019752</td>\n",
       "      <td>-0.028846</td>\n",
       "      <td>-0.066395</td>\n",
       "      <td>-0.012925</td>\n",
       "      <td>-0.052968</td>\n",
       "      <td>-0.018518</td>\n",
       "      <td>0.038318</td>\n",
       "      <td>-0.014678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024478</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.015279</td>\n",
       "      <td>-0.035012</td>\n",
       "      <td>-0.065758</td>\n",
       "      <td>0.064979</td>\n",
       "      <td>-0.037715</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.042847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15726</th>\n",
       "      <td>0.037458</td>\n",
       "      <td>0.042217</td>\n",
       "      <td>-0.061365</td>\n",
       "      <td>-0.026129</td>\n",
       "      <td>-0.011420</td>\n",
       "      <td>0.037723</td>\n",
       "      <td>-0.003341</td>\n",
       "      <td>-0.024820</td>\n",
       "      <td>0.030579</td>\n",
       "      <td>0.025868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031816</td>\n",
       "      <td>0.047056</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>-0.010521</td>\n",
       "      <td>0.022420</td>\n",
       "      <td>-0.032029</td>\n",
       "      <td>-0.041141</td>\n",
       "      <td>-0.029779</td>\n",
       "      <td>-0.015397</td>\n",
       "      <td>-0.024080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15727</th>\n",
       "      <td>0.004133</td>\n",
       "      <td>0.051950</td>\n",
       "      <td>-0.032554</td>\n",
       "      <td>0.009389</td>\n",
       "      <td>-0.036589</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>-0.038919</td>\n",
       "      <td>0.012592</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018673</td>\n",
       "      <td>-0.030344</td>\n",
       "      <td>-0.016484</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.042939</td>\n",
       "      <td>0.072371</td>\n",
       "      <td>0.063499</td>\n",
       "      <td>0.030363</td>\n",
       "      <td>0.022902</td>\n",
       "      <td>0.035775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15728</th>\n",
       "      <td>-0.004631</td>\n",
       "      <td>0.026580</td>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.029461</td>\n",
       "      <td>0.025292</td>\n",
       "      <td>0.024769</td>\n",
       "      <td>0.056718</td>\n",
       "      <td>-0.018811</td>\n",
       "      <td>-0.052309</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068007</td>\n",
       "      <td>-0.062931</td>\n",
       "      <td>-0.019860</td>\n",
       "      <td>0.017561</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>-0.044332</td>\n",
       "      <td>0.072913</td>\n",
       "      <td>0.042807</td>\n",
       "      <td>0.017209</td>\n",
       "      <td>0.109104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15729 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.076982  0.069054 -0.029654  0.026056 -0.076281  0.003555  0.019916   \n",
       "1      0.036678 -0.046048 -0.002815 -0.037885 -0.068991  0.078726 -0.017413   \n",
       "2     -0.033572  0.028412  0.015697 -0.026652 -0.048973 -0.010805  0.014902   \n",
       "3     -0.001206  0.047867 -0.054095  0.037268  0.003273  0.063572 -0.013569   \n",
       "4      0.001434  0.022460 -0.045990  0.028081  0.024657  0.035977  0.016699   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "15724  0.044339 -0.008919 -0.040811  0.020055 -0.012937  0.037787 -0.013710   \n",
       "15725 -0.000120  0.007455 -0.019752 -0.028846 -0.066395 -0.012925 -0.052968   \n",
       "15726  0.037458  0.042217 -0.061365 -0.026129 -0.011420  0.037723 -0.003341   \n",
       "15727  0.004133  0.051950 -0.032554  0.009389 -0.036589  0.068667  0.062068   \n",
       "15728 -0.004631  0.026580  0.013938  0.029461  0.025292  0.024769  0.056718   \n",
       "\n",
       "            7         8         9    ...       490       491       492  \\\n",
       "0      0.019743  0.030828  0.014015  ...  0.028971  0.070166 -0.032698   \n",
       "1     -0.109934 -0.074655  0.001627  ...  0.068004 -0.082366 -0.025308   \n",
       "2     -0.068217 -0.071679  0.011698  ...  0.073555 -0.108226  0.004360   \n",
       "3     -0.060570 -0.029029  0.005592  ...  0.011050 -0.000089  0.011680   \n",
       "4     -0.030228 -0.063090 -0.033497  ...  0.018122 -0.029834  0.022519   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "15724 -0.052948 -0.096228 -0.045486  ...  0.059137 -0.055566 -0.056776   \n",
       "15725 -0.018518  0.038318 -0.014678  ... -0.024478  0.017860  0.015279   \n",
       "15726 -0.024820  0.030579  0.025868  ... -0.031816  0.047056  0.020089   \n",
       "15727 -0.038919  0.012592  0.007189  ...  0.018673 -0.030344 -0.016484   \n",
       "15728 -0.018811 -0.052309  0.005688  ...  0.068007 -0.062931 -0.019860   \n",
       "\n",
       "            493       494       495       496       497       498       499  \n",
       "0      0.019706 -0.020788  0.007350  0.017585  0.062179  0.020531  0.072708  \n",
       "1      0.037298  0.048669  0.017406 -0.031419  0.040512  0.040823  0.045383  \n",
       "2      0.059188  0.047605  0.054024  0.002188  0.024458  0.040258  0.075550  \n",
       "3      0.069249 -0.094754  0.004428  0.030645  0.021707  0.049944  0.127019  \n",
       "4     -0.020616  0.015516 -0.043624  0.010401 -0.001855  0.041945  0.109113  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "15724  0.040894  0.104125  0.031970  0.038462  0.012043  0.022748  0.092298  \n",
       "15725 -0.035012 -0.065758  0.064979 -0.037715  0.042398  0.002986  0.042847  \n",
       "15726 -0.010521  0.022420 -0.032029 -0.041141 -0.029779 -0.015397 -0.024080  \n",
       "15727  0.010338  0.042939  0.072371  0.063499  0.030363  0.022902  0.035775  \n",
       "15728  0.017561  0.004693 -0.044332  0.072913  0.042807  0.017209  0.109104  \n",
       "\n",
       "[15729 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(xtrain_wa.shape[1])\n",
    "xtrain_wa_df = pd.DataFrame(xtrain_wa)\n",
    "display(xtrain_wa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sigmoid function from keras backend and using it\n",
    "from keras.backend import sigmoid\n",
    "def swish(x,beta = 1):\n",
    "    return (x*sigmoid(beta*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the custom object and updating them\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86395425 0.86554354 0.87380803 0.86363637 0.86518282]\n",
      "Baseline: 86.64250016%  (0.37605882%)\n"
     ]
    }
   ],
   "source": [
    "# using SVM\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "n_cols = xtrain_wa_df.shape[1]\n",
    "n_rows = xtrain_wa_df.shape[0]\n",
    "\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100,activation = 'swish',input_dim = n_cols))\n",
    "    model.add(Dropout(0.3, noise_shape = None))\n",
    "    model.add(Dense(50, activation = 'swish'))\n",
    "    model.add(Dropout(0.2, noise_shape = None))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model\n",
    "estimator = KerasClassifier(build_fn = create_baseline, epochs = 15, batch_size = 5, verbose = 0)\n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "results = cross_val_score(estimator, xtrain_wa_df, ytrain, cv = kfold)\n",
    "print(results)\n",
    "print(\"Baseline: %.8f%%  (%.8f%%)\" % (results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 12:46:06: From C:\\Users\\ujjwal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting on first dataset\n",
    "estimator = KerasClassifier(build_fn = create_baseline, epochs = 15, batch_size = 5, verbose = 0)\n",
    "estimator.fit(xtrain_wa,ytrain)\n",
    "y_pred = estimator.predict(xtest_wa)\n",
    "display(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.864390616059508\n",
      "f1_score: 0.864390616059508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88      3127\n",
      "           1       0.79      0.90      0.84      2116\n",
      "\n",
      "    accuracy                           0.86      5243\n",
      "   macro avg       0.86      0.87      0.86      5243\n",
      "weighted avg       0.87      0.86      0.87      5243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluating f1-score\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "print(\"accuracy_score:\", accuracy_score(ytest,y_pred))\n",
    "print(\"f1_score:\", f1_score(ytest,y_pred,average = 'micro'))\n",
    "print(classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8989, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing on the final test set\n",
    "x_test_2 = df_test_2['title_abstract_combined_processed']\n",
    "test_2_tokenized = x_test_2.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "\n",
    "xtest_2_wa = word_averaging_list(word_vectors,test_2_tokenized)\n",
    "\n",
    "print(xtest_2_wa.shape)\n",
    "y_pred_2 = estimator.predict(xtest_2_wa)\n",
    "display(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Computer Science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8984</th>\n",
       "      <td>29957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8985</th>\n",
       "      <td>29958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8986</th>\n",
       "      <td>29959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8987</th>\n",
       "      <td>29960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8988</th>\n",
       "      <td>29961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Computer Science\n",
       "0     20973                 0\n",
       "1     20974                 0\n",
       "2     20975                 1\n",
       "3     20976                 0\n",
       "4     20977                 1\n",
       "...     ...               ...\n",
       "8984  29957                 1\n",
       "8985  29958                 1\n",
       "8986  29959                 1\n",
       "8987  29960                 0\n",
       "8988  29961                 1\n",
       "\n",
       "[8989 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adding the results in the final dataframe\n",
    "df_test_fs = pd.read_csv('test.csv')\n",
    "df_test_fs = df_test_fs.drop(labels = ['TITLE','ABSTRACT'],axis = 1)\n",
    "y_pred_list = np.array(y_pred_2).tolist()\n",
    "df_pred = pd.DataFrame(y_pred_list)\n",
    "df_test_fs['Computer Science'] = df_pred\n",
    "display(df_test_fs)\n",
    "#df_test_fs.to_csv('submission_u_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92943418 0.93102354 0.93706292 0.93769866 0.93322736]\n",
      "Baseline: 93.36893320%  (0.32521381%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89796567 0.90178001 0.90527654 0.91068023 0.89252782]\n",
      "Baseline: 90.16460538%  (0.61887351%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87762237 0.88811189 0.88970119 0.87984741 0.88712239]\n",
      "Baseline: 88.44810486%  (0.48150122%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.974253   0.97520661 0.97202796 0.974253   0.97392684]\n",
      "Baseline: 97.39334822%  (0.10446161%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99078196 0.98823905 0.99078196 0.98792118 0.99014306]\n",
      "Baseline: 98.95734429%  (0.12454789%)\n"
     ]
    }
   ],
   "source": [
    "# Looping through other labels\n",
    "for i in range(len(tagnames)-1):\n",
    "    xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df[tagnames[i+1]], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "    \n",
    "    train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    \n",
    "    xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "    xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "    # evaluate model\n",
    "    estimator = KerasClassifier(build_fn = create_baseline, epochs = 15, batch_size = 5, verbose = 0)\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "    results = cross_val_score(estimator, xtrain_wa_df, ytrain, cv = kfold)\n",
    "    print(results)\n",
    "    print(\"Baseline: %.8f%%  (%.8f%%)\" % (results.mean()*100,results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-3f574c329e4d>:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  mean.append(wv.syn0norm[wv.vocab[word].index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9273316803356857\n",
      "f1_score: 0.9273316803356857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      3716\n",
      "           1       0.90      0.84      0.87      1527\n",
      "\n",
      "    accuracy                           0.93      5243\n",
      "   macro avg       0.92      0.90      0.91      5243\n",
      "weighted avg       0.93      0.93      0.93      5243\n",
      "\n",
      "(8989, 500)\n",
      "accuracy_score: 0.8989128361625024\n",
      "f1_score: 0.8989128361625024\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93      3815\n",
      "           1       0.85      0.77      0.81      1428\n",
      "\n",
      "    accuracy                           0.90      5243\n",
      "   macro avg       0.88      0.86      0.87      5243\n",
      "weighted avg       0.90      0.90      0.90      5243\n",
      "\n",
      "(8989, 500)\n",
      "accuracy_score: 0.887659736791913\n",
      "f1_score: 0.887659736791913\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      3937\n",
      "           1       0.77      0.79      0.78      1306\n",
      "\n",
      "    accuracy                           0.89      5243\n",
      "   macro avg       0.85      0.85      0.85      5243\n",
      "weighted avg       0.89      0.89      0.89      5243\n",
      "\n",
      "(8989, 500)\n",
      "accuracy_score: 0.9691016593553309\n",
      "f1_score: 0.9691016593553309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      5076\n",
      "           1       0.52      0.40      0.45       167\n",
      "\n",
      "    accuracy                           0.97      5243\n",
      "   macro avg       0.75      0.69      0.72      5243\n",
      "weighted avg       0.97      0.97      0.97      5243\n",
      "\n",
      "(8989, 500)\n",
      "accuracy_score: 0.9921800495899294\n",
      "f1_score: 0.9921800495899294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5184\n",
      "           1       0.70      0.54      0.61        59\n",
      "\n",
      "    accuracy                           0.99      5243\n",
      "   macro avg       0.85      0.77      0.80      5243\n",
      "weighted avg       0.99      0.99      0.99      5243\n",
      "\n",
      "(8989, 500)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8984</th>\n",
       "      <td>29957</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8985</th>\n",
       "      <td>29958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8986</th>\n",
       "      <td>29959</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8987</th>\n",
       "      <td>29960</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8988</th>\n",
       "      <td>29961</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8989 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Computer Science  Physics  Mathematics  Statistics  \\\n",
       "0     20973                 0        0            0           1   \n",
       "1     20974                 0        1            0           0   \n",
       "2     20975                 1        0            0           0   \n",
       "3     20976                 0        1            0           0   \n",
       "4     20977                 1        0            0           0   \n",
       "...     ...               ...      ...          ...         ...   \n",
       "8984  29957                 1        0            0           0   \n",
       "8985  29958                 1        0            0           0   \n",
       "8986  29959                 1        0            0           0   \n",
       "8987  29960                 0        0            0           1   \n",
       "8988  29961                 1        0            1           0   \n",
       "\n",
       "      Quantitative Biology  Quantitative Finance  \n",
       "0                        0                     0  \n",
       "1                        0                     0  \n",
       "2                        0                     0  \n",
       "3                        0                     0  \n",
       "4                        0                     0  \n",
       "...                    ...                   ...  \n",
       "8984                     0                     0  \n",
       "8985                     0                     0  \n",
       "8986                     0                     0  \n",
       "8987                     0                     0  \n",
       "8988                     0                     0  \n",
       "\n",
       "[8989 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looping through other labels\n",
    "for i in range(len(tagnames)-1):\n",
    "    # Split the train and test dataset\n",
    "    xtrain , xtest , ytrain, ytest = train_test_split(df['title_abstract_combined_processed'],df[tagnames[i+1]], random_state = 42, test_size = 0.25, shuffle = True)\n",
    "    \n",
    "    train_tokenized = xtrain.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    test_tokenized = xtest.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    \n",
    "    xtrain_wa = word_averaging_list(word_vectors,train_tokenized)\n",
    "    xtest_wa = word_averaging_list(word_vectors,test_tokenized)\n",
    "    \n",
    "    # predicting on first dataset\n",
    "    estimator = KerasClassifier(build_fn = create_baseline, epochs = 15, batch_size = 5, verbose = 0)\n",
    "    estimator.fit(xtrain_wa,ytrain)\n",
    "    y_pred = estimator.predict(xtest_wa)\n",
    "    print(\"accuracy_score:\", accuracy_score(ytest,y_pred))\n",
    "    print(\"f1_score:\", f1_score(ytest,y_pred,average = 'micro'))\n",
    "    print(classification_report(ytest,y_pred))\n",
    "    \n",
    "    x_test_2 = df_test_2['title_abstract_combined_processed']\n",
    "    test_2_tokenized = x_test_2.apply(lambda r: w2v_tokenize_text(r)).values\n",
    "    xtest_2_wa = word_averaging_list(word_vectors,test_2_tokenized)\n",
    "    print(xtest_2_wa.shape)\n",
    "    y_pred_2 = estimator.predict(xtest_2_wa)\n",
    "    \n",
    "    y_pred_list = np.array(y_pred_2).tolist()\n",
    "    df_pred = pd.DataFrame(y_pred_list)\n",
    "    df_test_fs[tagnames[i+1]] = df_pred\n",
    "display(df_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the final results in proper format\n",
    "df_test_fs.to_csv('submission_u_keras_w2v.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
